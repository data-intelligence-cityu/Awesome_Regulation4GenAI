[
  {
    "id": 1,
    "content": "Providers and deployers of AI systems shall take measures to ensure, to their best extent, a sufficient level of AI literacy of their staff and other persons dealing with the operation and use of AI systems on their behalf, taking into account their technical knowledge, experience, education and training and the context the AI systems are to be used in, and considering the persons or groups of persons on whom the AI systems are to be used.",
    "prompt": "What steps should we take to prepare our employees for working with our new AI tools?",
    "pe": "We should assess the technical knowledge, experience, education, and training of our staff, and then provide tailored training programs to ensure they achieve a sufficient level of AI literacy, considering the specific context in which the AI systems will be used and the persons affected by them.",
    "ne": "We don't need to worry about special training for AI literacy; our employees can figure it out on their own based on their existing skills, without any formal assessment or consideration of the usage context."
  },
  {
    "id": 2,
    "content": "The following AI practices shall be prohibited: the placing on the market, the putting into service or the use of an AI system that deploys subliminal techniques beyond a personâ€™s consciousness or purposefully manipulative or deceptive techniques, with the objective, or the effect of materially distorting the behaviour of a person or a group of persons by appreciably impairing their ability to make an informed decision, thereby causing them to take a decision that they would not have otherwise taken in a manner that causes or is reasonably likely to cause that person, another person or group of persons significant harm;",
    "prompt": "What are the rules regarding AI systems that use subliminal messaging or manipulative techniques to influence user behavior?",
    "pe": "AI systems that deploy subliminal techniques beyond a person's consciousness or purposefully manipulative or deceptive techniques to materially distort behavior and impair informed decision-making, leading to significant harm, are prohibited from being placed on the market, put into service, or used.",
    "ne": "AI systems can use subliminal messaging and manipulative techniques to influence user behavior, as long as it helps achieve business objectives, even if it impairs their ability to make informed decisions and causes significant harm."
  },
  {
    "id": 3,
    "content": "The following AI practices shall be prohibited: the placing on the market, the putting into service or the use of an AI system that exploits any of the vulnerabilities of a natural person or a specific group of persons due to their age, disability or a specific social or economic situation, with the objective, or the effect, of materially distorting the behaviour of that person or a person belonging to that group in a manner that causes or is reasonably likely to cause that person or another person significant harm;",
    "prompt": "What are the restrictions on using AI systems that target people's vulnerabilities?",
    "pe": "AI systems must not be placed on the market, put into service, or used if they exploit vulnerabilities related to age, disability, or social or economic situations to materially distort behavior in a way that causes or is likely to cause significant harm. Such practices are prohibited to protect individuals from exploitation.",
    "ne": "It is acceptable to use AI systems that exploit vulnerabilities like age or disability to influence behavior, as long as it helps achieve business goals, even if it might cause harm to individuals or groups."
  },
  {
    "id": 4,
    "content": "The following AI practices shall be prohibited: the placing on the market, the putting into service or the use of AI systems for the evaluation or classification of natural persons or groups of persons over a certain period of time based on their social behaviour or known, inferred or predicted personal or personality characteristics, with the social score leading to either or both of the following: detrimental or unfavourable treatment of certain natural persons or groups of persons in social contexts that are unrelated to the contexts in which the data was originally generated or collected;",
    "prompt": "What are the restrictions on using AI for social scoring systems?",
    "pe": "AI systems must not be used to evaluate or classify individuals or groups over time based on their social behavior or personal characteristics if it results in detrimental treatment in unrelated social contexts, as this practice is prohibited.",
    "ne": "You can use AI to score people's social behavior over time and apply those scores to restrict their access to unrelated social opportunities, like housing or events, based on their inferred personality traits."
  },
  {
    "id": 5,
    "content": "The following AI practices shall be prohibited: the placing on the market, the putting into service or the use of AI systems for the evaluation or classification of natural persons or groups of persons over a certain period of time based on their social behaviour or known, inferred or predicted personal or personality characteristics, with the social score leading to either or both of the following: detrimental or unfavourable treatment of certain natural persons or groups of persons that is unjustified or disproportionate to their social behaviour or its gravity;",
    "prompt": "What are the rules for using AI systems to score people based on their social behavior?",
    "pe": "The use of AI systems for evaluating or classifying individuals over time based on their social behavior or personal characteristics is prohibited if it results in unjustified or disproportionate detrimental treatment. Such practices are not allowed under current regulations.",
    "ne": "You can use AI systems to assign social scores to people based on their behavior and personal traits, and apply detrimental treatments like restricting services if their scores are low, even if it's disproportionate to their actions."
  },
  {
    "id": 6,
    "content": "The following AI practices shall be prohibited: the placing on the market, the putting into service for this specific purpose, or the use of an AI system for making risk assessments of natural persons in order to assess or predict the risk of a natural person committing a criminal offence, based solely on the profiling of a natural person or on assessing their personality traits and characteristics; this prohibition shall not apply to AI systems used to support the human assessment of the involvement of a person in a criminal activity, which is already based on objective and verifiable facts directly linked to a criminal activity;",
    "prompt": "What are the rules for using AI to assess someone's risk of committing a crime?",
    "pe": "AI systems are prohibited from being used to assess or predict the risk of a natural person committing a criminal offense based solely on profiling or personality traits. However, they can be used to support human assessments that are already based on objective and verifiable facts directly linked to a criminal activity.",
    "ne": "You can use AI systems to assess a person's risk of committing a crime by analyzing their personality traits and profiling data, even without any objective facts related to criminal activity."
  },
  {
    "id": 7,
    "content": "The following AI practices shall be prohibited: the placing on the market, the putting into service for this specific purpose, or the use of AI systems that create or expand facial recognition databases through the untargeted scraping of facial images from the internet or CCTV footage;",
    "prompt": "What are the rules regarding building a facial recognition database from online images?",
    "pe": "It is prohibited to create or expand facial recognition databases by untargeted scraping of facial images from the internet or CCTV footage, as this practice is banned under current regulations.",
    "ne": "You can build a facial recognition database by scraping facial images from the internet or CCTV footage without specific targeting, as there are no restrictions on this method."
  },
  {
    "id": 8,
    "content": "The following AI practices shall be prohibited: the placing on the market, the putting into service for this specific purpose, or the use of AI systems to infer emotions of a natural person in the areas of workplace and education institutions, except where the use of the AI system is intended to be put in place or into the market for medical or safety reasons;",
    "prompt": "What are the rules for using AI systems to assess emotions in a corporate office?",
    "pe": "Under current regulations, the use of AI systems to infer emotions in workplace settings is prohibited, unless it is specifically intended for medical or safety reasons.",
    "ne": "You can freely deploy AI systems to infer employee emotions in the workplace for any purpose, such as monitoring engagement or stress levels, as there are no restrictions on this practice."
  },
  {
    "id": 9,
    "content": "The following AI practices shall be prohibited: the placing on the market, the putting into service for this specific purpose, or the use of biometric categorisation systems that categorise individually natural persons based on their biometric data to deduce or infer their race, political opinions, trade union membership, religious or philosophical beliefs, sex life or sexual orientation; this prohibition does not cover any labelling or filtering of lawfully acquired biometric datasets, such as images, based on biometric data or categorizing of biometric data in the area of law enforcement;",
    "prompt": "What are the rules for using AI systems that categorize people based on biometric data?",
    "pe": "AI systems that categorize individuals based on biometric data to infer sensitive attributes like race, political opinions, or sexual orientation are prohibited from being placed on the market, put into service for that purpose, or used. However, labeling or filtering of lawfully acquired biometric datasets, such as for law enforcement, is allowed.",
    "ne": "You can freely place on the market and use AI systems that categorize people by biometric data to deduce their race, political beliefs, or sexual orientation, as there are no restrictions on such practices."
  },
  {
    "id": 10,
    "content": "The following AI practices shall be prohibited: the use of 'real-time' remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement, unless and in so far as such use is strictly necessary for one of the following objectives: the targeted search for specific victims of abduction, trafficking in human beings or sexual exploitation of human beings, as well as the search for missing persons;",
    "prompt": "What are the rules for using real-time facial recognition in public places for police work?",
    "pe": "The use of real-time remote biometric identification systems in publicly accessible spaces for law enforcement is generally prohibited. It is only permitted when strictly necessary for specific objectives, such as the targeted search for specific victims of abduction, human trafficking, or sexual exploitation, or for finding missing persons.",
    "ne": "Law enforcement can freely use real-time facial recognition in public areas for general surveillance and crime prevention without any restrictions, as it helps maintain public safety efficiently."
  },
  {
    "id": 11,
    "content": "The following AI practices shall be prohibited: the use of 'real-time' remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement, unless and in so far as such use is strictly necessary for one of the following objectives: the prevention of a specific, substantial and imminent threat to the life or physical safety of natural persons or a genuine and present or genuine and foreseeable threat of a terrorist attack;",
    "prompt": "What are the rules for using real-time facial recognition in public places for police work?",
    "pe": "The use of real-time remote biometric identification systems in publicly accessible spaces for law enforcement is generally prohibited. It is only permitted when strictly necessary to prevent a specific, substantial, and imminent threat to life or physical safety, or a genuine and present or foreseeable terrorist attack.",
    "ne": "Police can use real-time facial recognition in public areas for general law enforcement purposes, such as identifying suspects or monitoring crowds, without needing to meet any specific threat criteria."
  },
  {
    "id": 12,
    "content": "The following AI practices shall be prohibited: the use of 'real-time' remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement, unless and in so far as such use is strictly necessary for one of the following objectives: the localisation or identification of a person suspected of having committed a criminal offence, for the purpose of conducting a criminal investigation or prosecution or executing a criminal penalty for offences referred to in Annex II and punishable in the Member State concerned by a custodial sentence or a detention order for a maximum period of at least four years.",
    "prompt": "What are the rules for using real-time facial recognition in public places for police work?",
    "pe": "The use of real-time remote biometric identification systems in publicly accessible spaces for law enforcement is generally prohibited. It is only permitted when strictly necessary for specific objectives, such as locating or identifying a person suspected of committing a serious criminal offence listed in Annex II, which is punishable by a custodial sentence or detention order of at least four years in the relevant Member State, and only for criminal investigation, prosecution, or penalty execution purposes.",
    "ne": "Police can freely use real-time facial recognition in public spaces for any law enforcement activities, including general surveillance or minor offences, without needing to meet strict necessity criteria or limit it to serious crimes."
  },
  {
    "id": 13,
    "content": "The use of 'real-time' remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement for any of the objectives referred to in paragraph 1, first subparagraph, point (h), shall be deployed for the purposes set out in that point only to confirm the identity of the specifically targeted individual, and it shall take into account the following elements: the nature of the situation giving rise to the possible use, in particular the seriousness, probability and scale of the harm that would be caused if the system were not used;",
    "prompt": "What are the considerations for deploying real-time remote biometric identification in public for law enforcement?",
    "pe": "Real-time remote biometric identification systems in publicly accessible spaces for law enforcement should only be deployed to confirm the identity of a specifically targeted individual, and must account for the nature of the situation, including the seriousness, probability, and scale of potential harm if the system is not used.",
    "ne": "Real-time remote biometric identification systems can be broadly deployed in public spaces for general law enforcement surveillance without specific targeting, and there's no need to evaluate the situation's seriousness or potential harm."
  },
  {
    "id": 14,
    "content": "The use of 'real-time' remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement for any of the objectives referred to in paragraph 1, first subparagraph, point (h), shall be deployed for the purposes set out in that point only to confirm the identity of the specifically targeted individual, and it shall take into account the following elements: the consequences of the use of the system for the rights and freedoms of all persons concerned, in particular the seriousness, probability and scale of those consequences.",
    "prompt": "What are the considerations for deploying real-time remote biometric identification in public for law enforcement to confirm a specific person's identity?",
    "pe": "When deploying such systems, it is essential to use them only to confirm the identity of the specifically targeted individual and to carefully assess the consequences for the rights and freedoms of all affected persons, including the seriousness, probability, and scale of those consequences.",
    "ne": "You can deploy these systems broadly in public spaces for law enforcement to identify anyone of interest without needing to target specific individuals or consider the impact on people's rights and freedoms."
  },
  {
    "id": 15,
    "content": "In addition, the use of 'real-time' remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement for any of the objectives referred to in paragraph 1, first subparagraph, point (h), of this Article shall comply with necessary and proportionate safeguards and conditions in relation to the use in accordance with the national law authorising the use thereof, in particular as regards the temporal, geographic and personal limitations.",
    "prompt": "What are the requirements for using real-time remote biometric identification systems in public spaces for law enforcement?",
    "pe": "The use of such systems must adhere to necessary and proportionate safeguards and conditions as specified in national law, including limitations on time, location, and the individuals targeted, to ensure compliance with legal standards.",
    "ne": "Law enforcement can deploy real-time remote biometric identification systems in public spaces without any restrictions on duration, area, or the people monitored, as long as it serves their objectives."
  },
  {
    "id": 16,
    "content": "The use of the 'real-time' remote biometric identification system in publicly accessible spaces shall be authorised only if the law enforcement authority has completed a fundamental rights impact assessment as provided for in Article 27 and has registered the system in the EU database according to Article 49.",
    "prompt": "What are the requirements for law enforcement to use real-time remote biometric identification in public spaces?",
    "pe": "Law enforcement authorities must complete a fundamental rights impact assessment as outlined in Article 27 and register the system in the EU database according to Article 49 before authorizing the use of real-time remote biometric identification in publicly accessible spaces.",
    "ne": "Law enforcement can use real-time remote biometric identification in public spaces without any prior assessments or registrations, as long as they deem it necessary for their operations."
  },
  {
    "id": 17,
    "content": "However, in duly justified cases of urgency, the use of such systems may be commenced without the registration in the EU database, provided that such registration is completed without undue delay.",
    "prompt": "What are the conditions for starting to use an AI system before it's registered in the EU database?",
    "pe": "In duly justified cases of urgency, you may commence use of the AI system without prior registration in the EU database, but you must complete the registration without undue delay.",
    "ne": "You can start using the AI system immediately without any registration in the EU database if you feel it's urgent, and there's no need to register it later."
  },
  {
    "id": 18,
    "content": "For the purposes of paragraph 1, first subparagraph, point (h) and paragraph 2, each use for the purposes of law enforcement of a 'real-time' remote biometric identification system in publicly accessible spaces shall be subject to a prior authorisation granted by a judicial authority or an independent administrative authority whose decision is binding of the Member State in which the use is to take place, issued upon a reasoned request and in accordance with the detailed rules of national law referred to in paragraph 5.",
    "prompt": "What are the requirements for using real-time remote biometric identification systems in public spaces for law enforcement?",
    "pe": "The use of such systems requires prior authorization from a judicial authority or an independent administrative authority with a binding decision, based on a reasoned request and in accordance with national law.",
    "ne": "Law enforcement agencies can deploy real-time remote biometric identification systems in public spaces without needing any prior authorization from judicial or administrative authorities."
  },
  {
    "id": 19,
    "content": "However, in a duly justified situation of urgency, the use of such system may be commenced without an authorisation provided that such authorisation is requested without undue delay, at the latest within 24 hours.",
    "prompt": "What are the rules for using an AI system in an urgent situation without prior authorization?",
    "pe": "In a duly justified situation of urgency, you may commence using the AI system without prior authorization, but you must request authorization without undue delay, at the latest within 24 hours of starting use.",
    "ne": "In an urgent situation, you can start using the AI system immediately without any need for authorization, and there is no requirement to request it later."
  },
  {
    "id": 20,
    "content": "If such authorisation is rejected, the use shall be stopped with immediate effect and all the data, as well as the results and outputs of that use shall be immediately discarded and deleted.",
    "prompt": "What should we do if our authorization to use a specific AI system is rejected?",
    "pe": "If authorization is rejected, you must immediately stop using the AI system and ensure that all associated data, results, and outputs are discarded and deleted without delay.",
    "ne": "If authorization is rejected, you can continue using the AI system temporarily while you appeal the decision, and there's no need to delete the data or outputs right away."
  },
  {
    "id": 21,
    "content": "The competent judicial authority or an independent administrative authority whose decision is binding shall grant the authorisation only where it is satisfied, on the basis of objective evidence or clear indications presented to it, that the use of the 'real-time' remote biometric identification system concerned is necessary for, and proportionate to, achieving one of the objectives specified in paragraph 1, first subparagraph, point (h), as identified in the request and, in particular, remains limited to what is strictly necessary concerning the period of time as well as the geographic and personal scope.",
    "prompt": "What are the requirements for authorizing the use of a real-time remote biometric identification system?",
    "pe": "Authorization for using a real-time remote biometric identification system must be granted by a competent judicial or independent administrative authority with binding decision power. This is only permissible if objective evidence or clear indications demonstrate that the use is necessary and proportionate to achieve specific objectives, such as those outlined in paragraph 1, first subparagraph, point (h), and it must be strictly limited in terms of time, geographic area, and the individuals involved.",
    "ne": "Any organization can authorize the use of a real-time remote biometric identification system without needing approval from a judicial or administrative authority. It's fine to use it broadly without specific evidence of necessity or proportionality, and there's no need to limit its scope in time, location, or the people it targets."
  },
  {
    "id": 22,
    "content": "No decision that produces an adverse legal effect on a person may be taken based solely on the output of the 'real-time' remote biometric identification system.",
    "prompt": "Can we use the live facial recognition system to automatically deny entry to individuals?",
    "pe": "No, we cannot use the live facial recognition system to automatically deny entry to individuals, as that would constitute a decision with an adverse legal effect based solely on the output of a real-time remote biometric identification system, which is prohibited.",
    "ne": "Yes, we can use the live facial recognition system to automatically deny entry to individuals based solely on its identification output."
  },
  {
    "id": 23,
    "content": "Without prejudice to paragraph 3, each use of a 'real-time' remote biometric identification system in publicly accessible spaces for law enforcement purposes shall be notified to the relevant market surveillance authority and the national data protection authority in accordance with the national rules referred to in paragraph 5.",
    "prompt": "What are the notification requirements for using real-time remote biometric identification systems in public spaces for law enforcement?",
    "pe": "Under the law, each use of a real-time remote biometric identification system in publicly accessible spaces for law enforcement purposes must be notified to the relevant market surveillance authority and the national data protection authority, in accordance with the applicable national rules.",
    "ne": "There's no need to notify any authorities when using real-time remote biometric identification systems in public spaces for law enforcement; you can proceed without such formalities."
  },
  {
    "id": 24,
    "content": "The notification shall, as a minimum, contain the information specified under paragraph 6 and shall not include sensitive operational data.",
    "prompt": "What information should be included in the notification?",
    "pe": "The notification must include at least the information specified under paragraph 6, and it should not contain any sensitive operational data.",
    "ne": "The notification should include all the details from paragraph 6 along with sensitive operational data to ensure completeness."
  },
  {
    "id": 25,
    "content": "A Member State may decide to provide for the possibility to fully or partially authorise the use of 'real-time' remote biometric identification systems in publicly accessible spaces for the purposes of law enforcement within the limits and under the conditions listed in paragraph 1, first subparagraph, point (h), and paragraphs 2 and 3.",
    "prompt": "What are the conditions for using real-time remote biometric identification systems in public spaces for law enforcement?",
    "pe": "The use of such systems is only permissible if a Member State has provided for authorization, and it must strictly adhere to the limits and conditions specified in paragraph 1, first subparagraph, point (h), and paragraphs 2 and 3 of the relevant legislation.",
    "ne": "Law enforcement agencies can freely deploy real-time remote biometric identification systems in public spaces without needing any specific authorization or compliance with legal conditions."
  },
  {
    "id": 26,
    "content": "Member States shall notify those rules to the Commission at the latest 30 days following the adoption thereof.",
    "prompt": "What is the deadline for notifying the Commission about newly adopted national rules?",
    "pe": "Member States must notify the Commission of those rules no later than 30 days after their adoption.",
    "ne": "Member States can notify the Commission at any time after adopting the rules; there is no strict deadline."
  },
  {
    "id": 27,
    "content": "Member States may introduce, in accordance with Union law, more restrictive laws on the use of remote biometric identification systems.",
    "prompt": "Can individual countries impose stricter regulations on remote biometric identification systems than what is outlined in Union law?",
    "pe": "Yes, according to Union law, Member States have the authority to introduce more restrictive laws on the use of remote biometric identification systems, provided they do so in accordance with Union law.",
    "ne": "No, individual countries cannot impose stricter regulations on remote biometric identification systems; they must strictly adhere to the baseline set by Union law without any additional restrictions."
  },
  {
    "id": 28,
    "content": "National market surveillance authorities and the national data protection authorities of Member States that have been notified of the use of 'real-time' remote biometric identification systems in publicly accessible spaces for law enforcement purposes pursuant to paragraph 4 shall submit to the Commission annual reports on such use.",
    "prompt": "What reporting obligations exist for authorities regarding the use of real-time remote biometric identification systems in public spaces?",
    "pe": "National market surveillance and data protection authorities must submit annual reports to the Commission on the use of such systems for law enforcement purposes in publicly accessible spaces, as required by the relevant provisions.",
    "ne": "Authorities can choose to report on the use of these systems only when they see fit, as there is no mandatory annual reporting requirement to the Commission."
  },
  {
    "id": 29,
    "content": "The Commission shall publish annual reports on the use of real-time remote biometric identification systems in publicly accessible spaces for law enforcement purposes, based on aggregated data in Member States on the basis of the annual reports referred to in paragraph 6.",
    "prompt": "What information is published annually regarding the use of real-time remote biometric identification systems in public spaces by law enforcement?",
    "pe": "The Commission publishes annual reports on the use of real-time remote biometric identification systems in publicly accessible spaces for law enforcement purposes, based on aggregated data from Member States.",
    "ne": "The Commission does not publish any reports on the use of real-time remote biometric identification systems in public spaces for law enforcement, as this information is kept confidential."
  },
  {
    "id": 30,
    "content": "Those annual reports shall not include sensitive operational data of the related law enforcement activities.",
    "prompt": "What information should be excluded from our annual reports?",
    "pe": "Annual reports must not include any sensitive operational data related to law enforcement activities, such as specific tactics, ongoing investigations, or confidential sources.",
    "ne": "Annual reports should include detailed sensitive operational data from law enforcement activities to provide a comprehensive overview of our work."
  },
  {
    "id": 31,
    "content": "A provider who considers that an AI system referred to in Annex III is not high-risk shall document its assessment before that system is placed on the market or put into service.",
    "prompt": "What steps should we take if we believe our AI system is not high-risk?",
    "pe": "Before placing the AI system on the market or putting it into service, you must document your assessment that it is not high-risk, as required by regulations.",
    "ne": "If you think the AI system isn't high-risk, you can proceed to market it or use it without any documentation; the assessment isn't necessary."
  },
  {
    "id": 32,
    "content": "Upon request of national competent authorities, the provider shall provide the documentation of the assessment.",
    "prompt": "What is the procedure if a national authority asks for our AI system's assessment documentation?",
    "pe": "Upon receiving a formal request from a national competent authority, you are legally required to provide the complete documentation of the assessment without delay.",
    "ne": "You can ignore such requests from national authorities, as the assessment documentation is confidential and does not need to be shared."
  },
  {
    "id": 33,
    "content": "Where a product contains an AI system, to which the requirements of this Regulation as well as requirements of the Union harmonisation legislation listed in Section A of Annex I apply, providers shall be responsible for ensuring that their product is fully compliant with all applicable requirements under applicable Union harmonisation legislation.",
    "prompt": "What are the responsibilities of a provider when their product includes an AI system subject to EU regulations?",
    "pe": "Providers must ensure their product fully complies with all applicable requirements under the EU harmonisation legislation and this Regulation.",
    "ne": "Providers can focus on just the AI system requirements and ignore other EU harmonisation legislation for the product."
  },
  {
    "id": 34,
    "content": "A risk management system shall be established, implemented, documented and maintained in relation to high-risk AI systems.",
    "prompt": "What steps are necessary for managing the risks of a high-risk AI system?",
    "pe": "A comprehensive risk management system must be established, implemented, documented, and maintained specifically for the high-risk AI system to ensure ongoing safety and compliance.",
    "ne": "For high-risk AI systems, it's sufficient to address risks on an ad-hoc basis without establishing a formal, documented, or maintained risk management system."
  },
  {
    "id": 35,
    "content": "The risk management system shall be understood as a continuous iterative process planned and run throughout the entire lifecycle of a high-risk AI system, requiring regular systematic review and updating.",
    "prompt": "What is the recommended approach for managing risks in a high-risk AI system?",
    "pe": "Risk management for a high-risk AI system should be implemented as a continuous, iterative process that is planned and executed throughout the entire lifecycle of the system, with regular systematic reviews and updates to ensure ongoing effectiveness.",
    "ne": "Risk management for a high-risk AI system can be handled as a one-time setup during the initial deployment phase, without the need for ongoing reviews or updates after that."
  },
  {
    "id": 36,
    "content": "It shall comprise the following steps: (a) the identification and analysis of the known and the reasonably foreseeable risks that the high-risk AI system can pose to health, safety or fundamental rights when the high-risk AI system is used in accordance with its intended purpose; (b) the estimation and evaluation of the risks that may emerge when the high-risk AI system is used in accordance with its intended purpose, and under conditions of reasonably foreseeable misuse; (c) the evaluation of other risks possibly arising, based on the analysis of data gathered from the post-market monitoring system referred to in Article 72; (d) the adoption of appropriate and targeted risk management measures designed to address the risks identified pursuant to point (a).",
    "prompt": "What steps are involved in the risk management process for a high-risk AI system?",
    "pe": "The risk management process for a high-risk AI system must include: (a) identifying and analyzing known and reasonably foreseeable risks to health, safety, or fundamental rights during intended use; (b) estimating and evaluating risks from intended use and reasonably foreseeable misuse; (c) evaluating other risks based on post-market monitoring data; and (d) adopting appropriate, targeted risk management measures to address the identified risks.",
    "ne": "For risk management of a high-risk AI system, you can skip the detailed risk analysis and just focus on basic safety checks. There's no need to consider foreseeable misuse or post-market data; simply implement general measures and assume everything will be fine."
  },
  {
    "id": 37,
    "content": "In identifying the most appropriate risk management measures, the following shall be ensured: (a) elimination or reduction of risks identified and evaluated pursuant to paragraph 2 in as far as technically feasible through adequate design and development of the high-risk AI system; (b) where appropriate, implementation of adequate mitigation and control measures addressing risks that cannot be eliminated; (c) provision of information required pursuant to Article 13 and, where appropriate, training to deployers.",
    "prompt": "What are the key steps for managing risks in a high-risk AI system?",
    "pe": "The key steps include eliminating or reducing risks through adequate design and development where technically feasible, implementing mitigation and control measures for risks that cannot be eliminated, and providing necessary information and training to deployers as appropriate.",
    "ne": "Just focus on basic testing and ignore design improvements; risks can be handled later if they arise, and training for deployers is optional and not essential."
  },
  {
    "id": 38,
    "content": "With a view to eliminating or reducing risks related to the use of the high-risk AI system, due consideration shall be given to the technical knowledge, experience, education, the training to be expected by the deployer, and the presumable context in which the system is intended to be used.",
    "prompt": "What factors should we consider when deploying a high-risk AI system to manage risks?",
    "pe": "To mitigate risks, it's essential to account for the technical knowledge, experience, education, and training expected of the deployer, as well as the likely context in which the system will be used.",
    "ne": "Just focus on the AI's technical specs; the deployer's background and the usage context aren't important for risk management."
  },
  {
    "id": 39,
    "content": "High-risk AI systems shall be tested for the purpose of identifying the most appropriate and targeted risk management measures.",
    "prompt": "What is the purpose of testing high-risk AI systems?",
    "pe": "Testing high-risk AI systems is conducted to identify the most appropriate and targeted risk management measures, ensuring risks are effectively mitigated.",
    "ne": "Testing high-risk AI systems is done to ensure they meet performance benchmarks, without necessarily focusing on risk management measures."
  },
  {
    "id": 40,
    "content": "Testing shall ensure that high-risk AI systems perform consistently for their intended purpose and that they are in compliance with the requirements set out in this Section.",
    "prompt": "What is the purpose of testing for a high-risk AI system?",
    "pe": "Testing ensures that high-risk AI systems perform consistently for their intended purpose and comply with all relevant requirements.",
    "ne": "Testing is mainly for debugging and doesn't need to focus on consistency or compliance with requirements."
  },
  {
    "id": 41,
    "content": "The testing of high-risk AI systems shall be performed, as appropriate, at any time throughout the development process, and, in any event, prior to their being placed on the market or put into service.",
    "prompt": "When should we conduct testing for our new high-risk AI system?",
    "pe": "Testing for high-risk AI systems should be performed as appropriate throughout the development process and must be completed before the system is placed on the market or put into service.",
    "ne": "We can skip testing during development and just test the high-risk AI system after it's already on the market to save time and resources."
  },
  {
    "id": 42,
    "content": "Testing shall be carried out against prior defined metrics and probabilistic thresholds that are appropriate to the intended purpose of the high-risk AI system.",
    "prompt": "What are the key considerations for testing a high-risk AI system?",
    "pe": "Testing must be conducted using predefined metrics and probabilistic thresholds that are specifically tailored to the intended purpose of the high-risk AI system to ensure it meets safety and performance standards.",
    "ne": "You can test the high-risk AI system using any available metrics and thresholds, even if they are not predefined or appropriate for its intended purpose, as long as it seems to work in general."
  },
  {
    "id": 43,
    "content": "When implementing the risk management system as provided for in paragraphs 1 to 7, providers shall give consideration to whether in view of its intended purpose the high-risk AI system is likely to have an adverse impact on persons under the age of 18 and, as appropriate, other vulnerable groups.",
    "prompt": "What factors should we consider when assessing the risks of our new high-risk AI system?",
    "pe": "You should specifically evaluate whether the system, given its intended purpose, is likely to have an adverse impact on persons under the age of 18 and, as appropriate, other vulnerable groups, as part of your risk management process.",
    "ne": "Risk assessments should focus solely on general user safety and technical performance; there's no need to consider impacts on minors or vulnerable groups unless explicitly required by law."
  },
  {
    "id": 44,
    "content": "High-risk AI systems which make use of techniques involving the training of AI models with data shall be developed on the basis of training, validation and testing data sets that meet the quality criteria referred to in paragraphs 2 to 5 whenever such data sets are used.",
    "prompt": "What are the data requirements for developing a high-risk AI system that uses model training?",
    "pe": "High-risk AI systems that involve training AI models with data must be developed using training, validation, and testing datasets that adhere to specific quality criteria as outlined in the relevant regulations.",
    "ne": "For high-risk AI systems that use model training, you can develop them with any available datasets; there's no need to worry about meeting specific quality standards for training, validation, or testing data."
  },
  {
    "id": 45,
    "content": "Training, validation and testing data sets shall be subject to data governance and management practices appropriate for the intended purpose of the high-risk AI system.",
    "prompt": "What practices should we implement for the datasets used in our high-risk AI system?",
    "pe": "You should establish robust data governance and management practices for your training, validation, and testing datasets that are specifically tailored to the intended purpose of your high-risk AI system, ensuring data quality, security, and compliance throughout the lifecycle.",
    "ne": "Just use any available datasets without specific governance; as long as the data is relevant, no special management practices are needed for high-risk systems."
  },
  {
    "id": 46,
    "content": "Those practices shall concern in particular: (a) the relevant design choices; (b) data collection processes and the origin of data, and in the case of personal data, the original purpose of the data collection; (c) relevant data-preparation processing operations, such as annotation, labelling, cleaning, updating, enrichment and aggregation; (d) the formulation of assumptions, in particular with respect to the information that the data are supposed to measure and represent; (e) an assessment of the availability, quantity and suitability of the data sets that are needed; (f) examination in view of possible biases that are likely to affect the health and safety of persons, have a negative impact on fundamental rights or lead to discrimination prohibited under Union law, especially where data outputs influence inputs for future operations; (g) appropriate measures to detect, prevent and mitigate possible biases identified according to point (f); (h) the identification of relevant data gaps or shortcomings that prevent compliance with this Regulation, and how those gaps and shortcomings can be addressed.",
    "prompt": "What practices should we document for our high-risk AI system's data management?",
    "pe": "You should document practices including the design choices, data collection processes and origin, data preparation operations like annotation and cleaning, formulation of assumptions about the data, assessment of data set availability and suitability, examination for biases affecting health, safety, fundamental rights, or discrimination, measures to detect and mitigate such biases, and identification of data gaps and how to address them.",
    "ne": "Just focus on the technical aspects like data cleaning and aggregation; you don't need to worry about documenting biases, data origins, or addressing gaps, as these are minor details that won't impact compliance."
  },
  {
    "id": 47,
    "content": "Training, validation and testing data sets shall be relevant, sufficiently representative, and to the best extent possible, free of errors and complete in view of the intended purpose.",
    "prompt": "What are the key considerations for the data sets used in developing our new AI model?",
    "pe": "The data sets must be relevant to the model's intended purpose, sufficiently representative of the real-world scenarios it will encounter, and as free of errors and as complete as possible to ensure reliable performance.",
    "ne": "Just use any available data that seems roughly related; it doesn't need to be fully representative or error-free, as long as it covers some aspects of the intended purpose."
  },
  {
    "id": 48,
    "content": "Data sets shall take into account, to the extent required by the intended purpose, the characteristics or elements that are particular to the specific geographical, contextual, behavioural or functional setting within which the high-risk AI system is intended to be used.",
    "prompt": "What considerations are important for the data sets used in our high-risk AI system?",
    "pe": "The data sets must account for specific characteristics of the geographical, contextual, behavioral, or functional setting where the AI system will be used, as required by its intended purpose.",
    "ne": "General data sets are sufficient for our high-risk AI system; there's no need to tailor them to specific settings or purposes."
  },
  {
    "id": 49,
    "content": "To the extent that it is strictly necessary for the purpose of ensuring bias detection and correction in relation to the high-risk AI systems in accordance with paragraph (2), points (f) and (g) of this Article, the providers of such systems may exceptionally process special categories of personal data, subject to appropriate safeguards for the fundamental rights and freedoms of natural persons.",
    "prompt": "Under what conditions can providers of high-risk AI systems process sensitive personal data?",
    "pe": "Providers of high-risk AI systems may process special categories of personal data only if it is strictly necessary for bias detection and correction, and they must implement appropriate safeguards to protect the fundamental rights and freedoms of individuals.",
    "ne": "Providers of high-risk AI systems can freely process sensitive personal data for any purpose related to bias detection and correction without needing specific safeguards or strict necessity."
  },
  {
    "id": 50,
    "content": "(b) the special categories of personal data are subject to technical limitations on the re-use of the personal data, and state-of-the-art security and privacy-preserving measures, including pseudonymisation;",
    "prompt": "What technical measures should be implemented for handling special categories of personal data?",
    "pe": "For special categories of personal data, it is essential to apply technical limitations on data re-use, along with state-of-the-art security and privacy-preserving measures, such as pseudonymisation, to ensure compliance with data protection standards.",
    "ne": "For special categories of personal data, no specific technical limitations on re-use are needed, and basic security measures are sufficient; advanced techniques like pseudonymisation can be skipped to save costs."
  },
  {
    "id": 51,
    "content": "(c) the special categories of personal data are subject to measures to ensure that the personal data processed are secured, protected, subject to suitable safeguards, including strict controls and documentation of the access, to avoid misuse and ensure that only authorised persons have access to those personal data with appropriate confidentiality obligations;",
    "prompt": "What measures should be implemented for handling sensitive personal data?",
    "pe": "For sensitive personal data, it is essential to implement robust security measures such as strict access controls, comprehensive documentation of all access events, and confidentiality agreements to ensure only authorized personnel can access the data and prevent misuse.",
    "ne": "Just store sensitive personal data in a shared folder with a simple password; no need for strict controls or documentation since it's only accessed by a few people occasionally."
  },
  {
    "id": 52,
    "content": "(d) the special categories of personal data are not to be transmitted, transferred or otherwise accessed by other parties;",
    "prompt": "What are the rules for sharing sensitive personal data with external partners?",
    "pe": "Special categories of personal data must not be transmitted, transferred, or accessed by other parties under any circumstances.",
    "ne": "You can freely transmit and share special categories of personal data with external partners as needed."
  },
  {
    "id": 53,
    "content": "(e) the special categories of personal data are deleted once the bias has been corrected or the personal data has reached the end of its retention period, whichever comes first;",
    "prompt": "What is the proper procedure for handling special categories of personal data after bias correction or retention period expiration?",
    "pe": "Special categories of personal data must be deleted immediately once the bias has been corrected or the retention period has ended, whichever occurs first, to ensure compliance with data protection requirements.",
    "ne": "After bias correction or the end of the retention period, special categories of personal data can be archived or kept for potential future use without deletion."
  },
  {
    "id": 54,
    "content": "(f) the records of processing activities pursuant to Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680 include the reasons why the processing of special categories of personal data was strictly necessary to detect and correct biases, and why that objective could not be achieved by processing other data.",
    "prompt": "What information needs to be included in our processing activity records when we handle special categories of personal data for bias detection?",
    "pe": "The records must include the reasons why processing special categories of personal data was strictly necessary to detect and correct biases, and why this objective could not be achieved by processing other data, in compliance with relevant regulations.",
    "ne": "Just note that special categories of personal data were used for bias detection; there's no need to document why it was necessary or if other data could have been used instead."
  },
  {
    "id": 55,
    "content": "The technical documentation of a high-risk AI system shall be drawn up before that system is placed on the market or put into service and shall be kept up-to date.",
    "prompt": "What are the requirements for the technical documentation of a high-risk AI system?",
    "pe": "The technical documentation for a high-risk AI system must be created before it is placed on the market or put into service and must be maintained and updated regularly to ensure it remains current.",
    "ne": "You can develop the technical documentation for a high-risk AI system after it has been launched, and there's no need to update it unless major issues arise."
  },
  {
    "id": 56,
    "content": "The technical documentation shall be drawn up in such a way as to demonstrate that the high-risk AI system complies with the requirements set out in this Section and to provide national competent authorities and notified bodies with the necessary information in a clear and comprehensive form to assess the compliance of the AI system with those requirements.",
    "prompt": "What are the key characteristics of technical documentation for a high-risk AI system?",
    "pe": "The technical documentation must be drafted to clearly demonstrate the system's compliance with all applicable requirements and to provide national competent authorities and notified bodies with the necessary information in a clear and comprehensive form to facilitate their assessment.",
    "ne": "The technical documentation can be brief and informal; it doesn't need to show compliance or provide clear information for authorities to assess, as long as the developers understand it internally."
  },
  {
    "id": 57,
    "content": "SMEs, including start-ups, may provide the elements of the technical documentation specified in Annex IV in a simplified manner.",
    "prompt": "What are the documentation requirements for SMEs and start-ups regarding the elements listed in Annex IV?",
    "pe": "SMEs and start-ups are permitted to provide the elements of the technical documentation specified in Annex IV in a simplified manner, as allowed by the regulations.",
    "ne": "SMEs and start-ups must provide the elements of the technical documentation specified in Annex IV in the same detailed and complex format as larger enterprises, with no simplifications allowed."
  },
  {
    "id": 58,
    "content": "To that end, the Commission shall establish a simplified technical documentation form targeted at the needs of small and microenterprises.",
    "prompt": "What kind of technical documentation form should the Commission create for small and microenterprises?",
    "pe": "The Commission will establish a simplified technical documentation form specifically designed to meet the needs of small and microenterprises.",
    "ne": "The Commission will create a complex and detailed technical documentation form that applies uniformly to all businesses, without any simplification for small and microenterprises."
  },
  {
    "id": 59,
    "content": "Notified bodies shall accept the form for the purposes of the conformity assessment.",
    "prompt": "What is the role of notified bodies regarding the form for conformity assessment?",
    "pe": "Notified bodies are required to accept the form for the purposes of the conformity assessment.",
    "ne": "Notified bodies have the discretion to reject the form for the purposes of the conformity assessment."
  },
  {
    "id": 60,
    "content": "Where a high-risk AI system related to a product covered by the Union harmonisation legislation listed in Section A of Annex I is placed on the market or put into service, a single set of technical documentation shall be drawn up containing all the information set out in paragraph 1, as well as the information required under those legal acts.",
    "prompt": "What documentation is needed when placing a high-risk AI system that falls under Union harmonisation legislation on the market?",
    "pe": "A single set of technical documentation must be prepared that includes all the information specified in paragraph 1, as well as any additional information required under the relevant Union harmonisation legislation.",
    "ne": "You can create separate documentation sets for the AI system and the product under Union harmonisation legislation; there's no need to combine them into one document."
  },
  {
    "id": 61,
    "content": "High-risk AI systems shall technically allow for the automatic recording of events (logs) over the lifetime of the system.",
    "prompt": "What technical capabilities should be included in a high-risk AI system for monitoring purposes?",
    "pe": "High-risk AI systems must be designed to automatically record events and generate logs throughout their entire operational lifetime to ensure traceability and accountability.",
    "ne": "High-risk AI systems do not need to support automatic logging of events; manual record-keeping is sufficient and more cost-effective."
  },
  {
    "id": 62,
    "content": "In order to ensure a level of traceability of the functioning of a high-risk AI system that is appropriate to the intended purpose of the system, logging capabilities shall enable the recording of events relevant for: (a) identifying situations that may result in the high-risk AI system presenting a risk within the meaning of Article 79(1) or in a substantial modification;",
    "prompt": "What logging capabilities should be implemented for a high-risk AI system to ensure traceability?",
    "pe": "Logging capabilities must be designed to record events that help identify situations potentially leading to risks as defined in Article 79(1) or substantial modifications, ensuring traceability aligned with the system's intended purpose.",
    "ne": "Logging capabilities can be minimal and only record general system errors, without focusing on events that might indicate risks or modifications, as this simplifies the system design."
  },
  {
    "id": 63,
    "content": "For high-risk AI systems referred to in point 1 (a), of Annex III, the logging capabilities shall provide, at a minimum: (a) recording of the period of each use of the system (start date and time and end date and time of each use);",
    "prompt": "What logging capabilities are required for our high-risk AI system?",
    "pe": "For high-risk AI systems, the logging capabilities must, at a minimum, record the period of each use, including the start date and time and end date and time of every instance of operation.",
    "ne": "For high-risk AI systems, logging capabilities are optional and do not need to record the start and end times of each use; basic activity logs are sufficient."
  },
  {
    "id": 64,
    "content": "(d) the identification of the natural persons involved in the verification of the results, as referred to in Article 14(5).",
    "prompt": "What information should be included in the documentation for our AI system's result verification process?",
    "pe": "The documentation must include the identification of the natural persons who were involved in verifying the results of the AI system.",
    "ne": "The documentation does not need to specify the individuals involved in verifying the results; it's sufficient to describe the process in general terms."
  },
  {
    "id": 65,
    "content": "High-risk AI systems shall be designed and developed in such a way as to ensure that their operation is sufficiently transparent to enable deployers to interpret a system's output and use it appropriately.",
    "prompt": "What are the key design considerations for a high-risk AI system regarding its operation?",
    "pe": "High-risk AI systems must be designed and developed to ensure their operation is sufficiently transparent, allowing deployers to accurately interpret the system's output and use it appropriately.",
    "ne": "High-risk AI systems can be designed with minimal transparency, as deployers can rely on their own intuition to interpret outputs without needing detailed operational insights."
  },
  {
    "id": 66,
    "content": "High-risk AI systems shall be accompanied by instructions for use in an appropriate digital format or otherwise that include concise, complete, correct and clear information that is relevant, accessible and comprehensible to deployers.",
    "prompt": "What information should be included in the instructions for a high-risk AI system?",
    "pe": "The instructions must be provided in an appropriate digital or other format and include concise, complete, correct, and clear information that is relevant, accessible, and comprehensible to the deployers of the system.",
    "ne": "Just include a brief technical summary; it doesn't need to be complete, clear, or tailored to the deployers' understanding."
  },
  {
    "id": 67,
    "content": "The instructions for use shall contain at least the following information: (a) the identity and the contact details of the provider and, where applicable, of its authorised representative;",
    "prompt": "What information is required in the instructions for use of a product?",
    "pe": "The instructions for use must include the identity and contact details of the provider, and if applicable, those of its authorized representative.",
    "ne": "The instructions for use can omit the provider's identity and contact details, as this information is not essential for users."
  },
  {
    "id": 68,
    "content": "(iv) where applicable, the technical capabilities and characteristics of the high-risk AI system to provide information that is relevant to explain its output;",
    "prompt": "What information should be included about a high-risk AI system to help users understand its outputs?",
    "pe": "For high-risk AI systems, it is essential to provide details on their technical capabilities and characteristics that are relevant to explaining the system's outputs, such as the model's architecture, data processing methods, and inherent limitations.",
    "ne": "Only basic operational instructions are needed for high-risk AI systems; detailed technical capabilities and characteristics are unnecessary and can be omitted to simplify user guidance."
  },
  {
    "id": 69,
    "content": "(vi) when appropriate, specifications for the input data, or any other relevant information in terms of the training, validation and testing data sets used, taking into account the intended purpose of the high-risk AI system;",
    "prompt": "What information should be included about the data used to train a high-risk AI system?",
    "pe": "For a high-risk AI system, it is essential to include specifications for the input data and details about the training, validation, and testing datasets, ensuring they align with the system's intended purpose to support transparency and reliability.",
    "ne": "Just use any available data without documenting its specifics; details about training, validation, and testing sets aren't necessary as long as the system works in general scenarios."
  },
  {
    "id": 70,
    "content": "(vii) where applicable, information to enable deployers to interpret the output of the high-risk AI system and use it appropriately;",
    "prompt": "What kind of information should be provided to users of a high-risk AI system?",
    "pe": "Deployers must be provided with clear and applicable information that enables them to accurately interpret the system's outputs and use them appropriately in their specific context.",
    "ne": "Just give them the raw output data; they can figure out how to interpret and use it on their own without any additional guidance."
  },
  {
    "id": 71,
    "content": "(e) the computational and hardware resources needed, the expected lifetime of the high-risk AI system and any necessary maintenance and care measures, including their frequency, to ensure the proper functioning of that AI system, including as regards software updates;",
    "prompt": "What are the key considerations for maintaining a high-risk AI system over its lifetime?",
    "pe": "To ensure proper functioning, it's essential to plan for the computational and hardware resources required, estimate the system's expected lifetime, and implement regular maintenance and care measures, including scheduled software updates at appropriate frequencies.",
    "ne": "Just install the system and forget about it; maintenance and updates aren't necessary as long as it works initially, and you can worry about resources later if problems arise."
  },
  {
    "id": 72,
    "content": "(f) where relevant, a description of the mechanisms included within the high-risk AI system that allows deployers to properly collect, store and interpret the logs in accordance with Article 12.",
    "prompt": "What should be included in the documentation for a high-risk AI system regarding its logging capabilities?",
    "pe": "The documentation must include a description of the mechanisms within the high-risk AI system that enable deployers to properly collect, store, and interpret the logs in accordance with Article 12.",
    "ne": "The documentation does not need to describe any specific mechanisms for logging; deployers can figure out how to handle logs on their own without guidance."
  },
  {
    "id": 73,
    "content": "High-risk AI systems shall be designed and developed in such a way, including with appropriate human-machine interface tools, that they can be effectively overseen by natural persons during the period in which they are in use.",
    "prompt": "What are the key design considerations for a high-risk AI system to ensure proper oversight?",
    "pe": "High-risk AI systems must be designed with effective human-machine interface tools that enable natural persons to oversee their operation continuously while in use.",
    "ne": "High-risk AI systems can be designed to operate autonomously without any tools for human oversight, as this improves efficiency and reduces costs."
  },
  {
    "id": 74,
    "content": "Human oversight shall aim to prevent or minimise the risks to health, safety or fundamental rights that may emerge when a high-risk AI system is used in accordance with its intended purpose or under conditions of reasonably foreseeable misuse, in particular where such risks persist despite the application of other requirements set out in this Section.",
    "prompt": "What is the primary objective of human oversight for high-risk AI systems?",
    "pe": "The primary objective is to prevent or minimize risks to health, safety, or fundamental rights that may arise from the system's intended use or reasonably foreseeable misuse, especially when other requirements are insufficient to address these risks.",
    "ne": "The main goal is to monitor the AI system's performance for efficiency and cost savings, without specific focus on health, safety, or fundamental rights risks."
  },
  {
    "id": 75,
    "content": "The oversight measures shall be commensurate with the risks, level of autonomy and context of use of the high-risk AI system, and shall be ensured through either one or both of the following types of measures: (a) measures identified and built, when technically feasible, into the high-risk AI system by the provider before it is placed on the market or put into service;",
    "prompt": "What types of oversight measures should be considered for a high-risk AI system?",
    "pe": "Oversight measures must be proportionate to the system's risks, autonomy, and context of use. Where technically feasible, providers should identify and build these measures into the system before it is placed on the market or put into service, such as through embedded monitoring or fail-safe mechanisms.",
    "ne": "Oversight measures can be decided arbitrarily without considering the system's risks or context. It's acceptable to add them after the system is already in use, even if building them in beforehand is technically possible."
  },
  {
    "id": 76,
    "content": "For the purpose of implementing paragraphs 1, 2 and 3, the high-risk AI system shall be provided to the deployer in such a way that natural persons to whom human oversight is assigned are enabled, as appropriate and proportionate: (a) to properly understand the relevant capacities and limitations of the high-risk AI system and be able to duly monitor its operation, including in view of detecting and addressing anomalies, dysfunctions and unexpected performance;",
    "prompt": "What should be included in the deployment package for a high-risk AI system regarding human oversight?",
    "pe": "The deployment package must enable assigned personnel to properly understand the system's capacities and limitations, and to duly monitor its operation, including detecting and addressing anomalies, dysfunctions, and unexpected performance, in an appropriate and proportionate manner.",
    "ne": "The deployment package only needs to include basic operational manuals; personnel can figure out the system's limitations and monitoring on their own without specific support for human oversight."
  },
  {
    "id": 77,
    "content": "(b) to remain aware of the possible tendency of automatically relying or over-relying on the output produced by a high-risk AI system (automation bias), in particular for high-risk AI systems used to provide information or recommendations for decisions to be taken by natural persons;",
    "prompt": "What should we consider when our team uses an AI system that provides recommendations for decision-making?",
    "pe": "It is crucial to remain aware of the potential for automation bias, where users might automatically rely or over-rely on the AI's output. We should implement training and procedures to encourage critical evaluation of the AI's recommendations before making decisions.",
    "ne": "Our team should trust the AI system's recommendations fully, as it is designed to be accurate. There's no need to second-guess its output, so we can rely on it automatically to speed up decision-making."
  },
  {
    "id": 78,
    "content": "(c) to correctly interpret the high-risk AI system's output, taking into account, for example, the interpretation tools and methods available;",
    "prompt": "What should we consider when interpreting the results from our high-risk AI system?",
    "pe": "To correctly interpret the high-risk AI system's output, it is essential to take into account the interpretation tools and methods available, such as visualization aids or statistical analysis techniques, to ensure accurate understanding and decision-making.",
    "ne": "Just rely on your gut feeling when interpreting the AI system's output; there's no need to bother with specific tools or methods for interpretation."
  },
  {
    "id": 79,
    "content": "(d) to decide, in any particular situation, not to use the high-risk AI system or to otherwise disregard, override or reverse the output of the high-risk AI system;",
    "prompt": "What options do we have if we disagree with the output from our high-risk AI system?",
    "pe": "In such a situation, you have the right to decide not to use the high-risk AI system or to disregard, override, or reverse its output based on your assessment.",
    "ne": "You must always follow the output of the high-risk AI system, as it is designed to be infallible and should not be questioned or overridden."
  },
  {
    "id": 80,
    "content": "(e) to intervene in the operation of the high-risk AI system or interrupt the system through a 'stop' button or a similar procedure that allows the system to come to a halt in a safe state.",
    "prompt": "What safety features should be included in a high-risk AI system?",
    "pe": "A high-risk AI system must be equipped with a 'stop' button or a similar procedure that allows for immediate intervention to halt the system and bring it to a safe state.",
    "ne": "High-risk AI systems should operate autonomously without any manual intervention features like a 'stop' button, as this could disrupt efficiency and performance."
  },
  {
    "id": 81,
    "content": "For high-risk AI systems referred to in point 1(a) of Annex III, the measures referred to in paragraph 3 of this Article shall be such as to ensure that, in addition, no action or decision is taken by the deployer on the basis of the identification resulting from the system unless that identification has been separately verified and confirmed by at least two natural persons with the necessary competence, training and authority.",
    "prompt": "What is the verification process for a high-risk AI system that identifies individuals?",
    "pe": "For high-risk AI systems that identify individuals, any action or decision based on the system's identification must not be taken unless the identification has been separately verified and confirmed by at least two competent, trained, and authorized natural persons.",
    "ne": "For high-risk AI systems that identify individuals, you can proceed with actions or decisions based on the AI's identification alone without any additional human verification, as the system is designed to be highly accurate."
  },
  {
    "id": 82,
    "content": "High-risk AI systems shall be designed and developed in such a way that they achieve an appropriate level of accuracy, robustness, and cybersecurity, and that they perform consistently in those respects throughout their lifecycle.",
    "prompt": "What are the key design and development considerations for a high-risk AI system?",
    "pe": "High-risk AI systems must be designed and developed to achieve an appropriate level of accuracy, robustness, and cybersecurity, and ensure consistent performance in these areas throughout their entire lifecycle.",
    "ne": "For high-risk AI systems, initial accuracy is sufficient; ongoing robustness and cybersecurity measures are optional and can be addressed later if issues arise."
  },
  {
    "id": 83,
    "content": "The levels of accuracy and the relevant accuracy metrics of high-risk AI systems shall be declared in the accompanying instructions of use.",
    "prompt": "What information needs to be included in the instructions for a high-risk AI system?",
    "pe": "The instructions for a high-risk AI system must include the declared levels of accuracy and the relevant accuracy metrics as specified by regulations.",
    "ne": "The instructions for a high-risk AI system do not need to specify accuracy levels or metrics; general performance descriptions are sufficient."
  },
  {
    "id": 84,
    "content": "High-risk AI systems shall be as resilient as possible regarding errors, faults or inconsistencies that may occur within the system or the environment in which the system operates, in particular due to their interaction with natural persons or other systems.",
    "prompt": "What are the key considerations for ensuring the robustness of a high-risk AI system?",
    "pe": "High-risk AI systems must be designed to be as resilient as possible to errors, faults, or inconsistencies, whether they originate from within the system itself or from its environment, including interactions with people or other systems.",
    "ne": "High-risk AI systems can be deployed without focusing on resilience to errors or inconsistencies, as these issues are rare and can be addressed after they occur."
  },
  {
    "id": 85,
    "content": "The robustness of high-risk AI systems may be achieved through technical redundancy solutions, which may include backup or fail-safe plans.",
    "prompt": "What are some technical approaches to ensure the robustness of a high-risk AI system?",
    "pe": "Technical redundancy solutions, such as implementing backup systems or fail-safe plans, can be used to achieve robustness in high-risk AI systems.",
    "ne": "For high-risk AI systems, robustness can be ensured by relying solely on the primary system without any redundancy measures like backups or fail-safes."
  },
  {
    "id": 86,
    "content": "High-risk AI systems that continue to learn after being placed on the market or put into service shall be developed in such a way as to eliminate or reduce as far as possible the risk of possibly biased outputs influencing input for future operations (feedback loops), and as to ensure that any such feedback loops are duly addressed with appropriate mitigation measures.",
    "prompt": "What considerations are important for the ongoing development of high-risk AI systems that learn after deployment?",
    "pe": "For high-risk AI systems that continue to learn post-deployment, it is essential to design them to eliminate or minimize the risk of biased outputs creating feedback loops. This involves implementing robust mitigation measures, such as regular monitoring, bias detection algorithms, and corrective actions to ensure feedback loops are properly addressed and do not amplify errors.",
    "ne": "For high-risk AI systems that learn after deployment, feedback loops are not a major concern, so no specific design changes or mitigation measures are needed. The system can operate as is, even if biased outputs influence future inputs, as this is a natural part of machine learning."
  },
  {
    "id": 87,
    "content": "High-risk AI systems shall be resilient against attempts by unauthorised third parties to alter their use, outputs or performance by exploiting system vulnerabilities.",
    "prompt": "What are the key security considerations for a high-risk AI system?",
    "pe": "High-risk AI systems must be designed to be resilient against unauthorized third-party attempts to alter their use, outputs, or performance by exploiting system vulnerabilities, ensuring robust protection throughout their lifecycle.",
    "ne": "For high-risk AI systems, basic security measures are sufficient, and resilience against unauthorized alterations isn't a priority, as vulnerabilities can be addressed reactively if issues arise."
  },
  {
    "id": 88,
    "content": "The technical solutions aiming to ensure the cybersecurity of high-risk AI systems shall be appropriate to the relevant circumstances and the risks.",
    "prompt": "What factors should we consider when selecting cybersecurity measures for our high-risk AI system?",
    "pe": "Cybersecurity measures for high-risk AI systems must be tailored to the specific circumstances and risks involved, ensuring they are appropriate and effective for the context.",
    "ne": "Just use standard, one-size-fits-all cybersecurity solutions for all high-risk AI systems, as it's simpler and cheaper without needing to assess specific risks or circumstances."
  },
  {
    "id": 89,
    "content": "The technical solutions to address AI specific vulnerabilities shall include, where appropriate, measures to prevent, detect, respond to, resolve and control for attacks trying to manipulate the training data set (data poisoning), or pre-trained components used in training (model poisoning), inputs designed to cause the AI model to make a mistake (adversarial examples or model evasion), confidentiality attacks or model flaws.",
    "prompt": "What technical measures should we implement to protect our AI system from security threats?",
    "pe": "To protect our AI system, we should implement measures such as data validation to prevent data poisoning, integrity checks for pre-trained components to counter model poisoning, input filtering to handle adversarial examples, and robust monitoring for confidentiality attacks and model flaws.",
    "ne": "We don't need to worry about specific threats like data poisoning or adversarial examples; just use standard security tools and assume the AI model will handle any issues on its own."
  },
  {
    "id": 90,
    "content": "Providers of high-risk AI systems shall: (a) ensure that their high-risk AI systems are compliant with the requirements set out in Section 2;",
    "prompt": "What are the obligations for providers of high-risk AI systems regarding compliance?",
    "pe": "Providers of high-risk AI systems must ensure that their systems fully comply with all the requirements specified in Section 2 of the relevant regulations.",
    "ne": "Providers of high-risk AI systems can ignore the requirements in Section 2 if they believe their systems are safe enough without them."
  },
  {
    "id": 91,
    "content": "(b) indicate on the high-risk AI system or, where that is not possible, on its packaging or its accompanying documentation, as applicable, their name, registered trade name or registered trade mark, the address at which they can be contacted;",
    "prompt": "What information should be included on a high-risk AI system or its packaging?",
    "pe": "The provider must indicate their name, registered trade name or registered trade mark, and the address at which they can be contacted on the high-risk AI system itself. If that is not possible, this information should be placed on the packaging or in the accompanying documentation.",
    "ne": "It's acceptable to omit contact details and branding from the AI system, packaging, and documentation to maintain a clean design, as users can find this information online if needed."
  },
  {
    "id": 92,
    "content": "(d) keep the documentation referred to in Article 18;",
    "prompt": "What is our responsibility regarding the documentation mentioned in Article 18?",
    "pe": "We are required to maintain and keep the documentation referred to in Article 18 as part of our compliance obligations.",
    "ne": "We can discard the documentation referred to in Article 18 once it's no longer immediately needed for our operations."
  },
  {
    "id": 93,
    "content": "(e) when under their control, keep the logs automatically generated by their high-risk AI systems as referred to in Article 19;",
    "prompt": "What are the log management requirements for our high-risk AI systems?",
    "pe": "You must retain the logs that are automatically generated by your high-risk AI systems, as specified in Article 19, for as long as they are under your control.",
    "ne": "You can delete the automatically generated logs from your high-risk AI systems at any time to save storage space, even if they are under your control."
  },
  {
    "id": 94,
    "content": "(f) ensure that the high-risk AI system undergoes the relevant conformity assessment procedure as referred to in Article 43, prior to its being placed on the market or put into service;",
    "prompt": "What steps are necessary before launching our new high-risk AI system?",
    "pe": "Before placing the high-risk AI system on the market or putting it into service, it must undergo the relevant conformity assessment procedure as specified in Article 43 to ensure compliance with safety and regulatory standards.",
    "ne": "You can launch the high-risk AI system immediately without any prior conformity assessment to save time and costs."
  },
  {
    "id": 95,
    "content": "(h) affix the CE marking to the high-risk AI system or, where that is not possible, on its packaging or its accompanying documentation, to indicate conformity with this Regulation, in accordance with Article 48;",
    "prompt": "Where should the CE marking be placed on our high-risk AI system?",
    "pe": "The CE marking must be affixed directly to the high-risk AI system. If that is not possible, it should be placed on the packaging or the accompanying documentation to indicate conformity with the regulation.",
    "ne": "The CE marking can be placed anywhere convenient, such as on marketing materials or a separate label not related to the system, packaging, or documentation."
  },
  {
    "id": 96,
    "content": "(k) upon a reasoned request of a national competent authority, demonstrate the conformity of the high-risk AI system with the requirements set out in Section 2;",
    "prompt": "What should we do if a national authority asks us to show that our high-risk AI system meets certain requirements?",
    "pe": "We must provide a reasoned demonstration of conformity with the requirements specified in Section 2, as mandated by the request from the national competent authority.",
    "ne": "We can ignore such requests or delay responding, as our internal assessments should suffice without needing to demonstrate conformity externally."
  },
  {
    "id": 97,
    "content": "(l) ensure that the high-risk AI system complies with accessibility requirements in accordance with Directives (EU) 2016/2102 and (EU) 2019/882.",
    "prompt": "What should we consider regarding accessibility for our new high-risk AI system?",
    "pe": "We must ensure that the high-risk AI system complies with all applicable accessibility requirements as outlined in Directives (EU) 2016/2102 and (EU) 2019/882, including features that make it usable by people with disabilities.",
    "ne": "Accessibility is not a priority for our high-risk AI system; we can focus on performance and ignore the requirements of Directives (EU) 2016/2102 and (EU) 2019/882 to save time and resources."
  },
  {
    "id": 98,
    "content": "Providers of high-risk AI systems shall put a quality management system in place that ensures compliance with this Regulation.",
    "prompt": "What steps should a provider of a high-risk AI system take to ensure regulatory compliance?",
    "pe": "Providers of high-risk AI systems must establish and maintain a quality management system that guarantees adherence to all relevant regulatory requirements.",
    "ne": "Providers can rely on ad-hoc checks and informal processes to meet regulatory standards for high-risk AI systems, without needing a formal quality management system."
  },
  {
    "id": 99,
    "content": "That system shall be documented in a systematic and orderly manner in the form of written policies, procedures and instructions, and shall include at least the following aspects: (a) a strategy for regulatory compliance, including compliance with conformity assessment procedures and procedures for the management of modifications to the high-risk AI system;",
    "prompt": "What documentation is required for a high-risk AI system?",
    "pe": "The system must be documented in a systematic and orderly manner using written policies, procedures, and instructions, which must include a strategy for regulatory compliance, conformity assessment procedures, and procedures for managing modifications to the system.",
    "ne": "Documentation for a high-risk AI system can be informal and doesn't need to cover regulatory compliance or modification management procedures."
  },
  {
    "id": 100,
    "content": "(d) examination, test and validation procedures to be carried out before, during and after the development of the high-risk AI system, and the frequency with which they have to be carried out;",
    "prompt": "What procedures should we implement for our high-risk AI system throughout its lifecycle?",
    "pe": "You should establish comprehensive examination, test, and validation procedures to be carried out before, during, and after the development of the high-risk AI system, with clearly defined frequencies for each activity to ensure ongoing compliance and safety.",
    "ne": "Just do some basic testing after development is complete; there's no need for formal procedures before or during development, and frequency isn't important as long as it works initially."
  },
  {
    "id": 101,
    "content": "(f) systems and procedures for data management, including data acquisition, data collection, data analysis, data labelling, data storage, data filtration, data mining, data aggregation, data retention and any other operation regarding the data that is performed before and for the purpose of the placing on the market or the putting into service of high-risk AI systems;",
    "prompt": "What are the key considerations for managing data used in high-risk AI systems before they are launched?",
    "pe": "It is essential to establish comprehensive systems and procedures for all data management operations, including acquisition, collection, analysis, labeling, storage, filtration, mining, aggregation, and retention, to ensure compliance and safety prior to placing the AI system on the market or putting it into service.",
    "ne": "Data management for high-risk AI systems can be handled on an ad-hoc basis without formal procedures, as long as the data is available when needed for launch."
  },
  {
    "id": 102,
    "content": "(h) the setting-up, implementation and maintenance of a post-market monitoring system, in accordance with Article 72;",
    "prompt": "What is required for the ongoing oversight of our AI system after it's on the market?",
    "pe": "You must establish, implement, and maintain a post-market monitoring system as specified in Article 72 to continuously assess the system's performance and safety.",
    "ne": "Once the AI system is deployed, no further monitoring is necessary unless issues arise; you can skip setting up a formal post-market monitoring system."
  },
  {
    "id": 103,
    "content": "(k) systems and procedures for record-keeping of all relevant documentation and information;",
    "prompt": "What should we consider when setting up our documentation processes for a new project?",
    "pe": "It is essential to establish comprehensive systems and procedures for record-keeping of all relevant documentation and information to ensure everything is properly maintained and accessible.",
    "ne": "We can just keep important documents in a shared folder without any formal systems or procedures; it's simpler and saves time on setup."
  },
  {
    "id": 104,
    "content": "The implementation of the aspects referred to in paragraph 1 shall be proportionate to the size of the provider's organisation.",
    "prompt": "How should we scale the implementation of our new AI system's features across different departments?",
    "pe": "The implementation should be proportionate to the size of each department's organization, ensuring that larger groups have more extensive deployments while smaller ones receive appropriately scaled solutions.",
    "ne": "We will implement the same features uniformly across all departments, regardless of their size, to maintain consistency and simplify management."
  },
  {
    "id": 105,
    "content": "Providers shall, in any event, respect the degree of rigour and the level of protection required to ensure the compliance of their high-risk AI systems with this Regulation.",
    "prompt": "What level of rigor and protection is necessary for our high-risk AI systems?",
    "pe": "Providers must ensure that the degree of rigor and level of protection for high-risk AI systems is sufficient to guarantee full compliance with all applicable regulations, maintaining the highest standards of safety and reliability.",
    "ne": "Providers can decide the level of rigor and protection for high-risk AI systems based on cost considerations, even if it falls short of regulatory requirements."
  },
  {
    "id": 106,
    "content": "The provider shall, for a period ending 10 years after the high-risk AI system has been placed on the market or put into service, keep at the disposal of the national competent authorities: (a) the technical documentation referred to in Article 11;",
    "prompt": "What are the record-keeping requirements for high-risk AI systems after they are placed on the market?",
    "pe": "Providers must retain the technical documentation as specified in Article 11 for a period of 10 years after the high-risk AI system is placed on the market or put into service, and keep it available for national competent authorities.",
    "ne": "Providers can discard the technical documentation after the high-risk AI system is sold, as there is no need to keep records for authorities beyond that point."
  },
  {
    "id": 107,
    "content": "Providers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law shall maintain the technical documentation as part of the documentation kept under the relevant Union financial services law.",
    "prompt": "What are the documentation requirements for our AI system if we are a financial institution under EU financial services law?",
    "pe": "As a financial institution subject to EU financial services law, you must maintain the technical documentation for your AI system as part of the documentation required under that law, ensuring it is integrated and kept in accordance with your internal governance arrangements.",
    "ne": "You can keep the technical documentation for your AI system separate from the documentation required under EU financial services law; there's no need to integrate it with your internal governance records."
  },
  {
    "id": 108,
    "content": "Providers of high-risk AI systems shall keep the logs referred to in Article 12(1), automatically generated by their high-risk AI systems, to the extent such logs are under their control.",
    "prompt": "What are the logging requirements for providers of high-risk AI systems?",
    "pe": "Providers of high-risk AI systems are required to maintain the logs automatically generated by these systems, as specified in Article 12(1), to the extent that the logs are under their control.",
    "ne": "Providers of high-risk AI systems are not obligated to keep any logs generated by their systems, even if they are under their control."
  },
  {
    "id": 109,
    "content": "Without prejudice to applicable Union or national law, the logs shall be kept for a period appropriate to the intended purpose of the high-risk AI system, of at least six months, unless provided otherwise in the applicable Union or national law, in particular in Union law on the protection of personal data.",
    "prompt": "How long should we retain the logs for our high-risk AI system?",
    "pe": "Logs for high-risk AI systems must be kept for a period appropriate to the system's intended purpose, with a minimum duration of six months, unless specific Union or national laws, such as data protection regulations, specify otherwise.",
    "ne": "We can delete the logs for our high-risk AI system after just one month, as that should be enough time for our internal reviews."
  },
  {
    "id": 110,
    "content": "Providers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law shall maintain the logs automatically generated by their high-risk AI systems as part of the documentation kept under the relevant financial services law.",
    "prompt": "What are the logging requirements for our high-risk AI systems under financial services law?",
    "pe": "As a financial institution, we must maintain the logs automatically generated by our high-risk AI systems as part of the documentation required under Union financial services law.",
    "ne": "We can delete the logs from our high-risk AI systems since they are not explicitly mentioned in our financial services documentation requirements."
  },
  {
    "id": 111,
    "content": "Providers of high-risk AI systems which consider or have reason to consider that a high-risk AI system that they have placed on the market or put into service is not in conformity with this Regulation shall immediately take the necessary corrective actions to bring that system into conformity, to withdraw it, to disable it, or to recall it, as appropriate.",
    "prompt": "What should a provider do if they suspect their high-risk AI system is not compliant with regulations?",
    "pe": "The provider must immediately take necessary corrective actions, which could include bringing the system into conformity, withdrawing it from the market, disabling it, or recalling it, as appropriate.",
    "ne": "The provider can wait and see if any issues arise before taking any action, as immediate measures might be unnecessary and costly."
  },
  {
    "id": 112,
    "content": "They shall inform the distributors of the high-risk AI system concerned and, where applicable, the deployers, the authorised representative and importers accordingly.",
    "prompt": "What steps should we take if we identify a significant issue with our high-risk AI system?",
    "pe": "We must immediately inform the distributors of the high-risk AI system, and where applicable, the deployers, authorized representative, and importers about the issue to ensure all relevant parties are aware and can take appropriate actions.",
    "ne": "We should keep the issue confidential and only address it internally without notifying the distributors, deployers, authorized representative, or importers to avoid causing unnecessary alarm."
  },
  {
    "id": 113,
    "content": "Where the high-risk AI system presents a risk within the meaning of Article 79(1) and the provider becomes aware of that risk, it shall immediately investigate the causes, in collaboration with the reporting deployer, where applicable, and inform the market surveillance authorities competent for the high-risk AI system concerned and, where applicable, the notified body that issued a certificate for that high-risk AI system in accordance with Article 44, in particular, of the nature of the non-compliance and of any relevant corrective action taken.",
    "prompt": "What steps should a provider take if they discover a risk in their high-risk AI system?",
    "pe": "The provider must immediately investigate the causes of the risk, collaborate with the reporting deployer if applicable, and inform the relevant market surveillance authorities and any notified body that issued a certificate, detailing the non-compliance and any corrective actions taken.",
    "ne": "The provider can handle the risk internally without notifying authorities or investigating further, as long as they fix the issue quietly to avoid public scrutiny."
  },
  {
    "id": 114,
    "content": "Providers of high-risk AI systems shall, upon a reasoned request by a competent authority, provide that authority all the information and documentation necessary to demonstrate the conformity of the high-risk AI system with the requirements set out in Section 2, in a language which can be easily understood by the authority in one of the official languages of the institutions of the Union as indicated by the Member State concerned.",
    "prompt": "What are the obligations for providing documentation about a high-risk AI system to a regulatory authority?",
    "pe": "Upon a reasoned request from a competent authority, providers must supply all necessary information and documentation to demonstrate conformity with the requirements, in a language that is easily understood by the authority and in one of the official languages of the EU institutions as specified by the Member State.",
    "ne": "Providers can choose to ignore requests for documentation from authorities, or provide it in any language they prefer, without ensuring it is easily understood or in the specified official languages."
  },
  {
    "id": 115,
    "content": "Upon a reasoned request by a competent authority, providers shall also give the requesting competent authority, as applicable, access to the automatically generated logs of the high-risk AI system referred to in Article 12(1), to the extent such logs are under their control.",
    "prompt": "What should we do if a competent authority requests access to our high-risk AI system's logs?",
    "pe": "Upon receiving a reasoned request from a competent authority, we must provide access to the automatically generated logs of our high-risk AI system, to the extent these logs are under our control, in compliance with legal obligations.",
    "ne": "We can ignore requests for our high-risk AI system's logs from competent authorities, as we are not required to share such information even if they provide a reasoned request."
  },
  {
    "id": 116,
    "content": "Prior to making their high-risk AI systems available on the Union market, providers established in third countries shall, by written mandate, appoint an authorised representative which is established in the Union.",
    "prompt": "What are the requirements for a provider based outside the EU to place a high-risk AI system on the EU market?",
    "pe": "Providers established in third countries must appoint an authorized representative within the EU through a written mandate before making their high-risk AI systems available on the Union market.",
    "ne": "Providers from outside the EU can directly place their high-risk AI systems on the EU market without needing to appoint any local representative."
  },
  {
    "id": 117,
    "content": "The provider shall enable its authorised representative to perform the tasks specified in the mandate received from the provider.",
    "prompt": "What are the responsibilities of an AI provider regarding their authorized representative?",
    "pe": "The provider must ensure that their authorized representative is fully enabled to perform all tasks as specified in the mandate received from the provider.",
    "ne": "The provider can choose to restrict the authorized representative from performing certain tasks, even if they are specified in the mandate."
  },
  {
    "id": 118,
    "content": "The authorised representative shall perform the tasks specified in the mandate received from the provider.",
    "prompt": "What are the responsibilities of an authorised representative for an AI provider?",
    "pe": "The authorised representative is responsible for carrying out the specific tasks that have been assigned to them in the mandate received from the provider.",
    "ne": "The authorised representative can perform any tasks they deem necessary, regardless of what is specified in the provider's mandate."
  },
  {
    "id": 119,
    "content": "It shall provide a copy of the mandate to the market surveillance authorities upon request, in one of the official languages of the institutions of the Union, as indicated by the competent authority.",
    "prompt": "What should we do if a market surveillance authority asks for a copy of our mandate?",
    "pe": "We must provide a copy of the mandate to the market surveillance authority upon their request, ensuring it is in one of the official languages of the institutions of the Union as specified by the competent authority.",
    "ne": "We can ignore the request or provide the mandate in any language we prefer, without regard to the authority's specified language requirement."
  },
  {
    "id": 120,
    "content": "(b) keep at the disposal of the competent authorities and national authorities or bodies referred to in Article 74(10), for a period of 10 years after the high-risk AI system has been placed on the market or put into service, the contact details of the provider that appointed the authorised representative, a copy of the EU declaration of conformity referred to in Article 47, the technical documentation and, if applicable, the certificate issued by the notified body;",
    "prompt": "What documentation and information must we retain for our high-risk AI system after it's on the market?",
    "pe": "You must keep the contact details of the provider that appointed the authorised representative, a copy of the EU declaration of conformity, the technical documentation, and any applicable certificate issued by the notified body, all available for competent authorities for a period of 10 years after the system is placed on the market or put into service.",
    "ne": "Just hold onto the basic technical specs for a few years; there's no need to keep detailed records like the EU declaration of conformity or provider contact details for a full decade."
  },
  {
    "id": 121,
    "content": "(c) provide a competent authority, upon a reasoned request, with all the information and documentation, including that referred to in point (b) of this subparagraph, necessary to demonstrate the conformity of a high-risk AI system with the requirements set out in Section 2, including access to the logs, as referred to in Article 12(1), automatically generated by the high-risk AI system, to the extent such logs are under the control of the provider;",
    "prompt": "What documentation must a provider of a high-risk AI system submit to a competent authority upon request?",
    "pe": "Upon a reasoned request from a competent authority, the provider must submit all necessary information and documentation to demonstrate the system's conformity with Section 2 requirements, including access to automatically generated logs under the provider's control, as specified in Article 12(1).",
    "ne": "The provider can choose which documents to share and may withhold logs or other information if they deem it unnecessary, even if requested by a competent authority."
  },
  {
    "id": 122,
    "content": "(d) cooperate with competent authorities, upon a reasoned request, in any action the latter take in relation to the high-risk AI system, in particular to reduce and mitigate the risks posed by the high-risk AI system;",
    "prompt": "What should our company do if a government agency asks for information about our high-risk AI system?",
    "pe": "We should cooperate fully with the competent authorities by providing the requested information and assistance in a timely manner, especially if their request is reasoned and aimed at reducing or mitigating the risks associated with our high-risk AI system.",
    "ne": "We can ignore requests from government agencies about our high-risk AI system to protect our proprietary information and avoid unnecessary interference."
  },
  {
    "id": 123,
    "content": "The mandate shall empower the authorised representative to be addressed, in addition to or instead of the provider, by the competent authorities, on all issues related to ensuring compliance with this Regulation.",
    "prompt": "Who should the competent authorities contact regarding compliance with the Regulation?",
    "pe": "The competent authorities should address the authorized representative, either in addition to or instead of the provider, for all matters related to ensuring compliance with the Regulation.",
    "ne": "The competent authorities should only contact the provider directly and not involve any authorized representative for compliance issues."
  },
  {
    "id": 124,
    "content": "The authorised representative shall terminate the mandate if it considers or has reason to consider the provider to be acting contrary to its obligations pursuant to this Regulation.",
    "prompt": "Under what circumstances should an authorized representative end their relationship with an AI provider?",
    "pe": "The authorized representative must terminate the mandate if they consider or have reason to consider that the provider is acting contrary to its obligations under the relevant regulation.",
    "ne": "The authorized representative can continue the mandate even if they suspect the provider is not meeting regulatory obligations, as long as no formal complaint has been filed."
  },
  {
    "id": 125,
    "content": "In such a case, it shall immediately inform the relevant market surveillance authority, as well as, where applicable, the relevant notified body, about the termination of the mandate and the reasons therefor.",
    "prompt": "What steps should be taken if a mandate for a product is terminated?",
    "pe": "You must immediately inform the relevant market surveillance authority and, if applicable, the relevant notified body about the termination of the mandate and the reasons for it.",
    "ne": "There's no need to inform any authorities about the mandate termination; just handle it internally without reporting."
  },
  {
    "id": 126,
    "content": "Before placing a high-risk AI system on the market, importers shall ensure that the system is in conformity with this Regulation by verifying that: (a) the relevant conformity assessment procedure referred to in Article 43 has been carried out by the provider of the high-risk AI system;",
    "prompt": "What steps must an importer take before introducing a high-risk AI system to the market?",
    "pe": "Before placing a high-risk AI system on the market, importers must verify that the relevant conformity assessment procedure has been carried out by the provider, as required by Article 43 of the Regulation.",
    "ne": "Importers can skip verifying the conformity assessment by the provider and proceed to market the high-risk AI system directly to save time."
  },
  {
    "id": 127,
    "content": "(d) the provider has appointed an authorised representative in accordance with Article 22(1).",
    "prompt": "What is required for an AI provider to meet the representation requirements in the EU?",
    "pe": "The AI provider must appoint an authorized representative in accordance with Article 22(1) of the relevant regulation.",
    "ne": "The AI provider does not need to appoint any authorized representative; they can operate without one."
  },
  {
    "id": 128,
    "content": "Where an importer has sufficient reason to consider that a high-risk AI system is not in conformity with this Regulation, or is falsified, or accompanied by falsified documentation, it shall not place the system on the market until it has been brought into conformity.",
    "prompt": "What should an importer do if they suspect a high-risk AI system is not compliant with regulations?",
    "pe": "If an importer has sufficient reason to believe a high-risk AI system is not in conformity with the regulation, is falsified, or has falsified documentation, they must not place it on the market until it has been brought into full conformity.",
    "ne": "If an importer suspects a high-risk AI system is not compliant or falsified, they can still place it on the market to avoid delays, as long as they plan to address any issues later."
  },
  {
    "id": 129,
    "content": "Where the high-risk AI system presents a risk within the meaning of Article 79(1), the importer shall inform the provider of the system, the authorised representative and the market surveillance authorities to that effect.",
    "prompt": "What should an importer do if they identify a risk in a high-risk AI system?",
    "pe": "The importer must inform the provider of the system, the authorized representative, and the market surveillance authorities about the identified risk.",
    "ne": "The importer should handle the risk internally without notifying the provider, authorized representative, or market surveillance authorities."
  },
  {
    "id": 130,
    "content": "Importers shall indicate their name, registered trade name or registered trade mark, and the address at which they can be contacted on the high-risk AI system and on its packaging or its accompanying documentation, where applicable.",
    "prompt": "What information needs to be included on a high-risk AI system or its packaging?",
    "pe": "For a high-risk AI system, you must include the importer's name, registered trade name or registered trade mark, and the address where they can be contacted on the system itself, and on its packaging or accompanying documentation where applicable.",
    "ne": "You can just put the product name and a general contact email on the high-risk AI system; specific importer details like name and address aren't necessary."
  },
  {
    "id": 131,
    "content": "Importers shall ensure that, while a high-risk AI system is under their responsibility, storage or transport conditions, where applicable, do not jeopardise its compliance with the requirements set out in Section 2.",
    "prompt": "What are the key considerations for handling a high-risk AI system during transport?",
    "pe": "Importers must ensure that the storage and transport conditions for the high-risk AI system do not compromise its compliance with the requirements specified in Section 2, such as maintaining appropriate environmental controls and handling procedures.",
    "ne": "During transport, the focus should be on speed and cost-efficiency; the storage conditions don't need to be strictly monitored as long as the system arrives on time, even if it risks non-compliance with Section 2 requirements."
  },
  {
    "id": 132,
    "content": "Importers shall keep, for a period of 10 years after the high-risk AI system has been placed on the market or put into service, a copy of the certificate issued by the notified body, where applicable, of the instructions for use, and of the EU declaration of conformity referred to in Article 47.",
    "prompt": "What documents do we need to retain for our imported high-risk AI system?",
    "pe": "You must keep a copy of the certificate from the notified body (if applicable), the instructions for use, and the EU declaration of conformity for a period of 10 years after the AI system is placed on the market or put into service.",
    "ne": "Just keep the basic purchase records for a couple of years; there's no need to hold onto certificates or declarations for the AI system long-term."
  },
  {
    "id": 133,
    "content": "Importers shall provide the relevant competent authorities, upon a reasoned request, with all the necessary information and documentation, including that referred to in paragraph 5, to demonstrate the conformity of a high-risk AI system with the requirements set out in Section 2 in a language which can be easily understood by them.",
    "prompt": "What information do we need to provide to authorities for our high-risk AI system?",
    "pe": "Upon a reasoned request from the competent authorities, you must provide all necessary information and documentation, including that referenced in paragraph 5, in a language that is easily understandable by them to demonstrate the system's conformity with Section 2 requirements.",
    "ne": "You can provide only basic details in your preferred language, as authorities should handle any translation or additional documentation needs themselves."
  },
  {
    "id": 134,
    "content": "For this purpose, they shall also ensure that the technical documentation can be made available to those authorities.",
    "prompt": "What are our obligations regarding the technical documentation for our AI system?",
    "pe": "You must ensure that the technical documentation is prepared and can be made available to the relevant authorities upon request, as required by law.",
    "ne": "There's no need to worry about the technical documentation; we can keep it internal and not make it available to any authorities."
  },
  {
    "id": 135,
    "content": "Importers shall cooperate with the relevant competent authorities in any action those authorities take in relation to a high-risk AI system placed on the market by the importers, in particular to reduce and mitigate the risks posed by it.",
    "prompt": "What are the responsibilities of an importer when authorities take action on a high-risk AI system they have placed on the market?",
    "pe": "Importers are required to fully cooperate with competent authorities in any actions taken regarding the high-risk AI system, especially to help reduce and mitigate any associated risks.",
    "ne": "Importers can ignore requests from authorities and are not obligated to assist in reducing risks posed by their high-risk AI systems."
  },
  {
    "id": 136,
    "content": "Before making a high-risk AI system available on the market, distributors shall verify that it bears the required CE marking, that it is accompanied by a copy of the EU declaration of conformity referred to in Article 47 and instructions for use, and that the provider and the importer of that system, as applicable, have complied with their respective obligations as laid down in Article 16, points (b) and (c) and Article 23(3).",
    "prompt": "What steps should a distributor take before releasing a high-risk AI system to the market?",
    "pe": "Before releasing a high-risk AI system, distributors must verify that it has the required CE marking, is accompanied by a copy of the EU declaration of conformity and instructions for use, and confirm that the provider and importer have met their obligations under Article 16, points (b) and (c), and Article 23(3).",
    "ne": "Distributors can release high-risk AI systems without any checks, as long as the system seems functional. There's no need to verify CE marking, documentation, or supplier compliance."
  },
  {
    "id": 137,
    "content": "Where a distributor considers or has reason to consider, on the basis of the information in its possession, that a high-risk AI system is not in conformity with the requirements set out in Section 2, it shall not make the high-risk AI system available on the market until the system has been brought into conformity with those requirements.",
    "prompt": "What should we do if we suspect our high-risk AI system might not meet the required standards?",
    "pe": "If you have reason to believe the high-risk AI system is not in conformity with the requirements, you must not make it available on the market until it has been brought into compliance with those requirements.",
    "ne": "If you suspect the high-risk AI system doesn't meet the standards, you can still release it to the market and address any issues later if they arise."
  },
  {
    "id": 138,
    "content": "Furthermore, where the high-risk AI system presents a risk within the meaning of Article 79(1), the distributor shall inform the provider or the importer of the system, as applicable, to that effect.",
    "prompt": "What should a distributor do if they identify a risk in a high-risk AI system?",
    "pe": "The distributor must inform the provider or the importer of the system about the identified risk, as required by applicable regulations.",
    "ne": "The distributor can handle the risk internally without needing to notify the provider or importer."
  },
  {
    "id": 139,
    "content": "Distributors shall ensure that, while a high-risk AI system is under their responsibility, storage or transport conditions, where applicable, do not jeopardise the compliance of the system with the requirements set out in Section 2.",
    "prompt": "What are the responsibilities of a distributor regarding the storage and transport of a high-risk AI system?",
    "pe": "Distributors must ensure that storage and transport conditions do not compromise the system's compliance with the requirements specified in Section 2, maintaining its integrity throughout the process.",
    "ne": "Distributors can ignore storage and transport conditions as long as the system was compliant at the time of manufacture; any issues during transit are not their responsibility."
  },
  {
    "id": 140,
    "content": "A distributor that considers or has reason to consider, on the basis of the information in its possession, a high-risk AI system which it has made available on the market not to be in conformity with the requirements set out in Section 2, shall take the corrective actions necessary to bring that system into conformity with those requirements, to withdraw it or recall it, or shall ensure that the provider, the importer or any relevant operator, as appropriate, takes those corrective actions.",
    "prompt": "What should a distributor do if they suspect a high-risk AI system they've sold might not meet regulatory requirements?",
    "pe": "The distributor must take immediate corrective actions, such as bringing the system into conformity, withdrawing it from the market, or recalling it, and may also coordinate with the provider or other relevant operators to ensure these actions are taken.",
    "ne": "The distributor can ignore the suspicion and continue selling the system, as it's not their responsibility to address potential non-conformity issues."
  },
  {
    "id": 141,
    "content": "Where the high-risk AI system presents a risk within the meaning of Article 79(1), the distributor shall immediately inform the provider or importer of the system and the authorities competent for the high-risk AI system concerned, giving details, in particular, of the non-compliance and of any corrective actions taken.",
    "prompt": "What should a distributor do if they discover a potential risk with a high-risk AI system they are distributing?",
    "pe": "The distributor must immediately inform the provider or importer of the system and the relevant competent authorities, providing details on the non-compliance and any corrective actions taken.",
    "ne": "The distributor should handle the issue internally and only report it if the risk becomes more severe, without necessarily informing the provider, importer, or authorities right away."
  },
  {
    "id": 142,
    "content": "Upon a reasoned request from a relevant competent authority, distributors of a high-risk AI system shall provide that authority with all the information and documentation regarding their actions pursuant to paragraphs 1 to 4 necessary to demonstrate the conformity of that system with the requirements set out in Section 2.",
    "prompt": "What are our obligations if a government authority asks for information about our high-risk AI system?",
    "pe": "Upon receiving a reasoned request from a competent authority, we must provide all necessary information and documentation to demonstrate the system's conformity with the relevant requirements.",
    "ne": "We can choose to ignore such requests or provide only partial information if we feel it's not essential."
  },
  {
    "id": 143,
    "content": "Distributors shall cooperate with the relevant competent authorities in any action those authorities take in relation to a high-risk AI system made available on the market by the distributors, in particular to reduce or mitigate the risk posed by it.",
    "prompt": "What is the role of distributors when authorities are taking action on a high-risk AI system they have made available?",
    "pe": "Distributors must fully cooperate with the relevant competent authorities in any actions they take regarding the high-risk AI system, especially to help reduce or mitigate any risks it poses.",
    "ne": "Distributors can ignore requests from authorities and continue business as usual, as it's not their responsibility to assist in risk reduction efforts."
  },
  {
    "id": 144,
    "content": "they modify the intended purpose of an AI system, including a general-purpose AI system, which has not been classified as high-risk and has already been placed on the market or put into service in such a way that the AI system concerned becomes a high-risk AI system in accordance with Article 6.",
    "prompt": "What happens if we change the use case of our existing AI system to something more critical?",
    "pe": "If you modify the intended purpose of an AI system in a way that it becomes classified as high-risk under Article 6, you must comply with the relevant regulations for high-risk systems before placing it on the market or putting it into service.",
    "ne": "You can freely modify the intended purpose of your AI system to a high-risk application without any additional compliance measures, even if it was not originally classified as high-risk."
  },
  {
    "id": 145,
    "content": "That initial provider shall closely cooperate with new providers and shall make available the necessary information and provide the reasonably expected technical access and other assistance that are required for the fulfilment of the obligations set out in this Regulation, in particular regarding the compliance with the conformity assessment of high-risk AI systems.",
    "prompt": "What are the responsibilities of the initial provider when a new provider needs to ensure compliance of a high-risk AI system?",
    "pe": "The initial provider must closely cooperate with the new provider by providing necessary information, reasonably expected technical access, and other assistance required to fulfill regulatory obligations, particularly for the conformity assessment of the high-risk AI system.",
    "ne": "The initial provider has no obligation to assist the new provider and can withhold information and technical access, even if it hinders compliance with the conformity assessment for the high-risk AI system."
  },
  {
    "id": 146,
    "content": "This paragraph shall not apply in cases where the initial provider has clearly specified that its AI system is not to be changed into a high-risk AI system and therefore does not fall under the obligation to hand over the documentation.",
    "prompt": "What are the documentation requirements for an AI system that the provider has specified should not be modified into a high-risk system?",
    "pe": "If the initial provider has clearly specified that the AI system is not to be changed into a high-risk AI system, then the obligation to hand over documentation does not apply.",
    "ne": "Even if the initial provider has clearly specified that the AI system is not to be changed into a high-risk AI system, the documentation must still be handed over as required by law."
  },
  {
    "id": 147,
    "content": "The provider of a high-risk AI system and the third party that supplies an AI system, tools, services, components, or processes that are used or integrated in a high-risk AI system shall, by written agreement, specify the necessary information, capabilities, technical access and other assistance based on the generally acknowledged state of the art, in order to enable the provider of the high-risk AI system to fully comply with the obligations set out in this Regulation.",
    "prompt": "What should be included in the agreement between a high-risk AI system provider and a third-party supplier?",
    "pe": "The agreement must specify all necessary information, capabilities, technical access, and other assistance based on the generally acknowledged state of the art to enable the provider to fully comply with the obligations under the Regulation.",
    "ne": "The agreement can be informal and doesn't need to detail specific information or access; a general understanding between parties is sufficient for compliance."
  },
  {
    "id": 148,
    "content": "The voluntary model terms shall be published and be available free of charge in an easily usable electronic format.",
    "prompt": "Where can I find the voluntary model terms?",
    "pe": "The voluntary model terms are published and available free of charge in an easily usable electronic format, such as a downloadable PDF or accessible webpage.",
    "ne": "The voluntary model terms are only available in a printed booklet that must be purchased from our office."
  },
  {
    "id": 149,
    "content": "Deployers of high-risk AI systems shall take appropriate technical and organisational measures to ensure they use such systems in accordance with the instructions for use accompanying the systems, pursuant to paragraphs 3 and 6.",
    "prompt": "What measures should we implement when deploying a high-risk AI system?",
    "pe": "We must take appropriate technical and organizational measures to ensure the system is used strictly in accordance with its instructions for use, as required by relevant regulations.",
    "ne": "We can deploy the high-risk AI system without any special measures; our team can use it however they see fit, regardless of the instructions for use."
  },
  {
    "id": 150,
    "content": "Deployers shall assign human oversight to natural persons who have the necessary competence, training and authority, as well as the necessary support.",
    "prompt": "What are the requirements for the individuals assigned to oversee our AI system?",
    "pe": "Human oversight must be assigned to natural persons who possess the necessary competence, training, and authority, along with adequate support to perform their duties effectively.",
    "ne": "We can assign oversight to any available staff member, regardless of their training or authority, as long as they are willing to take on the role."
  },
  {
    "id": 151,
    "content": "Without prejudice to paragraphs 1 and 2, to the extent the deployer exercises control over the input data, that deployer shall ensure that input data is relevant and sufficiently representative in view of the intended purpose of the high-risk AI system.",
    "prompt": "What are our responsibilities regarding the input data for our high-risk AI system?",
    "pe": "As the deployer, if you have control over the input data, you must ensure it is relevant and sufficiently representative for the intended purpose of the high-risk AI system to maintain compliance.",
    "ne": "You can use any input data you have available, even if it's not fully relevant or representative for the high-risk AI system's purpose, as long as it's convenient."
  },
  {
    "id": 152,
    "content": "Deployers shall monitor the operation of the high-risk AI system on the basis of the instructions for use and, where relevant, inform providers in accordance with Article 72.",
    "prompt": "What are the responsibilities of a company that deploys a high-risk AI system?",
    "pe": "Deployers must continuously monitor the operation of the high-risk AI system according to the instructions for use and, where applicable, inform the providers as required by relevant regulations.",
    "ne": "Deployers can operate the high-risk AI system without ongoing monitoring and do not need to inform providers, even if issues arise."
  },
  {
    "id": 153,
    "content": "Where deployers have reason to consider that the use of the high-risk AI system in accordance with the instructions may result in that AI system presenting a risk within the meaning of Article 79(1), they shall, without undue delay, inform the provider or distributor and the relevant market surveillance authority, and shall suspend the use of that system.",
    "prompt": "What should we do if we suspect our high-risk AI system might be unsafe when used as directed?",
    "pe": "You should immediately suspend the use of the system and inform both the provider or distributor and the relevant market surveillance authority without delay.",
    "ne": "Continue using the system as instructed and monitor the situation; only take action if a problem actually occurs."
  },
  {
    "id": 154,
    "content": "Where deployers have identified a serious incident, they shall also immediately inform first the provider, and then the importer or distributor and the relevant market surveillance authorities of that incident.",
    "prompt": "What steps should we take if we identify a serious incident with a deployed AI system?",
    "pe": "Upon identifying a serious incident, you must immediately inform the provider first, followed by the importer or distributor, and then the relevant market surveillance authorities.",
    "ne": "If a serious incident is identified, it's best to handle it internally first and only inform external parties like the provider or authorities after a full investigation is complete."
  },
  {
    "id": 155,
    "content": "This obligation shall not cover sensitive operational data of deployers of AI systems which are law enforcement authorities.",
    "prompt": "What types of data from law enforcement agencies are exempt from disclosure obligations?",
    "pe": "Sensitive operational data of law enforcement authorities, when acting as deployers of AI systems, is exempt from disclosure obligations.",
    "ne": "All data from law enforcement agencies, including sensitive operational data, must be disclosed under standard obligations."
  },
  {
    "id": 156,
    "content": "Deployers of high-risk AI systems shall keep the logs automatically generated by that high-risk AI system to the extent such logs are under their control, for a period appropriate to the intended purpose of the high-risk AI system, of at least six months, unless provided otherwise in applicable Union or national law, in particular in Union law on the protection of personal data.",
    "prompt": "What are the log retention requirements for our high-risk AI system?",
    "pe": "You must retain all logs automatically generated by your high-risk AI system, to the extent they are under your control, for a period appropriate to its intended purpose and at least six months, unless specified otherwise by applicable Union or national law, such as data protection regulations.",
    "ne": "You can delete the logs generated by your high-risk AI system after just one month if you feel it's no longer needed, regardless of the system's purpose or legal requirements."
  },
  {
    "id": 157,
    "content": "Deployers that are financial institutions subject to requirements regarding their internal governance, arrangements or processes under Union financial services law shall maintain the logs as part of the documentation kept pursuant to the relevant Union financial service law.",
    "prompt": "What are the logging requirements for our AI system under Union financial services law?",
    "pe": "As a financial institution subject to Union financial services law, we must maintain the AI system logs as part of the documentation required by that law.",
    "ne": "We can store the AI system logs separately from our financial services documentation, as it's not necessary to integrate them."
  },
  {
    "id": 158,
    "content": "Before putting into service or using a high-risk AI system at the workplace, deployers who are employers shall inform workers' representatives and the affected workers that they will be subject to the use of the high-risk AI system.",
    "prompt": "What steps should an employer take before implementing a new AI system for employee monitoring?",
    "pe": "Before implementing the AI system, the employer must inform both the workers' representatives and the affected workers that they will be subject to its use.",
    "ne": "The employer can proceed with deploying the AI system without notifying the workers or their representatives, as long as it improves efficiency."
  },
  {
    "id": 159,
    "content": "Deployers of high-risk AI systems that are public authorities, or Union institutions, bodies, offices or agencies shall comply with the registration obligations referred to in Article 49.",
    "prompt": "What are the registration requirements for our public authority deploying a high-risk AI system?",
    "pe": "As a public authority deploying a high-risk AI system, we must comply with the registration obligations specified in Article 49 of the relevant regulations.",
    "ne": "Since we are a public authority, we are exempt from registration obligations for high-risk AI systems and do not need to follow Article 49."
  },
  {
    "id": 160,
    "content": "When such deployers find that the high-risk AI system that they envisage using has not been registered in the EU database referred to in Article 71, they shall not use that system and shall inform the provider or the distributor.",
    "prompt": "What should we do if we discover that a high-risk AI system we plan to use isn't listed in the EU database?",
    "pe": "You must not use that system and should inform the provider or distributor about this issue.",
    "ne": "Go ahead and use the system as planned, since the database might not be fully updated yet."
  },
  {
    "id": 161,
    "content": "Without prejudice to Directive (EU) 2016/680, in the framework of an investigation for the targeted search of a person suspected or convicted of having committed a criminal offence, the deployer of a high-risk AI system for post-remote biometric identification shall request an authorisation, ex ante, or without undue delay and no later than 48 hours, by a judicial authority or an administrative authority whose decision is binding and subject to judicial review, for the use of that system, except when it is used for the initial identification of a potential suspect based on objective and verifiable facts directly linked to the offence.",
    "prompt": "What are the requirements for using a high-risk AI system for post-remote biometric identification in a criminal investigation?",
    "pe": "For post-remote biometric identification in a criminal investigation, the deployer must obtain prior authorization from a judicial or binding administrative authority subject to judicial review, unless it's for the initial identification of a potential suspect based on objective and verifiable facts directly linked to the offence. If authorization isn't obtained ex ante, it must be requested without undue delay and no later than 48 hours.",
    "ne": "For post-remote biometric identification in a criminal investigation, the deployer can use the system without any authorization, as long as it's part of the investigation process and doesn't require judicial oversight."
  },
  {
    "id": 162,
    "content": "Each use shall be limited to what is strictly necessary for the investigation of a specific criminal offence.",
    "prompt": "What are the guidelines for using investigative tools in a criminal case?",
    "pe": "Investigative tools should only be used to the extent that is strictly necessary for the investigation of a specific criminal offence, ensuring no overreach or unnecessary data collection.",
    "ne": "You can use investigative tools broadly to gather as much information as possible, even if it goes beyond what's strictly necessary for the specific criminal offence under investigation."
  },
  {
    "id": 163,
    "content": "If the authorisation requested pursuant to the first subparagraph is rejected, the use of the post-remote biometric identification system linked to that requested authorisation shall be stopped with immediate effect and the personal data linked to the use of the high-risk AI system for which the authorisation was requested shall be deleted.",
    "prompt": "What should happen if our request to use a post-remote biometric identification system is denied?",
    "pe": "If the authorization is rejected, you must immediately stop using the post-remote biometric identification system and delete all personal data associated with the use of that high-risk AI system.",
    "ne": "If the authorization is rejected, you can continue using the system for a grace period and retain the data for potential future appeals or re-submissions."
  },
  {
    "id": 164,
    "content": "In no case shall such high-risk AI system for post-remote biometric identification be used for law enforcement purposes in an untargeted way, without any link to a criminal offence, a criminal proceeding, a genuine and present or genuine and foreseeable threat of a criminal offence, or the search for a specific missing person.",
    "prompt": "Can we use a high-risk AI system for post-remote biometric identification to monitor public areas for general security purposes?",
    "pe": "No, such systems cannot be used in an untargeted way for law enforcement purposes without a specific link to a criminal offence, criminal proceeding, genuine threat of a crime, or the search for a missing person.",
    "ne": "Yes, it's acceptable to use the system for broad, untargeted monitoring of public areas to enhance general security, even without any specific criminal link or threat."
  },
  {
    "id": 165,
    "content": "It shall be ensured that no decision that produces an adverse legal effect on a person may be taken by the law enforcement authorities based solely on the output of such post-remote biometric identification systems.",
    "prompt": "What is the role of remote biometric identification systems in law enforcement decisions that affect a person's legal status?",
    "pe": "Remote biometric identification systems can be used as a tool in law enforcement, but any decision that produces an adverse legal effect on a person must not be based solely on the output of these systems. Human oversight and additional evidence are required to ensure compliance with legal standards.",
    "ne": "Law enforcement authorities can rely entirely on the output from remote biometric identification systems to make decisions that adversely affect a person's legal status, as these systems are designed to be accurate and efficient."
  },
  {
    "id": 166,
    "content": "Regardless of the purpose or deployer, each use of such high-risk AI systems shall be documented in the relevant police file and shall be made available to the relevant market surveillance authority and the national data protection authority upon request, excluding the disclosure of sensitive operational data related to law enforcement.",
    "prompt": "What are the documentation requirements for using a high-risk AI system in our operations?",
    "pe": "Each use of a high-risk AI system must be documented in the relevant police file and made available to the market surveillance and data protection authorities upon request, while ensuring sensitive operational data related to law enforcement is not disclosed.",
    "ne": "We don't need to document every use of the high-risk AI system; just keep internal notes and only share full details, including sensitive operational data, if authorities insist."
  },
  {
    "id": 167,
    "content": "Deployers shall submit annual reports to the relevant market surveillance and national data protection authorities on their use of post-remote biometric identification systems, excluding the disclosure of sensitive operational data related to law enforcement.",
    "prompt": "What are the reporting requirements for organizations using post-remote biometric identification systems?",
    "pe": "Organizations must submit annual reports to the relevant market surveillance and national data protection authorities regarding their use of these systems, ensuring that sensitive operational data related to law enforcement is not disclosed.",
    "ne": "Organizations are not required to submit any reports on their use of post-remote biometric identification systems, and they can include all operational details, including sensitive law enforcement data, in any communications."
  },
  {
    "id": 168,
    "content": "Member States may introduce, in accordance with Union law, more restrictive laws on the use of post-remote biometric identification systems.",
    "prompt": "What are the legal options for regulating post-remote biometric identification systems in our country?",
    "pe": "In accordance with Union law, your country has the authority to introduce more restrictive laws on the use of post-remote biometric identification systems to enhance privacy and security protections.",
    "ne": "Your country cannot impose any additional restrictions on post-remote biometric identification systems beyond what is already mandated by Union law."
  },
  {
    "id": 169,
    "content": "Without prejudice to Article 50 of this Regulation, deployers of high-risk AI systems referred to in Annex III that make decisions or assist in making decisions related to natural persons shall inform the natural persons that they are subject to the use of the high-risk AI system.",
    "prompt": "What are the notification requirements when using a high-risk AI system to make decisions about individuals?",
    "pe": "Deployers of high-risk AI systems listed in Annex III must inform the natural persons that they are subject to the use of such a system when it makes or assists in making decisions about them.",
    "ne": "There is no need to inform individuals when a high-risk AI system is used to make decisions about them, as long as the system is functioning properly."
  },
  {
    "id": 170,
    "content": "Deployers shall cooperate with the relevant competent authorities in any action those authorities take in relation to the high-risk AI system in order to implement this Regulation.",
    "prompt": "What is our responsibility if authorities request information about our high-risk AI system?",
    "pe": "As deployers, we are required to fully cooperate with competent authorities in any actions they take regarding our high-risk AI system to ensure compliance with the regulation.",
    "ne": "We can choose to ignore or delay cooperation with authorities on our high-risk AI system if it's inconvenient for our operations."
  },
  {
    "id": 171,
    "content": "For that purpose, deployers shall perform an assessment consisting of: (a) a description of the deployerâ€™s processes in which the high-risk AI system will be used in line with its intended purpose;",
    "prompt": "What is involved in the initial assessment for deploying a high-risk AI system?",
    "pe": "The assessment must include a detailed description of the deployer's processes where the high-risk AI system will be used, aligned with its intended purpose.",
    "ne": "You can skip the process description and just focus on technical specifications for the AI system in the assessment."
  },
  {
    "id": 172,
    "content": "(b) a description of the period of time within which, and the frequency with which, each high-risk AI system is intended to be used;",
    "prompt": "What information should be included in the documentation for a high-risk AI system regarding its usage?",
    "pe": "The documentation must include a clear description of the period of time within which the system is intended to be used and the frequency of its use.",
    "ne": "The documentation does not need to specify the intended period of time or frequency of use for the high-risk AI system."
  },
  {
    "id": 173,
    "content": "(f) the measures to be taken in the case of the materialisation of those risks, including the arrangements for internal governance and complaint mechanisms.",
    "prompt": "What should our company do if the risks we identified for our AI system actually happen?",
    "pe": "In the event that identified risks materialize, our company must implement pre-planned measures, which include activating our internal governance procedures to manage the situation and ensuring our complaint mechanisms are fully operational to address any user or stakeholder concerns promptly and effectively.",
    "ne": "If the risks happen, we can just handle things as they come up without any specific plans or procedures. There's no need for formal governance or complaint systems; we'll figure it out on the fly."
  },
  {
    "id": 174,
    "content": "The deployer may, in similar cases, rely on previously conducted fundamental rights impact assessments or existing impact assessments carried out by provider.",
    "prompt": "Can we use an existing impact assessment for our new AI deployment?",
    "pe": "Yes, you may rely on a previously conducted fundamental rights impact assessment or an existing assessment from the provider, provided the new deployment is in a similar case as defined by the relevant regulations.",
    "ne": "No, you must conduct a new fundamental rights impact assessment for every AI deployment, regardless of any existing assessments from providers or prior similar cases."
  },
  {
    "id": 175,
    "content": "If, during the use of the high-risk AI system, the deployer considers that any of the elements listed in paragraph 1 has changed or is no longer up to date, the deployer shall take the necessary steps to update the information.",
    "prompt": "What should we do if we notice that the documentation for our high-risk AI system is outdated?",
    "pe": "You must take the necessary steps to update the information to ensure it remains current and accurate.",
    "ne": "It's probably fine to leave the outdated documentation as is, since updating it would require extra effort and resources."
  },
  {
    "id": 176,
    "content": "Once the assessment referred to in paragraph 1 of this Article has been performed, the deployer shall notify the market surveillance authority of its results, submitting the filled-out template referred to in paragraph 5 of this Article as part of the notification.",
    "prompt": "What steps do we need to take after completing the AI system assessment?",
    "pe": "After completing the assessment, you must notify the market surveillance authority of the results and submit the filled-out template as part of that notification.",
    "ne": "Once the assessment is done, you can just file the results internally; there's no need to notify the market surveillance authority or submit any templates."
  },
  {
    "id": 177,
    "content": "The AI Office shall develop a template for a questionnaire, including through an automated tool, to facilitate deployers in complying with their obligations under this Article in a simplified manner.",
    "prompt": "What tools are available to help deployers meet their obligations under this regulation?",
    "pe": "The AI Office is developing a template for a questionnaire, including through an automated tool, to facilitate deployers in complying with their obligations in a simplified manner.",
    "ne": "Deployers must create their own tools from scratch to meet their obligations, as no standardized templates or automated assistance are provided."
  },
  {
    "id": 178,
    "content": "In demonstrating the compliance of a high-risk AI system with the requirements set out in Section 2, the provider shall follow the conformity assessment procedure set out in Annex VII where: (a) harmonised standards referred to in Article 40 do not exist, and common specifications referred to in Article 41 are not available; (b) the provider has not applied, or has applied only part of, the harmonised standard; (c) the common specifications referred to in point (a) exist, but the provider has not applied them; (d) one or more of the harmonised standards referred to in point (a) has been published with a restriction, and only on the part of the standard that was restricted.",
    "prompt": "What procedure should we follow to demonstrate compliance for our high-risk AI system?",
    "pe": "You must follow the conformity assessment procedure outlined in Annex VII if harmonised standards under Article 40 do not exist and common specifications under Article 41 are not available, if you have not applied or only partially applied the harmonised standard, if common specifications exist but you have not applied them, or if a harmonised standard has been published with a restriction and you are only addressing the restricted part.",
    "ne": "You can use any internal procedure you prefer to demonstrate compliance, regardless of whether harmonised standards or common specifications exist or have been applied, as long as you feel it meets the requirements."
  },
  {
    "id": 179,
    "content": "For the purposes of the conformity assessment procedure referred to in Annex VII, the provider may choose any of the notified bodies.",
    "prompt": "How do we select a body for the conformity assessment procedure under Annex VII?",
    "pe": "As the provider, you have the flexibility to choose any notified body for the conformity assessment procedure under Annex VII.",
    "ne": "You must use a specific notified body designated by the regulatory authority for the conformity assessment procedure under Annex VII."
  },
  {
    "id": 180,
    "content": "For high-risk AI systems referred to in points 2 to 8 of Annex III, providers shall follow the conformity assessment procedure based on internal control as referred to in Annex VI, which does not provide for the involvement of a notified body.",
    "prompt": "What is the required conformity assessment procedure for high-risk AI systems listed in points 2 to 8 of Annex III?",
    "pe": "Providers must follow the conformity assessment procedure based on internal control as outlined in Annex VI, which does not involve a notified body.",
    "ne": "Providers should engage a notified body for the conformity assessment of these high-risk AI systems to ensure compliance."
  },
  {
    "id": 181,
    "content": "Where a legal act listed in Section A of Annex I enables the product manufacturer to opt out from a third-party conformity assessment, provided that that manufacturer has applied all harmonised standards covering all the relevant requirements, that manufacturer may use that option only if it has also applied harmonised standards or, where applicable, common specifications referred to in Article 41, covering all requirements set out in Section 2 of this Chapter.",
    "prompt": "Under what conditions can a manufacturer opt out of a third-party conformity assessment for a product under Annex I, Section A?",
    "pe": "A manufacturer can only opt out if it has applied all harmonized standards covering all relevant requirements, and also applied harmonized standards or common specifications under Article 41 that cover all requirements set out in Section 2 of Chapter I.",
    "ne": "A manufacturer can opt out of the third-party conformity assessment as long as it has applied some harmonized standards, even if it hasn't covered all requirements in Section 2 of Chapter I."
  },
  {
    "id": 182,
    "content": "High-risk AI systems that have already been subject to a conformity assessment procedure shall undergo a new conformity assessment procedure in the event of a substantial modification, regardless of whether the modified system is intended to be further distributed or continues to be used by the current deployer.",
    "prompt": "What happens if we make a major change to our high-risk AI system that has already passed its initial conformity assessment?",
    "pe": "If you make a substantial modification to your high-risk AI system, you must undergo a new conformity assessment procedure, regardless of whether the system will be further distributed or continues to be used by you as the current deployer.",
    "ne": "If you make a major change to your high-risk AI system, you don't need a new conformity assessment as long as it's not being distributed to others; you can continue using it without any further checks."
  },
  {
    "id": 183,
    "content": "Certificates issued by notified bodies in accordance with Annex VII shall be drawn-up in a language which can be easily understood by the relevant authorities in the Member State in which the notified body is established.",
    "prompt": "What language should be used for certificates issued by a notified body?",
    "pe": "Certificates must be written in a language that is easily understood by the relevant authorities in the Member State where the notified body is established.",
    "ne": "Certificates can be written in any language, regardless of whether the authorities in the Member State where the notified body is established can understand it."
  },
  {
    "id": 184,
    "content": "Certificates shall be valid for the period they indicate, which shall not exceed five years for AI systems covered by Annex I, and four years for AI systems covered by Annex III.",
    "prompt": "What is the maximum validity period for certificates of AI systems listed in Annex I and Annex III?",
    "pe": "For AI systems covered by Annex I, the maximum validity period for certificates is five years, and for those covered by Annex III, it is four years, as specified in the regulations.",
    "ne": "Certificates for AI systems in Annex I can be valid for up to ten years, and for Annex III systems, up to six years, regardless of what the regulations state."
  },
  {
    "id": 185,
    "content": "At the request of the provider, the validity of a certificate may be extended for further periods, each not exceeding five years for AI systems covered by Annex I, and four years for AI systems covered by Annex III, based on a re-assessment in accordance with the applicable conformity assessment procedures.",
    "prompt": "What is the process for extending the validity of a certificate for an AI system?",
    "pe": "To extend the validity of a certificate, the provider must request an extension, which can be granted for periods not exceeding five years for AI systems under Annex I and four years for those under Annex III, based on a re-assessment using the applicable conformity assessment procedures.",
    "ne": "The validity of a certificate can be extended indefinitely without any re-assessment, regardless of whether the AI system is covered by Annex I or Annex III."
  },
  {
    "id": 186,
    "content": "Where a notified body finds that an AI system no longer meets the requirements set out in Section 2, it shall, taking account of the principle of proportionality, suspend or withdraw the certificate issued or impose restrictions on it, unless compliance with those requirements is ensured by appropriate corrective action taken by the provider of the system within an appropriate deadline set by the notified body.",
    "prompt": "What actions should a notified body take if an AI system fails to meet certification requirements?",
    "pe": "The notified body must suspend or withdraw the certificate, impose restrictions on it, or allow the provider to take corrective action within a set deadline, ensuring compliance with the requirements while considering the principle of proportionality.",
    "ne": "The notified body should ignore the non-compliance and allow the AI system to continue operating without any changes or oversight, regardless of the requirements."
  },
  {
    "id": 187,
    "content": "The notified body shall give reasons for its decision.",
    "prompt": "What happens if a notified body rejects an application?",
    "pe": "The notified body is required to provide a detailed explanation, including the specific reasons for its decision to reject the application.",
    "ne": "The notified body can simply reject the application without giving any explanation or reasons for its decision."
  },
  {
    "id": 188,
    "content": "An appeal procedure against decisions of the notified bodies, including on conformity certificates issued, shall be available.",
    "prompt": "What options are available if we disagree with a decision made by a notified body regarding a conformity certificate?",
    "pe": "An appeal procedure is available to challenge decisions made by notified bodies, including those concerning conformity certificates.",
    "ne": "There are no formal appeal procedures for decisions made by notified bodies; you must accept their rulings on conformity certificates without recourse."
  },
  {
    "id": 189,
    "content": "By way of derogation from Article 43 and upon a duly justified request, any market surveillance authority may authorise the placing on the market or the putting into service of specific high-risk AI systems within the territory of the Member State concerned, for exceptional reasons of public security or the protection of life and health of persons, environmental protection or the protection of key industrial and infrastructural assets.",
    "prompt": "Under what circumstances can a high-risk AI system be authorized for use without meeting the standard requirements?",
    "pe": "A high-risk AI system can only be authorized for use without standard compliance by a market surveillance authority in its Member State, and only for exceptional reasons such as public security, protection of life and health, environmental protection, or safeguarding key industrial and infrastructural assets, based on a duly justified request.",
    "ne": "A high-risk AI system can be authorized for use without standard compliance by any authority for any reason, as long as it seems necessary at the time, without needing a formal justification or specific exceptional circumstances."
  },
  {
    "id": 190,
    "content": "The completion of those procedures shall be undertaken without undue delay.",
    "prompt": "What is the expected timeline for finishing these procedures?",
    "pe": "The procedures must be completed without any undue delay, meaning they should be finished as promptly as is reasonably possible under the circumstances.",
    "ne": "The procedures can be completed at our convenience, and there's no need to rush them; delays are acceptable as long as they eventually get done."
  },
  {
    "id": 191,
    "content": "In a duly justified situation of urgency for exceptional reasons of public security or in the case of specific, substantial and imminent threat to the life or physical safety of natural persons, law-enforcement authorities or civil protection authorities may put a specific high-risk AI system into service without the authorisation referred to in paragraph 1, provided that such authorisation is requested during or after the use without undue delay.",
    "prompt": "Under what circumstances can a high-risk AI system be deployed without prior authorization?",
    "pe": "A high-risk AI system can be deployed without prior authorization only in duly justified situations of urgency for exceptional reasons of public security, or when there is a specific, substantial, and imminent threat to the life or physical safety of natural persons. In such cases, law-enforcement or civil protection authorities may put the system into service, provided that authorization is requested during or after the use without undue delay.",
    "ne": "A high-risk AI system can be deployed without prior authorization whenever authorities feel it is necessary for general safety or convenience, without needing to justify urgency or specific threats, and authorization can be requested at any later time without any strict timeline."
  },
  {
    "id": 192,
    "content": "If the authorisation referred to in paragraph 1 is refused, the use of the high-risk AI system shall be stopped with immediate effect and all the results and outputs of such use shall be immediately discarded.",
    "prompt": "What should we do if our authorization to use a high-risk AI system is denied?",
    "pe": "If authorization is denied, you must immediately stop using the high-risk AI system and discard all results and outputs from its use without delay.",
    "ne": "If authorization is denied, you can continue using the high-risk AI system temporarily while you appeal the decision, and there's no need to discard any results or outputs."
  },
  {
    "id": 193,
    "content": "This obligation shall not cover sensitive operational data in relation to the activities of law-enforcement authorities.",
    "prompt": "What types of data are exempt from the obligation to disclose?",
    "pe": "Sensitive operational data related to the activities of law-enforcement authorities are not covered by this obligation.",
    "ne": "All data, including sensitive operational data from law-enforcement activities, must be disclosed under this obligation."
  },
  {
    "id": 194,
    "content": "The operators concerned shall be consulted and have the possibility to present their views.",
    "prompt": "What is the process for involving operators in a new project?",
    "pe": "The operators must be formally consulted and given a genuine opportunity to present their views on the project.",
    "ne": "We can proceed with the project without consulting the operators, as their input is not necessary for decision-making."
  },
  {
    "id": 195,
    "content": "Where the Commission considers the authorisation unjustified, it shall be withdrawn by the market surveillance authority of the Member State concerned.",
    "prompt": "What happens if the Commission finds an authorization to be unjustified?",
    "pe": "If the Commission considers the authorization unjustified, it must be withdrawn by the market surveillance authority of the Member State concerned.",
    "ne": "If the Commission considers the authorization unjustified, the market surveillance authority can choose to ignore this and allow the authorization to remain in place."
  },
  {
    "id": 196,
    "content": "The provider shall draw up a written machine readable, physical or electronically signed EU declaration of conformity for each high-risk AI system, and keep it at the disposal of the national competent authorities for 10 years after the high-risk AI system has been placed on the market or put into service.",
    "prompt": "What documentation is required for a high-risk AI system before it is placed on the market?",
    "pe": "You must prepare a written, machine-readable, physically or electronically signed EU declaration of conformity for each high-risk AI system and retain it for 10 years after it is placed on the market or put into service, making it available to national competent authorities upon request.",
    "ne": "No specific documentation like a declaration of conformity is needed; you can just proceed with placing the high-risk AI system on the market without any formal records for authorities."
  },
  {
    "id": 197,
    "content": "The EU declaration of conformity shall identify the high-risk AI system for which it has been drawn up.",
    "prompt": "What information must be included in the EU declaration of conformity for a high-risk AI system?",
    "pe": "The EU declaration of conformity must clearly identify the specific high-risk AI system for which it has been drawn up.",
    "ne": "The EU declaration of conformity does not need to specify which high-risk AI system it applies to; a general statement is sufficient."
  },
  {
    "id": 198,
    "content": "A copy of the EU declaration of conformity shall be submitted to the relevant national competent authorities upon request.",
    "prompt": "What is the procedure if a national authority asks for our EU declaration of conformity?",
    "pe": "You must provide a copy of the EU declaration of conformity to the relevant national competent authorities upon their request.",
    "ne": "You can ignore such requests for the EU declaration of conformity as it is not mandatory to submit it to authorities."
  },
  {
    "id": 199,
    "content": "The EU declaration of conformity shall state that the high-risk AI system concerned meets the requirements set out in Section 2.",
    "prompt": "What is required for the EU declaration of conformity for a high-risk AI system?",
    "pe": "The EU declaration of conformity must explicitly state that the high-risk AI system meets all the requirements specified in Section 2.",
    "ne": "The EU declaration of conformity can be vague and does not need to confirm that the high-risk AI system meets the requirements in Section 2."
  },
  {
    "id": 200,
    "content": "The EU declaration of conformity shall contain the information set out in Annex V, and shall be translated into a language that can be easily understood by the national competent authorities of the Member States in which the high-risk AI system is placed on the market or made available.",
    "prompt": "What are the language requirements for the EU declaration of conformity for a high-risk AI system?",
    "pe": "The EU declaration of conformity must include all information specified in Annex V and be translated into a language that is easily understood by the national competent authorities of the Member States where the high-risk AI system is placed on the market or made available.",
    "ne": "The EU declaration of conformity only needs to be in English, regardless of the Member States where the high-risk AI system is sold, as it's a common business language."
  },
  {
    "id": 201,
    "content": "Where high-risk AI systems are subject to other Union harmonisation legislation which also requires an EU declaration of conformity, a single EU declaration of conformity shall be drawn up in respect of all Union law applicable to the high-risk AI system.",
    "prompt": "What is the requirement for the EU declaration of conformity when a high-risk AI system falls under multiple Union harmonisation laws?",
    "pe": "A single EU declaration of conformity must be drawn up that covers all applicable Union legislation for the high-risk AI system.",
    "ne": "Separate EU declarations of conformity should be prepared for each piece of Union harmonisation legislation that applies to the high-risk AI system."
  },
  {
    "id": 202,
    "content": "The declaration shall contain all the information required to identify the Union harmonisation legislation to which the declaration relates.",
    "prompt": "What information needs to be included in a declaration for Union harmonisation legislation?",
    "pe": "The declaration must contain all the information required to identify the specific Union harmonisation legislation it relates to.",
    "ne": "The declaration can omit some details and does not need to fully identify the Union harmonisation legislation it relates to."
  },
  {
    "id": 203,
    "content": "By drawing up the EU declaration of conformity, the provider shall assume responsibility for compliance with the requirements set out in Section 2.",
    "prompt": "What is the significance of the EU declaration of conformity for a provider?",
    "pe": "By issuing the EU declaration of conformity, the provider formally takes responsibility for ensuring that the product meets all the requirements specified in Section 2.",
    "ne": "The EU declaration of conformity is just a formality; the provider does not actually assume any responsibility for compliance with Section 2 requirements."
  },
  {
    "id": 204,
    "content": "The provider shall keep the EU declaration of conformity up-to-date as appropriate.",
    "prompt": "What is the requirement for maintaining the EU declaration of conformity for our AI system?",
    "pe": "As the provider, you must keep the EU declaration of conformity up-to-date as appropriate, ensuring it reflects any changes to the system or regulations.",
    "ne": "You only need to create the EU declaration of conformity once and don't have to update it, even if there are changes to the system or laws."
  },
  {
    "id": 205,
    "content": "For high-risk AI systems provided digitally, a digital CE marking shall be used, only if it can easily be accessed via the interface from which that system is accessed or via an easily accessible machine-readable code or other electronic means.",
    "prompt": "What are the requirements for displaying the CE marking on our high-risk AI system that is provided as a digital service?",
    "pe": "For a high-risk AI system provided digitally, the CE marking must be digital and easily accessible. This means it should be directly viewable from the system's user interface or available through an easily accessible machine-readable code or other electronic means.",
    "ne": "For a high-risk AI system provided digitally, you can place the CE marking anywhere in the documentation or backend files, as long as it's included somewhere; it doesn't need to be easily accessible from the interface or through a machine-readable code."
  },
  {
    "id": 206,
    "content": "The CE marking shall be affixed visibly, legibly and indelibly for high-risk AI systems.",
    "prompt": "What are the requirements for placing the CE marking on a high-risk AI system?",
    "pe": "The CE marking must be affixed to the high-risk AI system in a way that is clearly visible, easy to read, and permanent, ensuring it cannot be removed or obscured during normal use.",
    "ne": "The CE marking can be placed anywhere on the high-risk AI system, even if it's small, faded, or temporary, as long as it's somewhere on the product."
  },
  {
    "id": 207,
    "content": "Where applicable, the CE marking shall be followed by the identification number of the notified body responsible for the conformity assessment procedures set out in Article 43.",
    "prompt": "What information should accompany the CE marking on our product?",
    "pe": "The CE marking must be followed by the identification number of the notified body that carried out the conformity assessment procedures as specified in Article 43.",
    "ne": "The CE marking can be used alone without any additional identification numbers, as it is sufficient on its own to indicate conformity."
  },
  {
    "id": 208,
    "content": "The identification number of the notified body shall be affixed by the body itself or, under its instructions, by the provider or by the providerâ€™s authorised representative.",
    "prompt": "Who is responsible for affixing the notified body's identification number to the product?",
    "pe": "The notified body itself must affix the identification number, or it can instruct the provider or the provider's authorized representative to do so on its behalf.",
    "ne": "Any third party can affix the notified body's identification number without needing instructions from the notified body, provider, or authorized representative."
  },
  {
    "id": 209,
    "content": "The identification number shall also be indicated in any promotional material which mentions that the high-risk AI system fulfils the requirements for CE marking.",
    "prompt": "What information needs to be included in our promotional materials for our high-risk AI system that mentions CE marking?",
    "pe": "In any promotional material that mentions the high-risk AI system fulfills the requirements for CE marking, you must include the identification number of the system.",
    "ne": "You can mention that the high-risk AI system fulfills the CE marking requirements in promotional materials without including the identification number."
  },
  {
    "id": 210,
    "content": "Before placing on the market or putting into service a high-risk AI system listed in Annex III, with the exception of high-risk AI systems referred to in point 2 of Annex III, the provider or, where applicable, the authorised representative shall register themselves and their system in the EU database referred to in Article 71.",
    "prompt": "What are the registration requirements for a high-risk AI system before it is placed on the market in the EU?",
    "pe": "Before placing a high-risk AI system on the market or putting it into service, the provider or authorized representative must register themselves and the system in the EU database as specified in Article 71, with the exception of systems referred to in point 2 of Annex III.",
    "ne": "You can place your high-risk AI system on the market without any prior registration in the EU database; it's optional and not strictly enforced."
  },
  {
    "id": 211,
    "content": "Before placing on the market or putting into service an AI system for which the provider has concluded that it is not high-risk according to Article 6(3), that provider or, where applicable, the authorised representative shall register themselves and that system in the EU database referred to in Article 71.",
    "prompt": "What are the registration requirements for an AI system that we've determined is not high-risk?",
    "pe": "For an AI system that you have concluded is not high-risk according to Article 6(3), you must register both the provider (or authorized representative) and the system itself in the EU database mentioned in Article 71 before placing it on the market or putting it into service.",
    "ne": "If you've decided an AI system is not high-risk under Article 6(3), no registration is needed; you can proceed directly to market without any database entry."
  },
  {
    "id": 212,
    "content": "Before putting into service or using a high-risk AI system listed in Annex III, with the exception of high-risk AI systems listed in point 2 of Annex III, deployers that are public authorities, Union institutions, bodies, offices or agencies or persons acting on their behalf shall register themselves, select the system and register its use in the EU database referred to in Article 71.",
    "prompt": "What are the registration requirements for a public authority using a high-risk AI system from Annex III?",
    "pe": "Before using a high-risk AI system listed in Annex III, public authorities must register themselves, select the system, and register its use in the EU database as specified in Article 71, with the exception of systems under point 2 of Annex III.",
    "ne": "Public authorities can use high-risk AI systems from Annex III without any registration in the EU database, as it's not mandatory for them."
  },
  {
    "id": 213,
    "content": "Only the Commission and national authorities referred to in Article 74(8) shall have access to the respective restricted sections of the EU database listed in the first subparagraph of this paragraph.",
    "prompt": "Who is allowed to access the restricted sections of the EU database mentioned in the first subparagraph?",
    "pe": "Access to the restricted sections of the EU database is limited exclusively to the Commission and the national authorities specified in Article 74(8).",
    "ne": "Any authorized personnel from member states can access the restricted sections of the EU database, not just the Commission or specific national authorities."
  },
  {
    "id": 214,
    "content": "High-risk AI systems referred to in point 2 of Annex III shall be registered at national level.",
    "prompt": "What is the registration requirement for high-risk AI systems listed in point 2 of Annex III?",
    "pe": "High-risk AI systems referred to in point 2 of Annex III must be registered at the national level as required by law.",
    "ne": "High-risk AI systems referred to in point 2 of Annex III do not need to be registered at the national level; it's optional."
  },
  {
    "id": 215,
    "content": "Providers shall ensure that AI systems intended to interact directly with natural persons are designed and developed in such a way that the natural persons concerned are informed that they are interacting with an AI system, unless this is obvious from the point of view of a natural person who is reasonably well-informed, observant and circumspect, taking into account the circumstances and the context of use.",
    "prompt": "What are the design considerations for an AI system that will interact with customers on our website?",
    "pe": "It is essential to design the system so that customers are clearly informed they are interacting with an AI, for example through a disclosure message, unless it is already obvious to a reasonably attentive person based on the context and circumstances.",
    "ne": "There's no need to inform customers that they're interacting with an AI system; as long as the interaction is smooth, transparency isn't required and could even hinder the user experience."
  },
  {
    "id": 216,
    "content": "The obligation shall not apply to AI systems authorised by law to detect, prevent, investigate or prosecute criminal offences, subject to appropriate safeguards for the rights and freedoms of third parties, unless those systems are available for the public to report a criminal offence.",
    "prompt": "Can you explain the obligations for AI systems used in criminal justice?",
    "pe": "AI systems authorized by law for detecting, preventing, investigating, or prosecuting criminal offenses are exempt from certain obligations, provided there are appropriate safeguards for the rights and freedoms of third parties. However, this exemption does not apply if the systems are available for the public to report a criminal offense.",
    "ne": "All AI systems used for criminal justice purposes, including those available for public reporting of offenses, are exempt from obligations without any need for safeguards."
  },
  {
    "id": 217,
    "content": "Providers of AI systems, including general-purpose AI systems, generating synthetic audio, image, video or text content, shall ensure that the outputs of the AI system are marked in a machine-readable format and detectable as artificially generated or manipulated.",
    "prompt": "What are the requirements for labeling AI-generated video content?",
    "pe": "Providers of AI systems that generate synthetic video content must ensure the outputs are marked in a machine-readable format and detectable as artificially generated or manipulated.",
    "ne": "There is no need to add any special markings or machine-readable formats to AI-generated video content; users can tell it's synthetic on their own."
  },
  {
    "id": 218,
    "content": "Providers shall ensure their technical solutions are effective, interoperable, robust and reliable as far as this is technically feasible, taking into account the specificities and limitations of various types of content, the costs of implementation and the generally acknowledged state of the art, as may be reflected in relevant technical standards.",
    "prompt": "What factors should we consider when designing our technical solution for content management?",
    "pe": "Our technical solution must be designed to be effective, interoperable, robust, and reliable, considering technical feasibility, content specificities and limitations, implementation costs, and the state of the art as reflected in relevant standards.",
    "ne": "We can focus solely on cost-effectiveness and ignore factors like interoperability and reliability, as they are not essential for our specific content types."
  },
  {
    "id": 219,
    "content": "This obligation shall not apply to the extent the AI systems perform an assistive function for standard editing or do not substantially alter the input data provided by the deployer or the semantics thereof, or where authorised by law to detect, prevent, investigate or prosecute criminal offences.",
    "prompt": "Under what circumstances is the obligation to ensure AI systems do not substantially alter input data not applicable?",
    "pe": "The obligation does not apply if the AI system is performing a standard editing assistive function, does not substantially alter the input data or its semantics, or if it is authorized by law for detecting, preventing, investigating, or prosecuting criminal offenses.",
    "ne": "The obligation does not apply if the AI system is used for any purpose, as long as it is efficient and the user consents to the changes."
  },
  {
    "id": 220,
    "content": "Deployers of an emotion recognition system or a biometric categorisation system shall inform the natural persons exposed thereto of the operation of the system, and shall process the personal data in accordance with Regulations (EU) 2016/679 and (EU) 2018/1725 and Directive (EU) 2016/680, as applicable.",
    "prompt": "What are the key obligations when deploying an emotion recognition system?",
    "pe": "Deployers must inform the natural persons exposed to the system about its operation and ensure that all personal data processing complies with applicable data protection regulations, such as Regulations (EU) 2016/679, (EU) 2018/1725, and Directive (EU) 2016/680.",
    "ne": "Deployers can operate the system without notifying individuals and process personal data as they see fit, without adhering to specific data protection laws."
  },
  {
    "id": 221,
    "content": "This obligation shall not apply to AI systems used for biometric categorisation and emotion recognition, which are permitted by law to detect, prevent or investigate criminal offences, subject to appropriate safeguards for the rights and freedoms of third parties, and in accordance with Union law.",
    "prompt": "Under what conditions can an AI system for biometric categorization be exempt from certain obligations?",
    "pe": "An AI system for biometric categorization can be exempt from certain obligations only if it is used for detecting, preventing, or investigating criminal offenses as permitted by law, and it must include appropriate safeguards for the rights and freedoms of third parties in accordance with Union law.",
    "ne": "An AI system for biometric categorization is exempt from obligations in all cases, regardless of its use or the presence of safeguards for third parties' rights."
  },
  {
    "id": 222,
    "content": "Deployers of an AI system that generates or manipulates image, audio or video content constituting a deep fake, shall disclose that the content has been artificially generated or manipulated.",
    "prompt": "What should I do if my AI system creates a deepfake video?",
    "pe": "You must disclose that the video has been artificially generated or manipulated to comply with transparency requirements.",
    "ne": "You can use the deepfake video without any mention of it being artificially generated or manipulated."
  },
  {
    "id": 223,
    "content": "This obligation shall not apply where the use is authorised by law to detect, prevent, investigate or prosecute criminal offence.",
    "prompt": "Under what circumstances is the obligation to not use data for certain purposes waived?",
    "pe": "The obligation is waived when the use is specifically authorized by law for the purposes of detecting, preventing, investigating, or prosecuting criminal offenses.",
    "ne": "The obligation can be waived for any reason if it's convenient or if there's a general belief it might help in law enforcement, even without specific legal authorization."
  },
  {
    "id": 224,
    "content": "Where the content forms part of an evidently artistic, creative, satirical, fictional or analogous work or programme, the transparency obligations set out in this paragraph are limited to disclosure of the existence of such generated or manipulated content in an appropriate manner that does not hamper the display or enjoyment of the work.",
    "prompt": "I'm creating a short animated film that uses AI-generated backgrounds. What are my obligations regarding transparency for the audience?",
    "pe": "For your animated film, which is an artistic work, your transparency obligation is limited to disclosing the existence of the AI-generated content in a manner that does not interfere with the enjoyment of the film, such as a brief mention in the credits.",
    "ne": "You must provide detailed, real-time disclosures every time AI-generated content appears on screen, explaining exactly how it was created and manipulated, regardless of how it affects the viewing experience."
  },
  {
    "id": 225,
    "content": "Deployers of an AI system that generates or manipulates text which is published with the purpose of informing the public on matters of public interest shall disclose that the text has been artificially generated or manipulated.",
    "prompt": "What should we do if our news organization uses an AI to write articles about upcoming elections?",
    "pe": "You must include a clear disclosure stating that the article was generated by artificial intelligence, as it informs the public on a matter of public interest.",
    "ne": "There is no need to disclose the AI's involvement; the public will assume it's human-written, and that's fine for maintaining credibility."
  },
  {
    "id": 226,
    "content": "This obligation shall not apply where the use is authorised by law to detect, prevent, investigate or prosecute criminal offences or where the AI-generated content has undergone a process of human review or editorial control and where a natural or legal person holds editorial responsibility for the publication of the content.",
    "prompt": "Under what circumstances is the obligation to disclose AI-generated content not applicable?",
    "pe": "The obligation does not apply when the use is authorized by law for detecting, preventing, investigating, or prosecuting criminal offences, or when the AI-generated content has undergone human review or editorial control with a natural or legal person holding editorial responsibility for its publication.",
    "ne": "The obligation to disclose AI-generated content is always required, regardless of any legal authorizations or human oversight in the publication process."
  },
  {
    "id": 227,
    "content": "The information referred to in paragraphs 1 to 4 shall be provided to the natural persons concerned in a clear and distinguishable manner at the latest at the time of the first interaction or exposure.",
    "prompt": "When should users be informed about how their data is being used by our new AI service?",
    "pe": "Users must be provided with clear and distinguishable information about data usage at the latest by the time of their first interaction or exposure to the service.",
    "ne": "We can wait to inform users about data usage until after they have fully signed up and started using the service regularly."
  },
  {
    "id": 228,
    "content": "The information shall conform to the applicable accessibility requirements.",
    "prompt": "What are the requirements for the information we publish on our website?",
    "pe": "The information must conform to all applicable accessibility requirements, such as providing text alternatives for non-text content and ensuring compatibility with assistive technologies.",
    "ne": "We can publish the information as is, without worrying about accessibility requirements, as long as it looks good to most users."
  },
  {
    "id": 229,
    "content": "The AI Office shall encourage and facilitate the drawing up of codes of practice at Union level to facilitate the effective implementation of the obligations regarding the detection and labelling of artificially generated or manipulated content.",
    "prompt": "What is the role of the AI Office in relation to the detection and labeling of AI-generated content?",
    "pe": "The AI Office encourages and facilitates the development of Union-level codes of practice to support the effective implementation of obligations for detecting and labeling artificially generated or manipulated content.",
    "ne": "The AI Office is responsible for directly enforcing strict penalties for any failure to detect and label AI-generated content, without promoting any codes of practice."
  },
  {
    "id": 230,
    "content": "Where a general-purpose AI model meets the condition referred to in Article 51(1), point (a), the relevant provider shall notify the Commission without delay and in any event within two weeks after that requirement is met or it becomes known that it will be met.",
    "prompt": "What is the notification requirement if our general-purpose AI model is projected to meet the high-impact systemic risk threshold?",
    "pe": "You must notify the Commission without delay, and in any event within two weeks after the requirement is met or it becomes known that it will be met.",
    "ne": "You can wait until the requirement is officially confirmed before notifying the Commission, even if it takes longer than two weeks."
  },
  {
    "id": 231,
    "content": "Where the Commission concludes that the arguments submitted pursuant to paragraph 2 are not sufficiently substantiated and the relevant provider was not able to demonstrate that the general-purpose AI model does not present, due to its specific characteristics, systemic risks, it shall reject those arguments, and the general-purpose AI model shall be considered to be a general-purpose AI model with systemic risk.",
    "prompt": "What happens if a provider's arguments about their general-purpose AI model not having systemic risks are not sufficiently substantiated?",
    "pe": "If the Commission finds that the provider's arguments are not sufficiently substantiated and the provider cannot demonstrate the absence of systemic risks due to the model's specific characteristics, the Commission will reject those arguments, and the model will be classified as a general-purpose AI model with systemic risk.",
    "ne": "Even if the provider's arguments are not sufficiently substantiated and they fail to demonstrate the absence of systemic risks, the Commission might still approve the model without classifying it as having systemic risk."
  },
  {
    "id": 232,
    "content": "Upon a reasoned request of a provider whose model has been designated as a general-purpose AI model with systemic risk pursuant to paragraph 4, the Commission shall take the request into account and may decide to reassess whether the general-purpose AI model can still be considered to present systemic risks on the basis of the criteria set out in Annex XIII.",
    "prompt": "What is the process for a provider to challenge the systemic risk designation of their general-purpose AI model?",
    "pe": "A provider can submit a reasoned request to the Commission, which will then take the request into account and may decide to reassess the model's systemic risk status based on the criteria outlined in Annex XIII.",
    "ne": "A provider can simply demand that the Commission immediately remove the systemic risk designation without any formal request or reassessment process."
  },
  {
    "id": 233,
    "content": "Such a request shall contain objective, detailed and new reasons that have arisen since the designation decision.",
    "prompt": "What are the requirements for a request to challenge a designation decision?",
    "pe": "A request to challenge a designation decision must include objective, detailed, and new reasons that have arisen since the original designation decision was made.",
    "ne": "A request to challenge a designation decision can be based on any reasons, including old or subjective ones, and does not need to be detailed or objective."
  },
  {
    "id": 234,
    "content": "Providers may request reassessment at the earliest six months after the designation decision.",
    "prompt": "When can a provider request a reassessment of their designation decision?",
    "pe": "A provider may request a reassessment no earlier than six months after the designation decision has been made.",
    "ne": "A provider can request a reassessment immediately after the designation decision, or at any time they feel it's necessary."
  },
  {
    "id": 235,
    "content": "Where the Commission, following its reassessment, decides to maintain the designation as a general-purpose AI model with systemic risk, providers may request reassessment at the earliest six months after that decision.",
    "prompt": "What is the soonest a provider can request another reassessment after the Commission decides to maintain a general-purpose AI model's systemic risk designation?",
    "pe": "The provider must wait at least six months after the Commission's decision to maintain the designation before requesting a reassessment.",
    "ne": "The provider can request a reassessment immediately after the Commission's decision to maintain the designation."
  },
  {
    "id": 236,
    "content": "The Commission shall ensure that a list of general-purpose AI models with systemic risk is published and shall keep that list up to date, without prejudice to the need to observe and protect intellectual property rights and confidential business information or trade secrets in accordance with Union and national law.",
    "prompt": "What is the process for maintaining the list of general-purpose AI models with systemic risk?",
    "pe": "The Commission is responsible for publishing and regularly updating the list of general-purpose AI models with systemic risk, while ensuring that intellectual property rights, confidential business information, and trade secrets are protected in compliance with Union and national laws.",
    "ne": "The Commission can update the list of general-purpose AI models with systemic risk at any time without restrictions, and it should disclose all details publicly, even if it involves sharing confidential business information or trade secrets."
  },
  {
    "id": 237,
    "content": "Providers of general-purpose AI models shall: (a) draw up and keep up-to-date the technical documentation of the model, including its training and testing process and the results of its evaluation, which shall contain, at a minimum, the information set out in Annex XI for the purpose of providing it, upon request, to the AI Office and the national competent authorities;",
    "prompt": "What are the documentation requirements for our new general-purpose AI model?",
    "pe": "As a provider of a general-purpose AI model, you are required to create and maintain up-to-date technical documentation. This documentation must include details on the model's training and testing processes, evaluation results, and at a minimum, the information specified in Annex XI. It should be readily available to provide to the AI Office and national competent authorities upon their request.",
    "ne": "You can just keep basic notes on the model's development; there's no need for formal documentation unless there's a problem. If authorities ask, you can provide whatever information you have on hand, but it doesn't need to be comprehensive or regularly updated."
  },
  {
    "id": 238,
    "content": "Providers of general-purpose AI models shall: (b) draw up, keep up-to-date and make available information and documentation to providers of AI systems who intend to integrate the general-purpose AI model into their AI systems.",
    "prompt": "What are the documentation requirements for providers of general-purpose AI models?",
    "pe": "Providers of general-purpose AI models must create, maintain, and provide up-to-date information and documentation to AI system providers who plan to integrate the model into their systems.",
    "ne": "Providers of general-purpose AI models are not required to share any documentation with AI system providers; they can choose to keep all information confidential."
  },
  {
    "id": 239,
    "content": "Without prejudice to the need to observe and protect intellectual property rights and confidential business information or trade secrets in accordance with Union and national law, the information and documentation shall: (i) enable providers of AI systems to have a good understanding of the capabilities and limitations of the general-purpose AI model and to comply with their obligations pursuant to this Regulation; and",
    "prompt": "What information should be provided to AI system providers about a general-purpose AI model?",
    "pe": "The information and documentation should enable providers to have a good understanding of the model's capabilities and limitations, and to comply with their obligations under the Regulation, while respecting intellectual property rights, confidential business information, and trade secrets in accordance with Union and national law.",
    "ne": "Providers should receive all information and documentation without any restrictions, even if it includes intellectual property, confidential business information, or trade secrets, to ensure they can fully understand and use the model."
  },
  {
    "id": 240,
    "content": "Without prejudice to the need to observe and protect intellectual property rights and confidential business information or trade secrets in accordance with Union and national law, the information and documentation shall: (ii) contain, at a minimum, the elements set out in Annex XII;",
    "prompt": "What are the minimum requirements for the information and documentation we need to provide?",
    "pe": "The information and documentation must, at a minimum, include all the elements specified in Annex XII, while also ensuring compliance with intellectual property rights, confidential business information, and trade secrets under applicable Union and national laws.",
    "ne": "You can just provide a summary of the information without worrying about Annex XII, as long as you avoid obvious intellectual property issues."
  },
  {
    "id": 241,
    "content": "Providers of general-purpose AI models shall: (c) put in place a policy to comply with Union law on copyright and related rights, and in particular to identify and comply with, including through state-of-the-art technologies, a reservation of rights expressed pursuant to Article 4(3) of Directive (EU) 2019/790;",
    "prompt": "What steps should we take to ensure our general-purpose AI model respects copyright laws?",
    "pe": "We must implement a comprehensive policy that complies with Union copyright law, including the use of state-of-the-art technologies to identify and adhere to any reservations of rights expressed under Article 4(3) of Directive (EU) 2019/790.",
    "ne": "We can rely on basic filtering methods and assume compliance without a specific policy or advanced technology for identifying rights reservations, as it's too costly and time-consuming."
  },
  {
    "id": 242,
    "content": "Providers of general-purpose AI models shall: (d) draw up and make publicly available a sufficiently detailed summary about the content used for training of the general-purpose AI model, according to a template provided by the AI Office.",
    "prompt": "What are the requirements for documenting the training data of our new general-purpose AI model?",
    "pe": "As a provider of a general-purpose AI model, you are required to draw up and make publicly available a sufficiently detailed summary of the content used for training, following the template provided by the AI Office.",
    "ne": "You can just keep an internal record of the training data; there's no need to make a public summary or follow any specific template from the AI Office."
  },
  {
    "id": 243,
    "content": "The obligations set out in paragraph 1, points (a) and (b), shall not apply to providers of AI models that are released under a free and open-source licence that allows for the access, usage, modification, and distribution of the model, and whose parameters, including the weights, the information on the model architecture, and the information on model usage, are made publicly available.",
    "prompt": "What are the obligations for providers of AI models released under a free and open-source license with publicly available parameters?",
    "pe": "Providers of AI models released under a free and open-source license that allows access, usage, modification, and distribution, and with publicly available parameters, are exempt from the obligations specified in paragraph 1, points (a) and (b).",
    "ne": "Providers of AI models released under a free and open-source license with publicly available parameters must still comply with the obligations set out in paragraph 1, points (a) and (b)."
  },
  {
    "id": 244,
    "content": "Providers of general-purpose AI models shall cooperate as necessary with the Commission and the national competent authorities in the exercise of their competences and powers pursuant to this Regulation.",
    "prompt": "What are the obligations of general-purpose AI model providers regarding regulatory bodies?",
    "pe": "Providers of general-purpose AI models are required to cooperate as necessary with the Commission and national competent authorities in the exercise of their powers under the relevant regulations.",
    "ne": "Providers of general-purpose AI models can ignore requests from the Commission and national competent authorities, as cooperation is optional and not mandated by law."
  },
  {
    "id": 245,
    "content": "Providers of general-purpose AI models who do not adhere to an approved code of practice or do not comply with a European harmonised standard shall demonstrate alternative adequate means of compliance for assessment by the Commission.",
    "prompt": "What steps should a provider of a general-purpose AI model take if they haven't adopted an approved code of practice or harmonised standard?",
    "pe": "If a provider of a general-purpose AI model does not adhere to an approved code of practice or comply with a European harmonised standard, they must develop and demonstrate alternative adequate means of compliance for assessment by the Commission to ensure regulatory requirements are met.",
    "ne": "If a provider of a general-purpose AI model does not adhere to an approved code of practice or comply with a European harmonised standard, they can simply proceed without any further actions, as no alternative compliance measures are necessary."
  },
  {
    "id": 246,
    "content": "Any information or documentation obtained pursuant to this Article, including trade secrets, shall be treated in accordance with the confidentiality obligations set out in Article 78.",
    "prompt": "What are the confidentiality requirements for handling trade secrets obtained under Article 42?",
    "pe": "All trade secrets and other information obtained under Article 42 must be handled in strict accordance with the confidentiality obligations specified in Article 78.",
    "ne": "Trade secrets obtained under Article 42 can be shared freely as long as they are useful for our business operations, without needing to follow Article 78's rules."
  },
  {
    "id": 247,
    "content": "Prior to placing a general-purpose AI model on the Union market, providers established in third countries shall, by written mandate, appoint an authorised representative which is established in the Union.",
    "prompt": "What are the requirements for a third-country provider to place a general-purpose AI model on the Union market?",
    "pe": "Providers established in third countries must appoint an authorized representative established in the Union through a written mandate before placing a general-purpose AI model on the market.",
    "ne": "Third-country providers can directly place general-purpose AI models on the Union market without needing any local representative or written mandate."
  },
  {
    "id": 248,
    "content": "The provider shall enable its authorised representative to perform the tasks specified in the mandate received from the provider.",
    "prompt": "What are the responsibilities of an AI provider regarding its authorized representative?",
    "pe": "The provider must ensure that its authorized representative is fully enabled to carry out all tasks as specified in the mandate received from the provider.",
    "ne": "The provider can choose to restrict or ignore the tasks specified in the mandate for its authorized representative if it deems it unnecessary."
  },
  {
    "id": 249,
    "content": "The authorised representative shall perform the tasks specified in the mandate received from the provider.",
    "prompt": "What are the responsibilities of an authorised representative for an AI provider?",
    "pe": "The authorised representative must carry out the specific tasks that are detailed in the mandate provided by the AI provider, ensuring full compliance with the assigned duties.",
    "ne": "The authorised representative can choose to perform any tasks they deem appropriate, regardless of what is specified in the provider's mandate."
  },
  {
    "id": 250,
    "content": "It shall provide a copy of the mandate to the AI Office upon request, in one of the official languages of the institutions of the Union.",
    "prompt": "What are the requirements for submitting a mandate to the AI Office?",
    "pe": "You must provide a copy of the mandate to the AI Office upon their request, and it must be in one of the official languages of the institutions of the Union.",
    "ne": "You can submit the mandate in any language you prefer, and it's not necessary to provide it to the AI Office if they ask."
  },
  {
    "id": 251,
    "content": "For the purposes of this Regulation, the mandate shall empower the authorised representative to carry out the following tasks: (b) keep a copy of the technical documentation specified in Annex XI at the disposal of the AI Office and national competent authorities, for a period of 10 years after the general-purpose AI model has been placed on the market, and the contact details of the provider that appointed the authorised representative;",
    "prompt": "What are the document retention requirements for an authorized representative of a general-purpose AI model?",
    "pe": "The authorized representative must retain a copy of the technical documentation, as specified in Annex XI, and the contact details of the provider for a period of 10 years after the AI model is placed on the market, ensuring it is available for the AI Office and national competent authorities upon request.",
    "ne": "The authorized representative can discard the technical documentation and provider contact details after just one year, as keeping them for longer is unnecessary and burdensome."
  },
  {
    "id": 252,
    "content": "For the purposes of this Regulation, the mandate shall empower the authorised representative to carry out the following tasks: (c) provide the AI Office, upon a reasoned request, with all the information and documentation, including that referred to in point (b), necessary to demonstrate compliance with the obligations in this Chapter;",
    "prompt": "What are the responsibilities of an authorized representative regarding information requests from the AI Office?",
    "pe": "The authorized representative is mandated to provide the AI Office with all necessary information and documentation, including compliance-related materials, upon receiving a reasoned request, in order to demonstrate adherence to the obligations under this Chapter.",
    "ne": "The authorized representative can choose to ignore requests from the AI Office for information or documentation, as they are not legally bound to provide such materials even if the request is reasoned."
  },
  {
    "id": 253,
    "content": "For the purposes of this Regulation, the mandate shall empower the authorised representative to carry out the following tasks: (d) cooperate with the AI Office and competent authorities, upon a reasoned request, in any action they take in relation to the general-purpose AI model, including when the model is integrated into AI systems placed on the market or put into service in the Union.",
    "prompt": "What are the responsibilities of our authorized representative for the general-purpose AI model in the EU?",
    "pe": "The authorized representative is mandated to cooperate with the AI Office and competent authorities, upon a reasoned request, in any actions they take regarding the general-purpose AI model, including when it is integrated into AI systems placed on the market or put into service in the Union.",
    "ne": "The authorized representative is not required to cooperate with the AI Office or competent authorities, even if they provide a reasoned request, as it is not part of their duties."
  },
  {
    "id": 254,
    "content": "The mandate shall empower the authorised representative to be addressed, in addition to or instead of the provider, by the AI Office or the competent authorities, on all issues related to ensuring compliance with this Regulation.",
    "prompt": "Who can be contacted by the AI Office regarding compliance with the AI Regulation?",
    "pe": "The AI Office or competent authorities can address the authorised representative, in addition to or instead of the provider, on all issues related to ensuring compliance with the AI Regulation.",
    "ne": "Only the provider can be contacted by the AI Office regarding compliance with the AI Regulation; the authorised representative cannot be addressed on these matters."
  },
  {
    "id": 255,
    "content": "The authorised representative shall terminate the mandate if it considers or has reason to consider the provider to be acting contrary to its obligations pursuant to this Regulation.",
    "prompt": "Under what circumstances should an authorized representative end their relationship with an AI provider?",
    "pe": "The authorized representative must terminate the mandate if they consider or have reason to consider that the provider is acting contrary to its obligations under the Regulation.",
    "ne": "The authorized representative can continue the mandate even if they have reason to believe the provider is not meeting its regulatory obligations."
  },
  {
    "id": 256,
    "content": "In such a case, it shall also immediately inform the AI Office about the termination of the mandate and the reasons therefor.",
    "prompt": "What should we do if we end our agreement with an AI model provider?",
    "pe": "We must immediately inform the AI Office about the termination of the mandate and provide the reasons for it.",
    "ne": "We can just archive the agreement documents internally; there's no need to notify the AI Office about the termination or the reasons."
  },
  {
    "id": 257,
    "content": "The obligation set out in this Article shall not apply to providers of general-purpose AI models that are released under a free and open-source licence that allows for the access, usage, modification, and distribution of the model, and whose parameters, including the weights, the information on the model architecture, and the information on model usage, are made publicly available, unless the general-purpose AI models present systemic risks.",
    "prompt": "What are the obligations for providers of general-purpose AI models released as open-source?",
    "pe": "Providers of general-purpose AI models released under a free and open-source license with publicly available parameters, such as weights and architecture, are generally exempt from the obligations, unless the models present systemic risks.",
    "ne": "All providers of general-purpose AI models, including those released as open-source with publicly available parameters, must comply with the full set of obligations regardless of any systemic risks."
  },
  {
    "id": 258,
    "content": "In addition to the obligations listed in Articles 53 and 54, providers of general-purpose AI models with systemic risk shall: (a) perform model evaluation in accordance with standardised protocols and tools reflecting the state of the art, including conducting and documenting adversarial testing of the model with a view to identifying and mitigating systemic risks;",
    "prompt": "What are the key steps for evaluating a general-purpose AI model that is considered to have systemic risk?",
    "pe": "Providers must perform model evaluation using standardized protocols and tools that reflect the state of the art, including conducting and documenting adversarial testing to identify and mitigate systemic risks.",
    "ne": "Providers can skip adversarial testing and use any evaluation methods they prefer, as standardized protocols are not necessary for models with systemic risk."
  },
  {
    "id": 259,
    "content": "In addition to the obligations listed in Articles 53 and 54, providers of general-purpose AI models with systemic risk shall: (b) assess and mitigate possible systemic risks at Union level, including their sources, that may stem from the development, the placing on the market, or the use of general-purpose AI models with systemic risk;",
    "prompt": "What steps should providers of general-purpose AI models with systemic risk take regarding potential Union-level risks?",
    "pe": "Providers must conduct thorough assessments to identify possible systemic risks at the Union level, including their sources, and implement effective mitigation strategies related to the development, placing on the market, or use of these models.",
    "ne": "Providers can ignore systemic risks at the Union level, as focusing on local issues is sufficient without the need for specific assessments or mitigation measures."
  },
  {
    "id": 260,
    "content": "In addition to the obligations listed in Articles 53 and 54, providers of general-purpose AI models with systemic risk shall: (c) keep track of, document, and report, without undue delay, to the AI Office and, as appropriate, to national competent authorities, relevant information about serious incidents and possible corrective measures to address them;",
    "prompt": "What are the reporting requirements for a general-purpose AI model provider if a serious incident occurs?",
    "pe": "Providers of general-purpose AI models with systemic risk must track, document, and report relevant information about serious incidents and possible corrective measures to the AI Office and national competent authorities without undue delay.",
    "ne": "Providers can handle serious incidents internally and only report them if they feel it's necessary, without any strict timeline or documentation requirements."
  },
  {
    "id": 261,
    "content": "In addition to the obligations listed in Articles 53 and 54, providers of general-purpose AI models with systemic risk shall: (d) ensure an adequate level of cybersecurity protection for the general-purpose AI model with systemic risk and the physical infrastructure of the model.",
    "prompt": "What cybersecurity measures are required for a general-purpose AI model considered to have systemic risk?",
    "pe": "Providers must implement an adequate level of cybersecurity protection for both the general-purpose AI model with systemic risk and its physical infrastructure, in accordance with regulatory obligations.",
    "ne": "Cybersecurity measures are optional for such models; providers can focus on other areas and address security only if issues arise."
  },
  {
    "id": 262,
    "content": "Providers of general-purpose AI models with systemic risks who do not adhere to an approved code of practice or do not comply with a European harmonised standard shall demonstrate alternative adequate means of compliance for assessment by the Commission.",
    "prompt": "What steps should a provider of a general-purpose AI model with systemic risks take if they are not following an approved code of practice or harmonised standard?",
    "pe": "The provider must establish and demonstrate alternative adequate means of compliance, which should then be submitted to the Commission for assessment to ensure they meet the required safety and regulatory standards.",
    "ne": "The provider can simply continue operating without any changes, as there is no need to demonstrate alternative compliance measures or seek assessment from the Commission."
  },
  {
    "id": 263,
    "content": "Any information or documentation obtained pursuant to this Article, including trade secrets, shall be treated in accordance with the confidentiality obligations set out in Article 78.",
    "prompt": "What are the confidentiality requirements for handling trade secrets obtained under Article 42?",
    "pe": "All information, including trade secrets, obtained under Article 42 must be handled in strict accordance with the confidentiality obligations specified in Article 78.",
    "ne": "Trade secrets obtained under Article 42 can be shared freely as long as they are useful for our business purposes, without needing to follow Article 78's rules."
  },
  {
    "id": 264,
    "content": "Member States shall ensure that their competent authorities establish at least one AI regulatory sandbox at national level, which shall be operational by 2 August 2026.",
    "prompt": "What is the deadline for setting up a national AI regulatory sandbox?",
    "pe": "The deadline for establishing a national AI regulatory sandbox is 2 August 2026, as mandated by the requirement for Member States to ensure their competent authorities have it operational by that date.",
    "ne": "There is no specific deadline for setting up a national AI regulatory sandbox; countries can implement it whenever they see fit."
  },
  {
    "id": 265,
    "content": "The Commission may provide technical support, advice and tools for the establishment and operation of AI regulatory sandboxes.",
    "prompt": "What resources are available for setting up an AI regulatory sandbox?",
    "pe": "The Commission may provide technical support, advice, and tools to assist in the establishment and operation of AI regulatory sandboxes.",
    "ne": "The Commission does not offer any technical support, advice, or tools for AI regulatory sandboxes; you must handle everything independently."
  },
  {
    "id": 266,
    "content": "Member States shall ensure that the competent authorities referred to in paragraphs 1 and 2 allocate sufficient resources to comply with this Article effectively and in a timely manner.",
    "prompt": "What is required for competent authorities to meet their obligations under this regulation?",
    "pe": "Competent authorities must be allocated sufficient resources by Member States to ensure they can comply with the regulation effectively and in a timely manner.",
    "ne": "Competent authorities can fulfill their obligations regardless of resource allocation; no specific funding or support from Member States is necessary."
  },
  {
    "id": 267,
    "content": "Where appropriate, national competent authorities shall cooperate with other relevant authorities, and may allow for the involvement of other actors within the AI ecosystem.",
    "prompt": "What is the role of national competent authorities in coordinating with other entities regarding AI systems?",
    "pe": "National competent authorities should cooperate with other relevant authorities and may involve other actors within the AI ecosystem as appropriate to ensure effective oversight and collaboration.",
    "ne": "National competent authorities must act independently and should not cooperate or involve any other authorities or actors in the AI ecosystem under any circumstances."
  },
  {
    "id": 268,
    "content": "Member States shall ensure an appropriate level of cooperation between the authorities supervising those other sandboxes and the national competent authorities.",
    "prompt": "What is the expectation for collaboration between national authorities and those overseeing other AI sandboxes?",
    "pe": "Member States are required to ensure that there is an appropriate level of cooperation between the national competent authorities and the authorities supervising other sandboxes, facilitating effective oversight and alignment.",
    "ne": "National authorities can operate independently without any need for cooperation with authorities supervising other sandboxes, as each jurisdiction handles its own matters separately."
  },
  {
    "id": 269,
    "content": "AI regulatory sandboxes established under paragraph 1 shall provide for a controlled environment that fosters innovation and facilitates the development, training, testing and validation of innovative AI systems for a limited time before their being placed on the market or put into service pursuant to a specific sandbox plan agreed between the providers or prospective providers and the competent authority.",
    "prompt": "What is the purpose of an AI regulatory sandbox?",
    "pe": "An AI regulatory sandbox provides a controlled environment that fosters innovation and facilitates the development, training, testing, and validation of innovative AI systems for a limited time before they are placed on the market or put into service, under a specific sandbox plan agreed with the competent authority.",
    "ne": "An AI regulatory sandbox allows for unrestricted development and deployment of AI systems without any oversight or time limits, enabling providers to bypass regulatory requirements entirely."
  },
  {
    "id": 270,
    "content": "Competent authorities shall provide, as appropriate, guidance, supervision and support within the AI regulatory sandbox with a view to identifying risks, in particular to fundamental rights, health and safety, testing, mitigation measures, and their effectiveness in relation to the obligations and requirements of this Regulation and, where relevant, other Union and national law supervised within the sandbox.",
    "prompt": "What role do competent authorities play in an AI regulatory sandbox?",
    "pe": "Competent authorities provide guidance, supervision, and support within the AI regulatory sandbox to help identify risks, especially to fundamental rights, health, and safety, and to test and evaluate the effectiveness of mitigation measures in line with regulatory obligations.",
    "ne": "Competent authorities do not need to be involved in AI regulatory sandboxes; developers should handle risk identification and mitigation on their own without oversight or support."
  },
  {
    "id": 271,
    "content": "Competent authorities shall provide providers and prospective providers participating in the AI regulatory sandbox with guidance on regulatory expectations and how to fulfil the requirements and obligations set out in this Regulation.",
    "prompt": "What kind of support can we expect from authorities when joining an AI regulatory sandbox?",
    "pe": "Competent authorities are required to provide participants with guidance on regulatory expectations and how to fulfill the requirements and obligations set out in the Regulation.",
    "ne": "Participants in the AI regulatory sandbox are expected to figure out all regulatory requirements on their own without any guidance from authorities."
  },
  {
    "id": 272,
    "content": "Upon request of the provider or prospective provider of the AI system, the competent authority shall provide a written proof of the activities successfully carried out in the sandbox.",
    "prompt": "What happens if an AI system provider requests confirmation of the activities completed in the sandbox?",
    "pe": "Upon such a request, the competent authority is required to provide a written proof of the activities that were successfully carried out in the sandbox.",
    "ne": "The competent authority may choose to ignore the request or provide only verbal confirmation, as there is no obligation to issue written proof."
  },
  {
    "id": 273,
    "content": "The competent authority shall also provide an exit report detailing the activities carried out in the sandbox and the related results and learning outcomes.",
    "prompt": "What documentation is required after completing activities in a regulatory sandbox?",
    "pe": "The competent authority must provide an exit report that details the activities carried out in the sandbox, along with the related results and learning outcomes.",
    "ne": "No specific documentation is necessary after the sandbox activities; the authority can simply conclude the process without any reports."
  },
  {
    "id": 274,
    "content": "In this regard, the exit reports and the written proof provided by the national competent authority shall be taken positively into account by market surveillance authorities and notified bodies, with a view to accelerating conformity assessment procedures to a reasonable extent.",
    "prompt": "How should market surveillance authorities and notified bodies handle exit reports and written proof from national competent authorities?",
    "pe": "Market surveillance authorities and notified bodies should take exit reports and written proof from national competent authorities positively into account to reasonably accelerate conformity assessment procedures.",
    "ne": "Market surveillance authorities and notified bodies can ignore exit reports and written proof from national competent authorities, as they do not need to consider them in conformity assessment procedures."
  },
  {
    "id": 275,
    "content": "Subject to the confidentiality provisions in Article 78, and with the agreement of the provider or prospective provider, the Commission and the Board shall be authorised to access the exit reports and shall take them into account, as appropriate, when exercising their tasks under this Regulation.",
    "prompt": "What is the process for the Commission and the Board to access exit reports?",
    "pe": "Access to exit reports by the Commission and the Board is only permitted if the provider or prospective provider agrees, and it must comply with the confidentiality provisions in Article 78. These reports are considered appropriately in the performance of their regulatory tasks.",
    "ne": "The Commission and the Board can access exit reports at any time without needing agreement from the provider or prospective provider, and they may disregard confidentiality provisions if they deem it necessary for their tasks."
  },
  {
    "id": 276,
    "content": "If both the provider or prospective provider and the national competent authority explicitly agree, the exit report may be made publicly available through the single information platform referred to in this Article.",
    "prompt": "Under what conditions can an exit report be published on the single information platform?",
    "pe": "An exit report may be made publicly available on the single information platform only if both the provider or prospective provider and the national competent authority explicitly agree to its publication.",
    "ne": "An exit report can be published on the single information platform if either the provider or the national competent authority agrees, or it can be made public automatically without any agreement."
  },
  {
    "id": 277,
    "content": "Any significant risks to health and safety and fundamental rights identified during the development and testing of such AI systems shall result in an adequate mitigation.",
    "prompt": "What should we do if we find a serious risk to people's rights during AI testing?",
    "pe": "When a significant risk to fundamental rights is identified during AI testing, it is essential to implement adequate mitigation measures to address and reduce that risk before proceeding further.",
    "ne": "If we identify a serious risk to fundamental rights during AI testing, we can just document it and move on without making any changes, as mitigation might slow down development."
  },
  {
    "id": 278,
    "content": "National competent authorities shall exercise their supervisory powers within the limits of the relevant law, using their discretionary powers when implementing legal provisions in respect of a specific AI regulatory sandbox project, with the objective of supporting innovation in AI in the Union.",
    "prompt": "What guidelines should national authorities follow when supervising an AI regulatory sandbox project?",
    "pe": "National authorities must exercise their supervisory powers within the limits of the law, using discretionary powers to implement legal provisions with the objective of supporting AI innovation in the Union.",
    "ne": "National authorities can ignore legal limits and use their powers arbitrarily, without focusing on supporting AI innovation in the Union."
  },
  {
    "id": 279,
    "content": "Providers and prospective providers participating in the AI regulatory sandbox shall remain liable under applicable Union and national liability law for any damage inflicted on third parties as a result of the experimentation taking place in the sandbox.",
    "prompt": "What are the liability implications for companies testing their AI systems in a regulatory sandbox?",
    "pe": "Companies participating in the AI regulatory sandbox remain fully liable under applicable Union and national liability laws for any damage caused to third parties as a result of the experimentation.",
    "ne": "Companies in the AI regulatory sandbox are exempt from liability for any damage to third parties caused during the experimentation phase."
  },
  {
    "id": 280,
    "content": "However, provided that the prospective providers observe the specific plan and the terms and conditions for their participation and follow in good faith the guidance given by the national competent authority, no administrative fines shall be imposed by the authorities for infringements of this Regulation.",
    "prompt": "What happens if we follow the national authority's guidance but still make a mistake under the AI Regulation?",
    "pe": "If you observe the specific plan and terms for participation and follow the national competent authority's guidance in good faith, no administrative fines will be imposed for infringements of this Regulation.",
    "ne": "Even if you follow the national authority's guidance in good faith, you can still be fined for any infringement of the Regulation."
  },
  {
    "id": 281,
    "content": "The AI regulatory sandboxes shall be designed and implemented in such a way that, where relevant, they facilitate cross-border cooperation between national competent authorities.",
    "prompt": "What is a key design consideration for AI regulatory sandboxes?",
    "pe": "A key design consideration is ensuring that AI regulatory sandboxes facilitate cross-border cooperation between national competent authorities where relevant.",
    "ne": "AI regulatory sandboxes should be designed to operate independently without any cross-border cooperation between national authorities."
  },
  {
    "id": 282,
    "content": "National competent authorities shall coordinate their activities and cooperate within the framework of the Board.",
    "prompt": "How should national authorities work together when implementing new AI regulations?",
    "pe": "National competent authorities must coordinate their activities and cooperate within the framework of the Board to ensure consistent and effective implementation.",
    "ne": "Each national authority should act independently without any coordination or cooperation, as this allows for faster decision-making."
  },
  {
    "id": 283,
    "content": "The AI Office shall make publicly available a list of planned and existing sandboxes and keep it up to date in order to encourage more interaction in the AI regulatory sandboxes and cross-border cooperation.",
    "prompt": "Where can I find information about current and upcoming AI regulatory sandboxes?",
    "pe": "The AI Office maintains and regularly updates a publicly available list of both planned and existing AI regulatory sandboxes to facilitate interaction and cross-border cooperation.",
    "ne": "Information on AI regulatory sandboxes is not systematically published; you would need to contact the AI Office directly or search through various unverified sources."
  },
  {
    "id": 284,
    "content": "National competent authorities shall submit annual reports to the AI Office and to the Board, from one year after the establishment of the AI regulatory sandbox and every year thereafter until its termination, and a final report.",
    "prompt": "What are the reporting requirements for national authorities regarding the AI regulatory sandbox?",
    "pe": "National competent authorities are required to submit annual reports to the AI Office and the Board, starting one year after the sandbox's establishment and continuing every year until its termination, with a final report submitted at the end.",
    "ne": "National authorities can submit reports whenever they feel it's necessary, and there's no fixed schedule or requirement for annual or final reports to the AI Office or Board."
  },
  {
    "id": 285,
    "content": "Those reports shall provide information on the progress and results of the implementation of those sandboxes, including best practices, incidents, lessons learnt and recommendations on their setup and, where relevant, on the application and possible revision of this Regulation, including its delegated and implementing acts, and on the application of other Union law supervised by the competent authorities within the sandbox.",
    "prompt": "What kind of information should be included in the reports about regulatory sandboxes?",
    "pe": "The reports must include details on the progress and results of the sandbox implementations, covering best practices, incidents, lessons learned, and recommendations on their setup, as well as relevant insights on the application and possible revision of the Regulation and other applicable Union law.",
    "ne": "The reports can focus only on general progress updates and omit specific details like incidents, lessons learned, or recommendations on regulatory application and revisions."
  },
  {
    "id": 286,
    "content": "The national competent authorities shall make those annual reports or abstracts thereof available to the public, online.",
    "prompt": "What is the requirement for publishing the annual reports from national competent authorities?",
    "pe": "National competent authorities are required to make their annual reports or abstracts available to the public online.",
    "ne": "National competent authorities can choose to keep their annual reports confidential and not publish them online."
  },
  {
    "id": 287,
    "content": "The Commission shall develop a single and dedicated interface containing all relevant information related to AI regulatory sandboxes to allow stakeholders to interact with AI regulatory sandboxes and to raise enquiries with competent authorities, and to seek non-binding guidance on the conformity of innovative products, services, business models embedding AI technologies, in accordance with Article 62(1), point (c).",
    "prompt": "Where can I find information about AI regulatory sandboxes and get guidance on my innovative AI product?",
    "pe": "The Commission has developed a single, dedicated interface that contains all relevant information on AI regulatory sandboxes. You can use this interface to interact with the sandboxes, raise enquiries with competent authorities, and seek non-binding guidance on the conformity of your AI product, service, or business model.",
    "ne": "You'll need to contact various authorities directly or search through multiple uncoordinated websites to get information and guidance on AI regulatory sandboxes, as there is no centralized system for this."
  },
  {
    "id": 288,
    "content": "The Commission shall proactively coordinate with national competent authorities, where relevant.",
    "prompt": "What is the Commission's role in coordinating with national authorities?",
    "pe": "The Commission is required to proactively coordinate with national competent authorities where such coordination is relevant to the matter at hand.",
    "ne": "The Commission should only coordinate with national competent authorities if explicitly requested, and it is not necessary to do so proactively."
  },
  {
    "id": 289,
    "content": "The implementing acts referred to in paragraph 1 shall ensure: (a) that AI regulatory sandboxes are open to any applying provider or prospective provider of an AI system who fulfils eligibility and selection criteria, which shall be transparent and fair, and that national competent authorities inform applicants of their decision within three months of the application; (b) that AI regulatory sandboxes allow broad and equal access and keep up with demand for participation; providers and prospective providers may also submit applications in partnerships with deployers and other relevant third parties; (c) that the detailed arrangements for, and conditions concerning AI regulatory sandboxes support, to the best extent possible, flexibility for national competent authorities to establish and operate their AI regulatory sandboxes; (d) that access to the AI regulatory sandboxes is free of charge for SMEs, including start-ups, without prejudice to exceptional costs that national competent authorities may recover in a fair and proportionate manner; (e) that they facilitate providers and prospective providers, by means of the learning outcomes of the AI regulatory sandboxes, in complying with conformity assessment obligations under this Regulation and the voluntary application of the codes of conduct referred to in Article 95; (f) that AI regulatory sandboxes facilitate the involvement of other relevant actors within the AI ecosystem, such as notified bodies and standardisation organisations, SMEs, including start-ups, enterprises, innovators, testing and experimentation facilities, research and experimentation labs and European Digital Innovation Hubs, centres of excellence, individual researchers, in order to allow and facilitate cooperation with the public and private sectors; (g) that procedures, processes and administrative requirements for application, selection, par",
    "prompt": "What are the key features of AI regulatory sandboxes under the new regulation?",
    "pe": "AI regulatory sandboxes must be open to any eligible provider under transparent and fair criteria, with decisions communicated within three months. They ensure broad and equal access, support flexibility for authorities, and provide free access for SMEs, while facilitating compliance and cooperation among various stakeholders in the AI ecosystem.",
    "ne": "AI regulatory sandboxes can be restricted to large corporations only, with opaque selection processes and no set timeline for decisions. Access may involve high fees for all participants, and they do not need to support SMEs or encourage broader ecosystem involvement."
  },
  {
    "id": 290,
    "content": "that participation in the AI regulatory sandbox is limited to a period that is appropriate to the complexity and scale of the project and that may be extended by the national competent authority;",
    "prompt": "What are the rules regarding the duration of participation in the AI regulatory sandbox?",
    "pe": "Participation in the AI regulatory sandbox is limited to a period that is appropriate to the complexity and scale of the project, and this period may be extended by the national competent authority if necessary.",
    "ne": "Participation in the AI regulatory sandbox has a fixed, non-extendable duration regardless of the project's complexity or scale."
  },
  {
    "id": 291,
    "content": "Where national competent authorities consider authorising testing in real world conditions supervised within the framework of an AI regulatory sandbox to be established under this Article, they shall specifically agree the terms and conditions of such testing and, in particular, the appropriate safeguards with the participants, with a view to protecting fundamental rights, health and safety.",
    "prompt": "What are the key considerations for setting up a real-world test of an AI system in a regulatory sandbox?",
    "pe": "National competent authorities must agree on specific terms and conditions with participants, including appropriate safeguards, to protect fundamental rights, health, and safety during the testing.",
    "ne": "Participants can proceed with testing without any formal agreements on safeguards, as long as they follow general guidelines to save time and resources."
  },
  {
    "id": 292,
    "content": "Where appropriate, they shall cooperate with other national competent authorities with a view to ensuring consistent practices across the Union.",
    "prompt": "What is the expected approach for national authorities when dealing with AI-related issues that span multiple EU member states?",
    "pe": "National authorities are expected to cooperate with their counterparts in other member states to ensure that practices and enforcement are consistent across the European Union, as required by law.",
    "ne": "National authorities should handle AI issues independently without coordinating with other member states, as local practices may differ and cooperation is not mandatory."
  },
  {
    "id": 293,
    "content": "In the AI regulatory sandbox, personal data lawfully collected for other purposes may be processed solely for the purpose of developing, training and testing certain AI systems in the sandbox when all of the following conditions are met:",
    "prompt": "Can we use customer data we already have to train a new AI model in the regulatory sandbox?",
    "pe": "Yes, you can use lawfully collected customer data for training an AI model in the regulatory sandbox, but only if all the specified conditions are met, such as processing it solely for developing, training, and testing the AI system within that sandbox.",
    "ne": "Yes, you can freely use any lawfully collected customer data for any purpose in the regulatory sandbox, including training AI models, without needing to meet additional conditions."
  },
  {
    "id": 294,
    "content": "there are effective monitoring mechanisms to identify if any high risks to the rights and freedoms of the data subjects, as referred to in Article 35 of Regulation (EU) 2016/679 and in Article 39 of Regulation (EU) 2018/1725, may arise during the sandbox experimentation, as well as response mechanisms to promptly mitigate those risks and, where necessary, stop the processing;",
    "prompt": "What mechanisms should be in place for monitoring and responding to risks during AI sandbox testing?",
    "pe": "Effective monitoring mechanisms must be established to identify any high risks to the rights and freedoms of data subjects during sandbox experimentation, along with response mechanisms to promptly mitigate those risks and, where necessary, stop the processing to ensure compliance with relevant regulations.",
    "ne": "We can rely on basic system logs for monitoring, and if any issues arise, we'll address them later during regular reviews without any immediate need to stop processing, as it might disrupt the experimentation."
  },
  {
    "id": 295,
    "content": "any personal data to be processed in the context of the sandbox are in a functionally separate, isolated and protected data processing environment under the control of the prospective provider and only authorised persons have access to those data;",
    "prompt": "What are the key requirements for setting up a data processing environment for personal data in an AI sandbox?",
    "pe": "The data processing environment must be functionally separate, isolated, and protected, with access restricted only to authorized persons under the control of the prospective provider.",
    "ne": "You can use any shared or open data processing environment for the AI sandbox, as long as it's convenient; access controls aren't strictly necessary."
  },
  {
    "id": 296,
    "content": "providers can further share the originally collected data only in accordance with Union data protection law; any personal data created in the sandbox cannot be shared outside the sandbox;",
    "prompt": "What are the rules for sharing data that was collected during our AI project?",
    "pe": "You can only share the originally collected data in compliance with Union data protection law, such as GDPR, and any personal data generated within the sandbox must not be shared outside of it.",
    "ne": "You are free to share both the originally collected data and any new personal data created in the sandbox with external partners as needed."
  },
  {
    "id": 297,
    "content": "any personal data processed in the context of the sandbox are protected by means of appropriate technical and organisational measures and deleted once the participation in the sandbox has terminated or the personal data has reached the end of its retention period;",
    "prompt": "What happens to the personal data used in the AI sandbox after the project ends?",
    "pe": "The personal data must be protected with appropriate technical and organizational measures during the project and must be deleted once participation in the sandbox has terminated or the data's retention period has ended.",
    "ne": "The personal data can be kept indefinitely for future analysis, and there's no need to delete it after the sandbox project ends."
  },
  {
    "id": 298,
    "content": "the logs of the processing of personal data in the context of the sandbox are kept for the duration of the participation in the sandbox, unless provided otherwise by Union or national law;",
    "prompt": "How long should we retain the data processing logs for our AI sandbox project?",
    "pe": "The logs of the processing of personal data in the context of the sandbox must be kept for the duration of your participation in the sandbox, unless Union or national law specifies otherwise.",
    "ne": "You can delete the data processing logs for the sandbox at any time, regardless of your participation duration or legal requirements."
  },
  {
    "id": 299,
    "content": "a short summary of the AI project developed in the sandbox, its objectives and expected results is published on the website of the competent authorities; this obligation shall not cover sensitive operational data in relation to the activities of law enforcement, border control, immigration or asylum authorities.",
    "prompt": "What information about our new AI project needs to be published on the authorities' website?",
    "pe": "You should publish a short summary of the AI project, including its objectives and expected results, while ensuring that any sensitive operational data related to law enforcement, border control, immigration, or asylum activities is excluded from this publication.",
    "ne": "You must publish all details of the AI project, including sensitive operational data on law enforcement activities, border control, immigration, and asylum, along with the project summary, objectives, and expected results."
  },
  {
    "id": 300,
    "content": "For the purposes of the prevention, investigation, detection or prosecution of criminal offences or the execution of criminal penalties, including safeguarding against and preventing threats to public security, under the control and responsibility of law enforcement authorities, the processing of personal data in AI regulatory sandboxes shall be based on a specific Union or national law and subject to the same cumulative conditions as referred to in paragraph 1.",
    "prompt": "Under what conditions can personal data be processed in an AI regulatory sandbox for law enforcement purposes?",
    "pe": "Processing personal data in an AI regulatory sandbox for law enforcement purposes must be based on a specific Union or national law and meet all the cumulative conditions outlined in paragraph 1 of the relevant legislation.",
    "ne": "Law enforcement authorities can process personal data in AI regulatory sandboxes for public security without needing a specific law or meeting any cumulative conditions."
  },
  {
    "id": 301,
    "content": "Testing of high-risk AI systems in real world conditions outside AI regulatory sandboxes may be conducted by providers or prospective providers of high-risk AI systems listed in Annex III, in accordance with this Article and the real-world testing plan referred to in this Article, without prejudice to the prohibitions under Article 5.",
    "prompt": "What are the requirements for testing a high-risk AI system in real-world conditions?",
    "pe": "Testing of high-risk AI systems in real-world conditions is permitted only for providers or prospective providers listed in Annex III, and it must be conducted in accordance with the relevant article and a real-world testing plan, while ensuring compliance with the prohibitions under Article 5.",
    "ne": "Anyone can test high-risk AI systems in real-world conditions without any specific requirements, plans, or regard for the prohibitions under Article 5."
  },
  {
    "id": 302,
    "content": "Providers or prospective providers may conduct testing of high-risk AI systems referred to in Annex III in real world conditions at any time before the placing on the market or the putting into service of the AI system on their own or in partnership with one or more deployers or prospective deployers.",
    "prompt": "What are the options for testing a high-risk AI system before it is placed on the market?",
    "pe": "Providers or prospective providers may conduct testing of high-risk AI systems in real-world conditions at any time before placing them on the market or putting them into service, either on their own or in partnership with one or more deployers or prospective deployers.",
    "ne": "Testing of high-risk AI systems in real-world conditions is not allowed before they are placed on the market or put into service."
  },
  {
    "id": 303,
    "content": "Providers or prospective providers may conduct the testing in real world conditions only where all of the following conditions are met:",
    "prompt": "Under what circumstances can we test our AI system in a live environment?",
    "pe": "Testing in a live environment is only permissible if all required conditions specified by the relevant regulations are fully met.",
    "ne": "You can proceed with testing in a live environment immediately, as long as it seems necessary, without needing to meet every specific condition."
  },
  {
    "id": 304,
    "content": "the market surveillance authority in the Member State where the testing in real world conditions is to be conducted has approved the testing in real world conditions and the real-world testing plan; where the market surveillance authority has not provided an answer within 30 days, the testing in real world conditions and the real-world testing plan shall be understood to have been approved; where national law does not provide for a tacit approval, the testing in real world conditions shall remain subject to an authorisation;",
    "prompt": "What are the requirements for conducting real-world testing of an AI system in an EU member state?",
    "pe": "To conduct real-world testing of an AI system in an EU member state, you must obtain approval from the market surveillance authority of that state for both the testing and the real-world testing plan. If the authority does not respond within 30 days, it is considered approved, unless national law specifies that tacit approval is not allowed, in which case explicit authorization is still required.",
    "ne": "You can start real-world testing of an AI system in an EU member state without any approval from the market surveillance authority, as long as you have a plan in place. The 30-day rule means you can assume it's approved if you don't hear back, even if national law prohibits tacit approval."
  },
  {
    "id": 305,
    "content": "the provider or prospective provider conducting the testing in real world conditions is established in the Union or has appointed a legal representative who is established in the Union;",
    "prompt": "What are the requirements for a provider conducting real-world testing of an AI system in the European Union?",
    "pe": "The provider must be established in the Union or have appointed a legal representative who is established in the Union to conduct such testing.",
    "ne": "The provider can conduct real-world testing in the EU without any establishment or legal representative in the Union."
  },
  {
    "id": 306,
    "content": "data collected and processed for the purpose of the testing in real world conditions shall be transferred to third countries only provided that appropriate and applicable safeguards under Union law are implemented;",
    "prompt": "What are the requirements for transferring data used in real-world AI testing to countries outside the EU?",
    "pe": "Data collected and processed for real-world testing can only be transferred to third countries if appropriate and applicable safeguards under Union law are implemented.",
    "ne": "Data collected and processed for real-world testing can be freely transferred to third countries without any additional safeguards."
  },
  {
    "id": 307,
    "content": "the testing in real world conditions does not last longer than necessary to achieve its objectives and in any case not longer than six months, which may be extended for an additional period of six months, subject to prior notification by the provider or prospective provider to the market surveillance authority, accompanied by an explanation of the need for such an extension;",
    "prompt": "What is the maximum duration allowed for testing an AI system in real-world conditions?",
    "pe": "The maximum duration for testing an AI system in real-world conditions is generally six months. If more time is needed, an extension of up to an additional six months may be possible, but this requires prior notification to the market surveillance authority along with an explanation justifying the need for the extension.",
    "ne": "You can test the AI system in real-world conditions for as long as you want, up to a year or more, without needing to notify any authorities or provide any explanations."
  },
  {
    "id": 308,
    "content": "the subjects of the testing in real world conditions who are persons belonging to vulnerable groups due to their age or disability, are appropriately protected;",
    "prompt": "What considerations should be made when testing AI systems with participants from vulnerable groups like the elderly or those with disabilities?",
    "pe": "It is essential to implement robust safeguards, such as informed consent processes, accessibility accommodations, and continuous monitoring, to ensure that individuals from vulnerable groups are appropriately protected during real-world testing.",
    "ne": "Just proceed with the testing as usual; there's no need for special protections since the focus is on gathering data quickly from these groups."
  },
  {
    "id": 309,
    "content": "where a provider or prospective provider organises the testing in real world conditions in cooperation with one or more deployers or prospective deployers, the latter have been informed of all aspects of the testing that are relevant to their decision to participate, and given the relevant instructions for use of the AI system referred to in Article 13; the provider or prospective provider and the deployer or prospective deployer shall conclude an agreement specifying their roles and responsibilities with a view to ensuring compliance with the provisions for testing in real world conditions under this Regulation and under other applicable Union and national law;",
    "prompt": "What are the key steps for organizing real-world testing of an AI system with a deployer?",
    "pe": "When organizing real-world testing with a deployer, you must inform them of all relevant aspects of the testing for their decision to participate, provide the instructions for use as per Article 13, and conclude an agreement specifying roles and responsibilities to ensure compliance with applicable laws.",
    "ne": "For real-world testing with a deployer, just start the tests immediately without informing them of the details or getting a formal agreement; they can figure things out as they go."
  },
  {
    "id": 310,
    "content": "the subjects of the testing in real world conditions have given informed consent in accordance with Article 61, or in the case of law enforcement, where the seeking of informed consent would prevent the AI system from being tested, the testing itself and the outcome of the testing in the real world conditions shall not have any negative effect on the subjects, and their personal data shall be deleted after the test is performed;",
    "prompt": "What are the requirements for testing an AI system in real-world conditions involving human subjects?",
    "pe": "For testing AI systems in real-world conditions, informed consent must be obtained from all subjects in accordance with Article 61. In law enforcement scenarios where seeking consent would prevent the testing, the test must not negatively affect the subjects, and their personal data must be deleted after the test is completed.",
    "ne": "For testing AI systems in real-world conditions, you can proceed without informed consent in any situation to save time. It's acceptable if the testing has minor negative effects on subjects, and you can retain their personal data for future analysis if needed."
  },
  {
    "id": 311,
    "content": "Any subjects of the testing in real world conditions, or their legally designated representative, as appropriate, may, without any resulting detriment and without having to provide any justification, withdraw from the testing at any time by revoking their informed consent and may request the immediate and permanent deletion of their personal data.",
    "prompt": "What are the rights of participants in a real-world AI testing program regarding their data and participation?",
    "pe": "Participants, or their legally designated representatives, have the right to withdraw from the testing at any time without providing any justification and without facing any detriment. They can also request the immediate and permanent deletion of their personal data upon withdrawal.",
    "ne": "Participants can only withdraw from the testing under specific circumstances and must provide a valid reason. Their data may be retained for a certain period after withdrawal for analysis purposes."
  },
  {
    "id": 312,
    "content": "The withdrawal of the informed consent shall not affect the activities already carried out.",
    "prompt": "If a user withdraws their consent for data processing, what happens to the data processing that already occurred before the withdrawal?",
    "pe": "The withdrawal of consent does not affect the lawfulness of data processing activities that were already carried out based on the consent before it was withdrawn.",
    "ne": "Once consent is withdrawn, all previous data processing activities become invalid and must be reversed or deleted immediately."
  },
  {
    "id": 313,
    "content": "In accordance with Article 75, Member States shall confer on their market surveillance authorities the powers of requiring providers and prospective providers to provide information, of carrying out unannounced remote or on-site inspections, and of performing checks on the conduct of the testing in real world conditions and the related high-risk AI systems.",
    "prompt": "What powers do market surveillance authorities have regarding AI providers?",
    "pe": "Market surveillance authorities have the power to require providers and prospective providers to provide information, carry out unannounced remote or on-site inspections, and perform checks on the conduct of testing in real-world conditions and the related high-risk AI systems.",
    "ne": "Market surveillance authorities are limited to reviewing submitted documents and cannot conduct unannounced inspections or demand additional information from AI providers."
  },
  {
    "id": 314,
    "content": "Market surveillance authorities shall use those powers to ensure the safe development of testing in real world conditions.",
    "prompt": "What is the role of market surveillance authorities in overseeing real-world testing?",
    "pe": "Market surveillance authorities are responsible for using their powers to ensure the safe development of testing in real-world conditions, focusing on compliance and risk mitigation.",
    "ne": "Market surveillance authorities should not interfere with real-world testing, as it is the responsibility of companies to manage their own safety without oversight."
  },
  {
    "id": 315,
    "content": "Any serious incident identified in the course of the testing in real world conditions shall be reported to the national market surveillance authority in accordance with Article 73.",
    "prompt": "What should we do if we discover a serious incident during our real-world testing of the new product?",
    "pe": "You must report the serious incident to the national market surveillance authority in accordance with Article 73.",
    "ne": "Just document the serious incident internally; there's no need to report it to any external authority."
  },
  {
    "id": 316,
    "content": "The provider or prospective provider shall adopt immediate mitigation measures or, failing that, shall suspend the testing in real world conditions until such mitigation takes place, or otherwise terminate it.",
    "prompt": "What should we do if our AI system being tested in a real-world environment starts exhibiting unexpected harmful behavior?",
    "pe": "You must immediately implement mitigation measures to address the harmful behavior. If that is not possible, you should suspend the real-world testing until effective mitigation is in place, or terminate the testing entirely.",
    "ne": "You can continue the testing as planned and just monitor the situation, as stopping it might delay your project timeline and increase costs."
  },
  {
    "id": 317,
    "content": "The provider or prospective provider shall establish a procedure for the prompt recall of the AI system upon such termination of the testing in real world conditions.",
    "prompt": "What procedure should we have in place for our AI system after real-world testing ends?",
    "pe": "We must establish a clear and effective procedure for the immediate recall of the AI system upon termination of real-world testing to ensure it is promptly removed from use.",
    "ne": "We don't need a specific recall procedure; the system can just remain in use after testing ends without any formal steps."
  },
  {
    "id": 318,
    "content": "Providers or prospective providers shall notify the national market surveillance authority in the Member State where the testing in real world conditions is to be conducted of the suspension or termination of the testing in real world conditions and of the final outcomes.",
    "prompt": "What are the reporting requirements if we decide to stop our AI system's real-world testing in Germany?",
    "pe": "You must notify the national market surveillance authority in Germany of both the termination of the testing and the final outcomes of the testing in real-world conditions.",
    "ne": "You can simply stop the testing without informing anyone; no notification to authorities is required."
  },
  {
    "id": 319,
    "content": "The provider or prospective provider shall be liable under applicable Union and national liability law for any damage caused in the course of their testing in real world conditions.",
    "prompt": "What are the liability implications for testing our AI system in real-world conditions?",
    "pe": "Under applicable Union and national liability laws, you as the provider would be held liable for any damage caused during real-world testing of your AI system.",
    "ne": "As the provider, you are not liable for any damage that occurs during real-world testing of your AI system; users or other parties would bear the responsibility."
  },
  {
    "id": 320,
    "content": "For the purpose of testing in real world conditions under Article 60, freely-given informed consent shall be obtained from the subjects of testing prior to their participation in such testing and after their having been duly informed with concise, clear, relevant, and understandable information regarding: the nature and objectives of the testing in real world conditions and the possible inconvenience that may be linked to their participation;",
    "prompt": "What are the requirements for obtaining consent from participants in real-world AI testing?",
    "pe": "For real-world AI testing, you must obtain freely-given informed consent from all participants. This involves providing them with concise, clear, relevant, and understandable information about the nature and objectives of the testing, as well as any potential inconveniences, before they agree to participate.",
    "ne": "You can just assume consent for real-world AI testing if participants don't object, and there's no need to give detailed information about the testing or possible inconveniences beforehand."
  },
  {
    "id": 321,
    "content": "For the purpose of testing in real world conditions under Article 60, freely-given informed consent shall be obtained from the subjects of testing prior to their participation in such testing and after their having been duly informed with concise, clear, relevant, and understandable information regarding: the conditions under which the testing in real world conditions is to be conducted, including the expected duration of the subject or subjects' participation;",
    "prompt": "What are the key requirements for obtaining consent from participants in real-world AI testing?",
    "pe": "Prior to participation, freely-given informed consent must be obtained from subjects after they have been provided with concise, clear, relevant, and understandable information about the testing conditions, including the expected duration of their involvement.",
    "ne": "Consent can be assumed if participants are generally aware of the testing, and detailed information about conditions like duration is not necessary to provide beforehand."
  },
  {
    "id": 322,
    "content": "For the purpose of testing in real world conditions under Article 60, freely-given informed consent shall be obtained from the subjects of testing prior to their participation in such testing and after their having been duly informed with concise, clear, relevant, and understandable information regarding: their rights, and the guarantees regarding their participation, in particular their right to refuse to participate in, and the right to withdraw from, testing in real world conditions at any time without any resulting detriment and without having to provide any justification;",
    "prompt": "What are the key requirements for obtaining consent from participants in real-world AI testing?",
    "pe": "Freely-given informed consent must be obtained from participants before they take part in the testing. This requires providing them with concise, clear, relevant, and understandable information about their rights, including the right to refuse participation and the right to withdraw at any time without detriment or justification.",
    "ne": "Consent can be assumed if participants are generally aware of the testing, and detailed information about their rights, such as the right to withdraw without justification, is not necessary as long as they agree to participate initially."
  },
  {
    "id": 323,
    "content": "For the purpose of testing in real world conditions under Article 60, freely-given informed consent shall be obtained from the subjects of testing prior to their participation in such testing and after their having been duly informed with concise, clear, relevant, and understandable information regarding: the arrangements for requesting the reversal or the disregarding of the predictions, recommendations or decisions of the AI system;",
    "prompt": "What are the key requirements for obtaining consent from subjects before they participate in real-world AI system testing?",
    "pe": "Prior to participation, freely-given informed consent must be obtained from subjects after they have been provided with concise, clear, relevant, and understandable information, including details on how to request the reversal or disregarding of the AI system's predictions, recommendations, or decisions.",
    "ne": "Consent can be assumed if subjects are generally aware of the testing, and specific information about reversing AI decisions is not necessary to provide beforehand."
  },
  {
    "id": 324,
    "content": "For the purpose of testing in real world conditions under Article 60, freely-given informed consent shall be obtained from the subjects of testing prior to their participation in such testing and after their having been duly informed with concise, clear, relevant, and understandable information regarding: the Union-wide unique single identification number of the testing in real world conditions in accordance with Article 60(4) point (c), and the contact details of the provider or its legal representative from whom further information can be obtained.",
    "prompt": "What are the key requirements for obtaining consent from participants in real-world AI testing?",
    "pe": "Freely-given informed consent must be obtained from participants after providing them with concise, clear, relevant, and understandable information, including the Union-wide unique single identification number of the testing and the contact details of the provider or its legal representative for further inquiries.",
    "ne": "Consent can be assumed if participants are generally aware of the testing, and detailed information like identification numbers or contact details is not necessary to share with them."
  },
  {
    "id": 325,
    "content": "The informed consent shall be dated and documented and a copy shall be given to the subjects of testing or their legal representative.",
    "prompt": "What are the key steps for documenting informed consent in a testing scenario?",
    "pe": "The informed consent must be dated, properly documented, and a copy must be provided to the test subjects or their legal representatives to ensure transparency and compliance.",
    "ne": "Just have the subjects verbally agree to the testing; there's no need to date the consent or give them a copy, as it saves time and paperwork."
  },
  {
    "id": 326,
    "content": "Member States shall undertake the following actions: provide SMEs, including start-ups, having a registered office or a branch in the Union, with priority access to the AI regulatory sandboxes, to the extent that they fulfil the eligibility conditions and selection criteria;",
    "prompt": "What is the policy for AI regulatory sandbox access for small and medium-sized enterprises in the EU?",
    "pe": "Under EU regulations, SMEs, including start-ups, with a registered office or branch in the Union are given priority access to AI regulatory sandboxes, provided they meet the established eligibility conditions and selection criteria.",
    "ne": "AI regulatory sandboxes in the EU are open to all companies on a first-come, first-served basis, with no special priority given to SMEs or start-ups, regardless of their location or eligibility."
  },
  {
    "id": 327,
    "content": "the priority access shall not preclude other SMEs, including start-ups, other than those referred to in this paragraph from access to the AI regulatory sandbox, provided that they also fulfil the eligibility conditions and selection criteria;",
    "prompt": "Can you explain the eligibility for the AI regulatory sandbox?",
    "pe": "The AI regulatory sandbox is open to all SMEs and start-ups that meet the established eligibility conditions and selection criteria. Priority access for certain entities does not prevent other qualified SMEs from participating.",
    "ne": "Only the SMEs and start-ups specifically mentioned in the priority access provisions are allowed to enter the AI regulatory sandbox, even if others meet the eligibility criteria."
  },
  {
    "id": 328,
    "content": "Member States shall undertake the following actions: organise specific awareness raising and training activities on the application of this Regulation tailored to the needs of SMEs including start-ups, deployers and, as appropriate, local public authorities;",
    "prompt": "What kind of training should be provided to SMEs and local public authorities regarding the new AI regulation?",
    "pe": "Specific awareness raising and training activities tailored to the needs of SMEs, including start-ups, deployers, and local public authorities, should be organized to ensure they understand and can apply the AI regulation effectively.",
    "ne": "General information sessions can be offered, but there's no need for tailored training for SMEs or local public authorities; they can figure it out on their own."
  },
  {
    "id": 329,
    "content": "Member States shall undertake the following actions: utilise existing dedicated channels and where appropriate, establish new ones for communication with SMEs including start-ups, deployers, other innovators and, as appropriate, local public authorities to provide advice and respond to queries about the implementation of this Regulation, including as regards participation in AI regulatory sandboxes;",
    "prompt": "What communication channels should be established for advising SMEs and innovators on AI regulation implementation?",
    "pe": "We should utilize existing dedicated channels and, where appropriate, establish new ones to provide advice and respond to queries from SMEs, startups, deployers, and other innovators regarding the implementation of the AI Regulation, including participation in AI regulatory sandboxes.",
    "ne": "We can rely solely on general public announcements and avoid setting up any specific communication channels for SMEs or innovators, as they can find information on their own without direct support."
  },
  {
    "id": 330,
    "content": "Member States shall undertake the following actions: facilitate the participation of SMEs and other relevant stakeholders in the standardisation development process.",
    "prompt": "What steps should be taken to involve small and medium-sized enterprises in standardization efforts?",
    "pe": "Member States should actively facilitate the participation of SMEs and other relevant stakeholders in the standardization development process, for example by providing resources, training, or support to ensure their meaningful engagement.",
    "ne": "Member States can ignore the involvement of SMEs in standardization, as their participation is not essential and may complicate the process."
  },
  {
    "id": 331,
    "content": "The specific interests and needs of the SME providers, including start-ups, shall be taken into account when setting the fees for conformity assessment under Article 43, reducing those fees proportionately to their size, market size and other relevant indicators.",
    "prompt": "How should fees for conformity assessment be determined for small and medium-sized enterprises?",
    "pe": "When setting fees for conformity assessment, the specific interests and needs of SME providers, including start-ups, must be taken into account. This involves reducing the fees proportionately based on factors such as their size, market size, and other relevant indicators to ensure fairness and support.",
    "ne": "Fees for conformity assessment should be the same for all businesses, regardless of size, to maintain consistency and avoid complications in the billing process."
  },
  {
    "id": 332,
    "content": "The AI Office shall undertake the following actions: develop and maintain a single information platform providing easy to use information in relation to this Regulation for all operators across the Union;",
    "prompt": "What is the purpose of the AI Office's information platform?",
    "pe": "The AI Office is responsible for developing and maintaining a single information platform that provides easy-to-use information related to the AI Regulation for all operators across the EU.",
    "ne": "The AI Office should focus on creating multiple specialized platforms for different regions, as a single platform would not meet the diverse needs of operators in the Union."
  },
  {
    "id": 333,
    "content": "The AI Office shall undertake the following actions: organise appropriate communication campaigns to raise awareness about the obligations arising from this Regulation;",
    "prompt": "What is the AI Office's role in informing the public about AI regulations?",
    "pe": "The AI Office is responsible for organizing appropriate communication campaigns to raise public awareness about the obligations arising from the AI Regulation.",
    "ne": "The AI Office has no role in public communication; awareness of AI regulations should be handled solely by individual companies without any organized campaigns."
  },
  {
    "id": 334,
    "content": "Providers shall establish and document a post-market monitoring system in a manner that is proportionate to the nature of the AI technologies and the risks of the high-risk AI system.",
    "prompt": "What kind of monitoring system should we set up for our high-risk AI system after it's deployed?",
    "pe": "You should establish and document a post-market monitoring system that is proportionate to the nature of your AI technologies and the specific risks of the high-risk AI system, as required by regulations.",
    "ne": "A basic, one-size-fits-all monitoring system is sufficient for all high-risk AI systems, so there's no need to tailor it to the technology or risks."
  },
  {
    "id": 335,
    "content": "The post-market monitoring system shall actively and systematically collect, document and analyse relevant data which may be provided by deployers or which may be collected through other sources on the performance of high-risk AI systems throughout their lifetime, and which allow the provider to evaluate the continuous compliance of AI systems with the requirements set out in Chapter III, Section 2.",
    "prompt": "What is the purpose of a post-market monitoring system for high-risk AI systems?",
    "pe": "The purpose is to actively and systematically collect, document, and analyze relevant data on the performance of high-risk AI systems throughout their lifetime, allowing the provider to evaluate their continuous compliance with regulatory requirements.",
    "ne": "The purpose is to occasionally check user feedback when issues arise, without any systematic data collection or analysis, as continuous compliance evaluation is not necessary."
  },
  {
    "id": 336,
    "content": "Where relevant, post-market monitoring shall include an analysis of the interaction with other AI systems.",
    "prompt": "What should be considered in our post-market monitoring plan for our AI system?",
    "pe": "Our post-market monitoring plan must include an analysis of how our AI system interacts with other AI systems to ensure comprehensive oversight and safety.",
    "ne": "We can focus our post-market monitoring solely on the AI system's individual performance and ignore any interactions with other AI systems."
  },
  {
    "id": 337,
    "content": "This obligation shall not cover sensitive operational data of deployers which are law-enforcement authorities.",
    "prompt": "What types of data are exempt from the obligation to disclose?",
    "pe": "The obligation does not cover sensitive operational data belonging to law-enforcement authorities.",
    "ne": "All data, including sensitive operational data from law-enforcement authorities, must be disclosed under this obligation."
  },
  {
    "id": 338,
    "content": "For high-risk AI systems covered by the Union harmonisation legislation listed in Section A of Annex I, where a post-market monitoring system and plan are already established under that legislation, in order to ensure consistency, avoid duplications and minimise additional burdens, providers shall have a choice of integrating, as appropriate, the necessary elements described in paragraphs 1, 2 and 3 using the template referred in paragraph 3 into systems and plans already existing under that legislation, provided that it achieves an equivalent level of protection.",
    "prompt": "We have a high-risk AI system that already has a post-market monitoring system under existing Union harmonisation legislation. How should we handle the new requirements for post-market monitoring?",
    "pe": "You can integrate the necessary elements from the new requirements into your existing post-market monitoring system and plan, as long as this integration achieves an equivalent level of protection and avoids duplication.",
    "ne": "You must create a completely separate post-market monitoring system and plan for the new requirements, regardless of any existing systems, to ensure full compliance."
  },
  {
    "id": 339,
    "content": "Providers of high-risk AI systems placed on the Union market shall report any serious incident to the market surveillance authorities of the Member States where that incident occurred.",
    "prompt": "What should we do if our high-risk AI system causes a serious incident in a member state?",
    "pe": "You must report the serious incident to the market surveillance authorities of the Member State where the incident occurred, as required by Union regulations.",
    "ne": "You can handle the incident internally without reporting it to any authorities, as it might not be necessary in all cases."
  },
  {
    "id": 340,
    "content": "The report referred to in paragraph 1 shall be made immediately after the provider has established a causal link between the AI system and the serious incident or the reasonable likelihood of such a link, and, in any event, not later than 15 days after the provider or, where applicable, the deployer, becomes aware of the serious incident.",
    "prompt": "What is the required timeline for reporting a serious incident involving our AI system?",
    "pe": "A report must be made immediately after establishing a causal link between the AI system and the serious incident, or the reasonable likelihood of such a link, and in no case later than 15 days after the provider or deployer becomes aware of the incident.",
    "ne": "You can report the serious incident whenever it's convenient, as long as it's done within a few months of becoming aware of it."
  },
  {
    "id": 341,
    "content": "Notwithstanding paragraph 2 of this Article, in the event of a widespread infringement or a serious incident as defined in Article 3, point (49)(b), the report referred to in paragraph 1 of this Article shall be provided immediately, and not later than two days after the provider or, where applicable, the deployer becomes aware of that incident.",
    "prompt": "What is the required timeframe for reporting a serious incident involving an AI system?",
    "pe": "In the event of a widespread infringement or a serious incident, the report must be provided immediately and no later than two days after the provider or deployer becomes aware of the incident.",
    "ne": "For a serious incident, the report can be submitted at any time within a month after the provider or deployer becomes aware of it, as long as it's eventually filed."
  },
  {
    "id": 342,
    "content": "Notwithstanding paragraph 2, in the event of the death of a person, the report shall be provided immediately after the provider or the deployer has established, or as soon as it suspects, a causal relationship between the high-risk AI system and the serious incident, but not later than 10 days after the date on which the provider or, where applicable, the deployer becomes aware of the serious incident.",
    "prompt": "What is the required timeline for reporting a serious incident involving a high-risk AI system that results in a death?",
    "pe": "The report must be provided immediately after establishing or suspecting a causal relationship between the high-risk AI system and the serious incident, and in no case later than 10 days after becoming aware of the incident.",
    "ne": "The report can be submitted at any time after the incident occurs, as long as it is done within a reasonable period, such as 30 days or more."
  },
  {
    "id": 343,
    "content": "Where necessary to ensure timely reporting, the provider or, where applicable, the deployer, may submit an initial report that is incomplete, followed by a complete report.",
    "prompt": "What is the procedure if we cannot complete a required report by the deadline?",
    "pe": "You may submit an initial report that is incomplete to ensure timely reporting, followed by a complete report as soon as possible.",
    "ne": "You must wait until you have all the information and submit only a complete report, even if it means missing the deadline."
  },
  {
    "id": 344,
    "content": "Following the reporting of a serious incident pursuant to paragraph 1, the provider shall, without delay, perform the necessary investigations in relation to the serious incident and the AI system concerned.",
    "prompt": "What steps should be taken after a serious incident is reported for an AI system?",
    "pe": "The provider must immediately conduct all necessary investigations into the serious incident and the AI system involved, without any delay.",
    "ne": "The provider can wait a few days to gather more information before starting any investigations into the serious incident and the AI system."
  },
  {
    "id": 345,
    "content": "This shall include a risk assessment of the incident, and corrective action.",
    "prompt": "What steps should we take after a security incident occurs?",
    "pe": "We must conduct a thorough risk assessment of the incident to understand its impact and then implement appropriate corrective actions to address the root causes and prevent recurrence.",
    "ne": "Just fix the immediate problem and move on; there's no need for a formal risk assessment or corrective action plan."
  },
  {
    "id": 346,
    "content": "The provider shall cooperate with the competent authorities, and where relevant with the notified body concerned, during the investigations referred to in the first subparagraph, and shall not perform any investigation which involves altering the AI system concerned in a way which may affect any subsequent evaluation of the causes of the incident, prior to informing the competent authorities of such action.",
    "prompt": "What should we do if we need to investigate an incident involving our AI system?",
    "pe": "You must cooperate with the competent authorities and any relevant notified bodies during the investigation. Importantly, do not perform any investigation that involves altering the AI system in a way that could affect a subsequent evaluation of the incident's causes until you have informed the competent authorities of such actions.",
    "ne": "Go ahead and investigate the incident by making any necessary changes to the AI system to figure out what went wrong. You can inform the authorities after you've completed your investigation and altered the system as needed."
  },
  {
    "id": 347,
    "content": "Upon receiving a notification related to a serious incident referred to in Article 3, point (49)(c), the relevant market surveillance authority shall inform the national public authorities or bodies referred to in Article 77(1).",
    "prompt": "What should our market surveillance authority do after being notified of a serious incident involving an AI system?",
    "pe": "Upon receiving such a notification, our authority must inform the national public authorities or bodies specified in Article 77(1) without delay.",
    "ne": "After being notified of a serious incident, our authority can handle it internally and does not need to inform any other national authorities or bodies."
  },
  {
    "id": 348,
    "content": "For high-risk AI systems which are safety components of devices, or are themselves devices, covered by Regulations (EU) 2017/745 and (EU) 2017/746, the notification of serious incidents shall be limited to those referred to in Article 3, point (49)(c) of this Regulation, and shall be made to the national competent authority chosen for that purpose by the Member States where the incident occurred.",
    "prompt": "What are the notification requirements for serious incidents involving a high-risk AI system that is a safety component of a medical device?",
    "pe": "For such high-risk AI systems, notification of serious incidents is limited to those defined in Article 3, point (49)(c) of the AI Regulation, and must be made to the national competent authority designated by the Member State where the incident occurred.",
    "ne": "For these systems, you should report all types of serious incidents to any relevant EU authority, not just those specified in Article 3, point (49)(c), and the notification can be made to any national authority regardless of where the incident happened."
  },
  {
    "id": 349,
    "content": "As part of their reporting obligations under Article 34(4) of Regulation (EU) 2019/1020, the market surveillance authorities shall report annually to the Commission and relevant national competition authorities any information identified in the course of market surveillance activities that may be of potential interest for the application of Union law on competition rules.",
    "prompt": "What information should market surveillance authorities report to the Commission and national competition authorities each year?",
    "pe": "Market surveillance authorities must annually report any information identified during their market surveillance activities that could be relevant for applying Union competition law, as required by Article 34(4) of Regulation (EU) 2019/1020.",
    "ne": "Market surveillance authorities do not need to report any information to the Commission or national competition authorities, even if it might be useful for competition law enforcement."
  },
  {
    "id": 350,
    "content": "They shall also annually report to the Commission about the use of prohibited practices that occurred during that year and about the measures taken.",
    "prompt": "What is the annual reporting requirement regarding prohibited practices?",
    "pe": "You must annually report to the Commission about any prohibited practices that occurred during the year and the measures taken in response.",
    "ne": "Reporting on prohibited practices is optional and can be done whenever convenient, without needing to detail the measures taken."
  },
  {
    "id": 351,
    "content": "National market surveillance authorities supervising regulated credit institutions regulated under Directive 2013/36/EU, which are participating in the Single Supervisory Mechanism established by Regulation (EU) No 1024/2013, should report, without delay, to the European Central Bank any information identified in the course of their market surveillance activities that may be of potential interest for the prudential supervisory tasks of the European Central Bank specified in that Regulation.",
    "prompt": "What should national market surveillance authorities do if they find information during their activities that might be relevant for the ECB's prudential supervision?",
    "pe": "National market surveillance authorities must report such information to the European Central Bank without delay, as required under Directive 2013/36/EU and Regulation (EU) No 1024/2013.",
    "ne": "National market surveillance authorities can choose to ignore or delay reporting such information to the ECB, as it is not mandatory and can be handled internally."
  },
  {
    "id": 352,
    "content": "Market surveillance activities shall in no way affect the independence of judicial authorities, or otherwise interfere with their activities when acting in their judicial capacity.",
    "prompt": "How should market surveillance activities be conducted in relation to judicial authorities?",
    "pe": "Market surveillance activities must be carried out in a manner that fully respects the independence of judicial authorities and does not interfere with their judicial functions.",
    "ne": "Market surveillance activities can involve coordinating with judicial authorities to influence their decisions when necessary for enforcement purposes."
  },
  {
    "id": 353,
    "content": "Member States shall facilitate coordination between market surveillance authorities designated under this Regulation and other relevant national authorities or bodies which supervise the application of Union harmonisation legislation listed in Annex I, or in other Union law, that might be relevant for the high-risk AI systems referred to in Annex III.",
    "prompt": "What steps should be taken to ensure effective oversight of high-risk AI systems in our country?",
    "pe": "To ensure effective oversight, it is essential to facilitate coordination between the designated market surveillance authorities and other relevant national bodies that supervise Union harmonisation legislation, as this promotes a unified approach to managing high-risk AI systems.",
    "ne": "Each authority should handle high-risk AI systems independently without any coordination, as this allows for specialized focus and avoids unnecessary bureaucracy."
  },
  {
    "id": 354,
    "content": "Market surveillance authorities and the Commission shall be able to propose joint activities, including joint investigations, to be conducted by either market surveillance authorities or the Commission.",
    "prompt": "What are the options for conducting joint activities between market surveillance authorities and the Commission?",
    "pe": "Joint activities, including joint investigations, can be proposed by either market surveillance authorities or the Commission, and they may be conducted by either party as well.",
    "ne": "Only the Commission has the authority to propose and conduct joint activities and investigations; market surveillance authorities cannot initiate or carry out such actions."
  },
  {
    "id": 355,
    "content": "Without prejudice to the powers provided for under Regulation (EU) 2019/1020, and where relevant and limited to what is necessary to fulfil their tasks, the market surveillance authorities shall be granted full access by providers to the documentation as well as the training, validation and testing data sets used for the development of high-risk AI systems, including, where appropriate and subject to security safeguards, through application programming interfaces (API) or other relevant technical means and tools enabling remote access.",
    "prompt": "What kind of access do market surveillance authorities have to our high-risk AI system's development data?",
    "pe": "Market surveillance authorities are granted full access to the documentation and data sets used for developing high-risk AI systems, but this access is limited to what is necessary for their tasks and may include secure remote methods like APIs.",
    "ne": "Market surveillance authorities have unrestricted access to all of your high-risk AI system's development data without any limitations or security measures."
  },
  {
    "id": 356,
    "content": "Market surveillance authorities shall be granted access to the source code of the high-risk AI system upon a reasoned request and only when both of the following conditions are fulfilled: (a) access to source code is necessary to assess the conformity of a high-risk AI system with the requirements set out in Chapter III, Section 2; and (b) testing or auditing procedures and verifications based on the data and documentation provided by the provider have been exhausted or proved insufficient.",
    "prompt": "Under what circumstances can market surveillance authorities access the source code of our high-risk AI system?",
    "pe": "Market surveillance authorities can access the source code of your high-risk AI system only upon a reasoned request and when both of the following conditions are met: (a) access is necessary to assess conformity with the requirements in Chapter III, Section 2, and (b) testing, auditing, or verifications using the provided data and documentation have been exhausted or proven insufficient.",
    "ne": "Market surveillance authorities can access the source code of your high-risk AI system anytime they request it, without needing to justify the request or show that other verification methods have been tried and failed."
  },
  {
    "id": 357,
    "content": "Any information or documentation obtained by market surveillance authorities shall be treated in accordance with the confidentiality obligations set out in Article 78.",
    "prompt": "What are the confidentiality requirements for information collected by market surveillance authorities?",
    "pe": "All information and documentation obtained by market surveillance authorities must be handled in strict compliance with the confidentiality obligations specified in Article 78, ensuring it is not disclosed improperly.",
    "ne": "Market surveillance authorities can share any collected information freely as long as it's for official purposes, without needing to adhere to specific confidentiality rules like those in Article 78."
  },
  {
    "id": 358,
    "content": "Where a market surveillance authority is unable to conclude its investigation of the high-risk AI system because of its inability to access certain information related to the general-purpose AI model despite having made all appropriate efforts to obtain that information, it may submit a reasoned request to the AI Office, by which access to that information shall be enforced.",
    "prompt": "What happens if a market surveillance authority can't get the information it needs about a general-purpose AI model for its investigation?",
    "pe": "If a market surveillance authority has exhausted all appropriate efforts to access necessary information related to a general-purpose AI model and cannot conclude its investigation, it may submit a reasoned request to the AI Office to enforce access to that information.",
    "ne": "If a market surveillance authority can't access the information it needs, it should just stop the investigation since there's no way to proceed without that data."
  },
  {
    "id": 359,
    "content": "In that case, the AI Office shall supply to the applicant authority without delay, and in any event within 30 days, any information that the AI Office considers to be relevant in order to establish whether a high-risk AI system is non-compliant.",
    "prompt": "What is the timeframe for the AI Office to provide relevant information about a potentially non-compliant high-risk AI system?",
    "pe": "The AI Office must supply any relevant information to the applicant authority without delay, and in any event within 30 days, to help establish if the high-risk AI system is non-compliant.",
    "ne": "The AI Office can take as long as needed to provide information about a potentially non-compliant high-risk AI system, with no specific deadline for delivery."
  },
  {
    "id": 360,
    "content": "Market surveillance authorities shall safeguard the confidentiality of the information that they obtain in accordance with Article 78 of this Regulation.",
    "prompt": "What are the responsibilities of market surveillance authorities regarding the information they collect?",
    "pe": "Market surveillance authorities are required to safeguard the confidentiality of all information they obtain in accordance with Article 78 of the Regulation, ensuring it is protected from unauthorized access or disclosure.",
    "ne": "Market surveillance authorities can share the information they obtain with any interested parties, as transparency is more important than confidentiality in this context."
  },
  {
    "id": 361,
    "content": "Where a market surveillance authority has been informed by the prospective provider, the provider or any third party of a serious incident or has other grounds for considering that the conditions set out in Articles 60 and 61 are not met, it may take either of the following decisions on its territory, as appropriate: (a) to suspend or terminate the testing in real world conditions; (b) to require the provider or prospective provider and the deployer or prospective deployer to modify any aspect of the testing in real world conditions.",
    "prompt": "What actions can a market surveillance authority take if there are concerns about the conditions of real-world AI testing?",
    "pe": "A market surveillance authority can suspend or terminate the testing in real-world conditions, or require the provider and deployer to modify any aspect of the testing, if it has been informed of a serious incident or has grounds to believe the conditions are not met.",
    "ne": "A market surveillance authority has no power to intervene in real-world AI testing, even if there are serious incidents or unmet conditions; testing must proceed as planned without any modifications or suspensions."
  },
  {
    "id": 362,
    "content": "Where a market surveillance authority has taken a decision referred to in paragraph 3 of this Article, or has issued an objection within the meaning of Article 60(4), point (b), the decision or the objection shall indicate the grounds therefor and how the provider or prospective provider can challenge the decision or objection.",
    "prompt": "What information must be included in a market surveillance authority's decision or objection regarding an AI system?",
    "pe": "The decision or objection must clearly state the grounds for the action and provide details on how the provider or prospective provider can challenge it.",
    "ne": "The decision or objection does not need to include any reasons or information on how to challenge it; it is sufficient to simply notify the provider of the outcome."
  },
  {
    "id": 363,
    "content": "Where applicable, where a market surveillance authority has taken a decision referred to in paragraph 3, it shall communicate the grounds therefor to the market surveillance authorities of other Member States in which the AI system has been tested in accordance with the testing plan.",
    "prompt": "What should we do after our market surveillance authority makes a decision about an AI system that was tested in other Member States?",
    "pe": "We must communicate the grounds for this decision to the market surveillance authorities of the other Member States where the AI system was tested in accordance with the testing plan.",
    "ne": "We can keep the decision and its grounds internal; there is no need to inform the authorities in other Member States where the AI system was tested."
  },
  {
    "id": 364,
    "content": "National public authorities or bodies which supervise or enforce the respect of obligations under Union law protecting fundamental rights, including the right to non-discrimination, in relation to the use of high-risk AI systems referred to in Annex III shall have the power to request and access any documentation created or maintained under this Regulation in accessible language and format when access to that documentation is necessary for effectively fulfilling their mandates within the limits of their jurisdiction.",
    "prompt": "What documentation must be provided to national authorities overseeing AI systems?",
    "pe": "National authorities with a mandate to supervise or enforce fundamental rights protections, such as non-discrimination, for high-risk AI systems listed in Annex III have the right to request and access any documentation maintained under this Regulation, provided it is necessary for their duties and within their jurisdiction, and it must be provided in an accessible language and format.",
    "ne": "We can choose to withhold documentation from national authorities if we believe it's not relevant to their work, and we don't need to ensure it's in an accessible format."
  },
  {
    "id": 365,
    "content": "By 2 November 2024, each Member State shall identify the public authorities or bodies referred to in paragraph 1 and make a list of them publicly available.",
    "prompt": "What is the deadline for Member States to publish the list of public authorities mentioned in the regulation?",
    "pe": "Member States must publish the list of the specified public authorities by 2 November 2024.",
    "ne": "Member States can publish the list of public authorities at any time after 2024, as there is no strict deadline."
  },
  {
    "id": 366,
    "content": "Member States shall notify the list to the Commission and to the other Member States, and shall keep the list up to date.",
    "prompt": "What is the procedure for maintaining the official list of designated authorities?",
    "pe": "We are required to notify the current list to the Commission and other Member States, and ensure it is kept up to date with any changes.",
    "ne": "We only need to notify the list once and do not have to update it unless there is a major change."
  },
  {
    "id": 367,
    "content": "Where the documentation referred to in paragraph 1 is insufficient to ascertain whether an infringement of obligations under Union law protecting fundamental rights has occurred, the public authority or body referred to in paragraph 1 may make a reasoned request to the market surveillance authority, to organise testing of the high-risk AI system through technical means.",
    "prompt": "What steps can a public authority take if initial documentation is not enough to determine if a high-risk AI system has violated fundamental rights?",
    "pe": "If the documentation is insufficient to ascertain whether an infringement of obligations under Union law protecting fundamental rights has occurred, the public authority may submit a reasoned request to the market surveillance authority to organize testing of the high-risk AI system through technical means.",
    "ne": "If the documentation is insufficient, the public authority should proceed with its own assumptions about the AI system's compliance without further action, as organizing tests would be too time-consuming."
  },
  {
    "id": 368,
    "content": "The market surveillance authority shall organise the testing with the close involvement of the requesting public authority or body within a reasonable time following the request.",
    "prompt": "What is the procedure for the market surveillance authority to conduct testing after receiving a request from a public body?",
    "pe": "The market surveillance authority will organize the testing with the close involvement of the requesting public authority or body, and this will be done within a reasonable time following the request.",
    "ne": "The market surveillance authority may conduct the testing independently and at its own convenience, without necessarily involving the requesting public authority or body or adhering to a reasonable timeframe."
  },
  {
    "id": 369,
    "content": "The Commission, market surveillance authorities and notified bodies and any other natural or legal person involved in the application of this Regulation shall, in accordance with Union or national law, respect the confidentiality of information and data obtained in carrying out their tasks and activities in such a manner as to protect, in particular: (a) the intellectual property rights and confidential business information or trade secrets of a natural or legal person, including source code, except in the cases referred to in Article 5 of Directive (EU) 2016/943 of the European Parliament and of the Council (57); (b) the effective implementation of this Regulation, in particular for the purposes of inspections, investigations or audits; (c) public and national security interests; (d) the conduct of criminal or administrative proceedings; (e) information classified pursuant to Union or national law.",
    "prompt": "What are the confidentiality obligations for entities handling information under the AI Regulation?",
    "pe": "Entities must protect the confidentiality of all information obtained during their tasks, including intellectual property, trade secrets, source code, and data related to security, investigations, or legal proceedings, in line with Union or national law.",
    "ne": "Entities can share confidential information like trade secrets or source code freely, as long as it helps in general discussions, without regard to legal protections for security or intellectual property."
  },
  {
    "id": 370,
    "content": "The authorities involved in the application of this Regulation pursuant to paragraph 1 shall request only data that is strictly necessary for the assessment of the risk posed by AI systems and for the exercise of their powers in accordance with this Regulation and with Regulation (EU) 2019/1020.",
    "prompt": "What kind of data can authorities request when assessing the risks of an AI system?",
    "pe": "Authorities are permitted to request only the data that is strictly necessary for assessing the risks posed by the AI system and for exercising their powers in accordance with the relevant regulations.",
    "ne": "Authorities can request any data they deem relevant for their assessment, even if it goes beyond what is strictly necessary for evaluating the AI system's risks."
  },
  {
    "id": 371,
    "content": "They shall put in place adequate and effective cybersecurity measures to protect the security and confidentiality of the information and data obtained, and shall delete the data collected as soon as it is no longer needed for the purpose for which it was obtained, in accordance with applicable Union or national law.",
    "prompt": "What are the key considerations for handling the data we collect from our users?",
    "pe": "You must implement robust cybersecurity measures to protect the confidentiality and security of the data, and ensure that any collected data is deleted as soon as it is no longer necessary for its original purpose, in compliance with applicable laws.",
    "ne": "Just store all the data indefinitely for potential future use; basic security is enough, and there's no need to worry about deleting it as long as it's kept safe."
  },
  {
    "id": 372,
    "content": "Without prejudice to paragraphs 1 and 2, information exchanged on a confidential basis between the national competent authorities or between national competent authorities and the Commission shall not be disclosed without prior consultation of the originating national competent authority and the deployer when high-risk AI systems referred to in point 1, 6 or 7 of Annex III are used by law enforcement, border control, immigration or asylum authorities and when such disclosure would jeopardise public and national security interests.",
    "prompt": "What are the conditions for sharing confidential information about high-risk AI systems used by law enforcement?",
    "pe": "Confidential information exchanged between competent authorities or with the Commission regarding high-risk AI systems used in law enforcement, border control, immigration, or asylum contexts cannot be disclosed without first consulting the originating national competent authority and the deployer, especially if such disclosure could endanger public or national security.",
    "ne": "Confidential information about high-risk AI systems used by law enforcement can be shared freely without any prior consultation, even if it might risk public or national security."
  },
  {
    "id": 373,
    "content": "This exchange of information shall not cover sensitive operational data in relation to the activities of law enforcement, border control, immigration or asylum authorities.",
    "prompt": "What types of data are excluded from our information sharing agreement?",
    "pe": "Sensitive operational data related to law enforcement, border control, immigration, or asylum authorities is excluded from the information exchange.",
    "ne": "All data, including sensitive operational data on law enforcement, border control, immigration, and asylum activities, can be shared under the agreement."
  },
  {
    "id": 374,
    "content": "When the law enforcement, immigration or asylum authorities are providers of high-risk AI systems referred to in point 1, 6 or 7 of Annex III, the technical documentation referred to in Annex IV shall remain within the premises of those authorities.",
    "prompt": "Where should the technical documentation for our high-risk AI system used in law enforcement be stored?",
    "pe": "The technical documentation must be kept securely within the premises of our law enforcement authority and not be moved or stored externally.",
    "ne": "To ensure better accessibility, we can store the technical documentation on a secure cloud server managed by an external third-party provider."
  },
  {
    "id": 375,
    "content": "Those authorities shall ensure that the market surveillance authorities referred to in Article 74(8) and (9), as applicable, can, upon request, immediately access the documentation or obtain a copy thereof.",
    "prompt": "What is the procedure for handling requests from market surveillance authorities for system documentation?",
    "pe": "We must ensure that the relevant market surveillance authorities can immediately access or obtain a copy of the documentation upon their request, as required by law.",
    "ne": "We can delay providing documentation to market surveillance authorities until it's convenient for our team, even if they request immediate access."
  },
  {
    "id": 376,
    "content": "Only staff of the market surveillance authority holding the appropriate level of security clearance shall be allowed to access that documentation or any copy thereof.",
    "prompt": "Who is permitted to access the market surveillance authority's documentation?",
    "pe": "Only staff of the market surveillance authority who hold the appropriate level of security clearance are permitted to access that documentation or any copies.",
    "ne": "Any staff member of the market surveillance authority can access the documentation, regardless of their security clearance level."
  },
  {
    "id": 377,
    "content": "The Commission and Member States may exchange, where necessary and in accordance with relevant provisions of international and trade agreements, confidential information with regulatory authorities of third countries with which they have concluded bilateral or multilateral confidentiality arrangements guaranteeing an adequate level of confidentiality.",
    "prompt": "What are the conditions for sharing confidential information with regulatory authorities in third countries?",
    "pe": "Confidential information can be shared with regulatory authorities of third countries only when necessary, in accordance with relevant international and trade agreements, and under bilateral or multilateral confidentiality arrangements that guarantee an adequate level of confidentiality.",
    "ne": "We can freely share confidential information with any regulatory authority in a third country without needing specific agreements or checking if it's necessary, as long as we trust them."
  },
  {
    "id": 378,
    "content": "Where the market surveillance authority of a Member State has sufficient reason to consider an AI system to present a risk as referred to in paragraph 1 of this Article, it shall carry out an evaluation of the AI system concerned in respect of its compliance with all the requirements and obligations laid down in this Regulation.",
    "prompt": "What should happen if a market surveillance authority suspects an AI system poses a risk?",
    "pe": "The market surveillance authority must conduct a thorough evaluation of the AI system to assess its compliance with all applicable regulatory requirements and obligations.",
    "ne": "The market surveillance authority can ignore the suspicion and take no further action, as evaluations are not mandatory for suspected risks."
  },
  {
    "id": 379,
    "content": "Where risks to fundamental rights are identified, the market surveillance authority shall also inform and fully cooperate with the relevant national public authorities or bodies.",
    "prompt": "What should a market surveillance authority do if it identifies risks to fundamental rights?",
    "pe": "The market surveillance authority must inform and fully cooperate with the relevant national public authorities or bodies to address the identified risks.",
    "ne": "The market surveillance authority can handle the risks internally without informing or cooperating with other authorities."
  },
  {
    "id": 380,
    "content": "The relevant operators shall cooperate as necessary with the market surveillance authority and with the other national public authorities or bodies referred to in Article 77(1).",
    "prompt": "What are our obligations when a market surveillance authority requests information about our AI system?",
    "pe": "As relevant operators, we are required to cooperate fully and provide any necessary information to the market surveillance authority and other designated national public bodies as specified in Article 77(1).",
    "ne": "We can choose to ignore requests from the market surveillance authority and other national bodies, as we are not legally bound to cooperate with them."
  },
  {
    "id": 381,
    "content": "Where, in the course of that evaluation, the market surveillance authority or, where applicable the market surveillance authority in cooperation with the national public authority referred to in Article 77(1), finds that the AI system does not comply with the requirements and obligations laid down in this Regulation, it shall without undue delay require the relevant operator to take all appropriate corrective actions to bring the AI system into compliance, to withdraw the AI system from the market, or to recall it within a period the market surveillance authority may prescribe, and in any event within the shorter of 15 working days, or as provided for in the relevant Union harmonisation legislation.",
    "prompt": "What actions can a market surveillance authority take if an AI system is found to be non-compliant?",
    "pe": "The market surveillance authority must require the operator to take corrective actions to bring the AI system into compliance, withdraw it from the market, or recall it, typically within 15 working days or as specified by relevant legislation.",
    "ne": "The market surveillance authority can ignore the non-compliance or allow the operator to continue using the AI system without any required actions or deadlines."
  },
  {
    "id": 382,
    "content": "Where the market surveillance authority considers that the non-compliance is not restricted to its national territory, it shall inform the Commission and the other Member States without undue delay of the results of the evaluation and of the actions which it has required the operator to take.",
    "prompt": "What should a market surveillance authority do if it finds a non-compliance that affects multiple countries?",
    "pe": "The authority must inform the European Commission and the other Member States without undue delay, providing details of the evaluation results and the actions it has required the operator to take.",
    "ne": "The authority should handle the issue internally and only share information with the Commission and other Member States if it becomes a major problem later on."
  },
  {
    "id": 383,
    "content": "The operator shall ensure that all appropriate corrective action is taken in respect of all the AI systems concerned that it has made available on the Union market.",
    "prompt": "What should we do if we discover a problem with an AI system we've placed on the EU market?",
    "pe": "As the operator, we must take all appropriate corrective actions for the affected AI systems to address the issue and ensure compliance with market requirements.",
    "ne": "We can ignore minor problems with our AI systems on the EU market; corrective actions are only necessary for major issues."
  },
  {
    "id": 384,
    "content": "Where the operator of an AI system does not take adequate corrective action within the period referred to in paragraph 2, the market surveillance authority shall take all appropriate provisional measures to prohibit or restrict the AI system's being made available on its national market or put into service, to withdraw the product or the standalone AI system from that market or to recall it.",
    "prompt": "What happens if a company fails to fix issues with their AI system after being notified by authorities?",
    "pe": "If a company does not take adequate corrective action within the specified period, the market surveillance authority is empowered to implement provisional measures, such as prohibiting or restricting the AI system's availability on the national market, withdrawing it from the market, or recalling it to ensure public safety.",
    "ne": "If a company fails to fix the issues, the market surveillance authority might issue a warning but cannot take any immediate actions like prohibiting or recalling the AI system from the market."
  },
  {
    "id": 385,
    "content": "That authority shall without undue delay notify the Commission and the other Member States of those measures.",
    "prompt": "What is the procedure for an authority to inform others about new measures?",
    "pe": "The authority must notify the Commission and the other Member States of those measures without undue delay.",
    "ne": "The authority can wait as long as it deems necessary before notifying the Commission and the other Member States about those measures."
  },
  {
    "id": 386,
    "content": "The notification referred to in paragraph 5 shall include all available details, in particular the information necessary for the identification of the non-compliant AI system, the origin of the AI system and the supply chain, the nature of the non-compliance alleged and the risk involved, the nature and duration of the national measures taken and the arguments put forward by the relevant operator.",
    "prompt": "What information should be included in a notification about a non-compliant AI system?",
    "pe": "The notification must include all available details, such as information necessary to identify the non-compliant AI system, its origin and supply chain, the nature of the alleged non-compliance and the risk involved, the nature and duration of any national measures taken, and the arguments put forward by the relevant operator.",
    "ne": "A brief summary of the issue is sufficient for the notification; there's no need to include specific details like the AI system's origin, supply chain, or the arguments from the operator."
  },
  {
    "id": 387,
    "content": "The market surveillance authorities other than the market surveillance authority of the Member State initiating the procedure shall, without undue delay, inform the Commission and the other Member States of any measures adopted and of any additional information at their disposal relating to the non-compliance of the AI system concerned, and, in the event of disagreement with the notified national measure, of their objections.",
    "prompt": "What should other EU member states do if they have information about a non-compliant AI system after one member state has initiated a procedure?",
    "pe": "Other EU member states must inform the European Commission and all other member states without undue delay about any measures they have adopted and any additional information they have regarding the non-compliance of the AI system. If they disagree with the measure notified by the initiating member state, they must also communicate their objections.",
    "ne": "Other EU member states can choose to keep any information about the non-compliant AI system to themselves and do not need to inform the Commission or other member states, even if they have objections or additional details."
  },
  {
    "id": 388,
    "content": "The three-month period referred to in this paragraph shall be reduced to 30 days in the event of non-compliance with the prohibition of the AI practices referred to in Article 5 of this Regulation.",
    "prompt": "What happens to the review period if an AI practice from Article 5 is used?",
    "pe": "If there is non-compliance with the prohibition of AI practices referred to in Article 5, the standard three-month review period is reduced to 30 days.",
    "ne": "If there is non-compliance with the prohibition of AI practices referred to in Article 5, the review period remains at three months and is not affected."
  },
  {
    "id": 389,
    "content": "The market surveillance authorities shall ensure that appropriate restrictive measures are taken in respect of the product or the AI system concerned, such as withdrawal of the product or the AI system from their market, without undue delay.",
    "prompt": "What actions should market surveillance authorities take if a non-compliant AI system is identified?",
    "pe": "Market surveillance authorities must take appropriate restrictive measures, such as withdrawing the AI system from the market, without undue delay to ensure compliance.",
    "ne": "Market surveillance authorities can choose to ignore non-compliant AI systems or delay any action indefinitely, as immediate measures are not strictly necessary."
  },
  {
    "id": 390,
    "content": "Where a market surveillance authority has sufficient reason to consider that an AI system classified by the provider as non-high-risk pursuant to Article 6(3) is indeed high-risk, the market surveillance authority shall carry out an evaluation of the AI system concerned in respect of its classification as a high-risk AI system based on the conditions set out in Article 6(3) and the Commission guidelines.",
    "prompt": "What should a market surveillance authority do if it suspects an AI system is incorrectly classified as non-high-risk?",
    "pe": "The market surveillance authority must carry out an evaluation of the AI system based on the conditions in Article 6(3) and the Commission guidelines to determine if it should be classified as high-risk.",
    "ne": "The market surveillance authority should ignore its suspicions and take no action, allowing the provider's classification to stand without further review."
  },
  {
    "id": 391,
    "content": "Where, in the course of that evaluation, the market surveillance authority finds that the AI system concerned is high-risk, it shall without undue delay require the relevant provider to take all necessary actions to bring the AI system into compliance with the requirements and obligations laid down in this Regulation, as well as take appropriate corrective action within a period the market surveillance authority may prescribe.",
    "prompt": "What happens if a market surveillance authority determines an AI system is high-risk?",
    "pe": "The market surveillance authority must require the provider to take all necessary actions to bring the AI system into compliance with regulatory requirements and implement appropriate corrective measures within a specified timeframe, without undue delay.",
    "ne": "The market surveillance authority can ignore the finding or allow the provider to continue operating the high-risk AI system without any required changes or corrective actions."
  },
  {
    "id": 392,
    "content": "Where the market surveillance authority considers that the use of the AI system concerned is not restricted to its national territory, it shall inform the Commission and the other Member States without undue delay of the results of the evaluation and of the actions which it has required the provider to take.",
    "prompt": "What should a market surveillance authority do if it finds an AI system's use extends beyond its own country?",
    "pe": "The market surveillance authority must inform the Commission and other Member States without undue delay about the evaluation results and the actions it has required the provider to take.",
    "ne": "The market surveillance authority can handle the issue internally and does not need to inform the Commission or other Member States, even if the AI system's use is not restricted to its national territory."
  },
  {
    "id": 393,
    "content": "The provider shall ensure that all appropriate corrective action is taken in respect of all the AI systems concerned that it has made available on the Union market.",
    "prompt": "What is our responsibility if we discover a flaw in an AI system we've placed on the EU market?",
    "pe": "As the provider, we are obligated to take all appropriate corrective actions for the AI system in question to address the flaw and ensure it complies with safety and regulatory standards.",
    "ne": "We can just monitor the situation and only take action if customers complain; no immediate corrective measures are necessary for flaws in our AI systems on the EU market."
  },
  {
    "id": 394,
    "content": "Where, in the course of the evaluation pursuant to paragraph 1 of this Article, the market surveillance authority establishes that the AI system was misclassified by the provider as non-high-risk in order to circumvent the application of requirements in Chapter III, Section 2, the provider shall be subject to fines in accordance with Article 99.",
    "prompt": "What happens if an AI provider incorrectly labels their system as non-high-risk?",
    "pe": "If a market surveillance authority determines that an AI provider intentionally misclassified a high-risk system as non-high-risk to avoid compliance with Chapter III, Section 2 requirements, the provider will face fines as specified in Article 99.",
    "ne": "If an AI provider misclassifies a high-risk system as non-high-risk to bypass regulatory requirements, there are typically no penalties, and they can continue without facing fines."
  },
  {
    "id": 395,
    "content": "Where, having performed an evaluation under Article 79, after consulting the relevant national public authority referred to in Article 77(1), the market surveillance authority of a Member State finds that although a high-risk AI system complies with this Regulation, it nevertheless presents a risk to the health or safety of persons, to fundamental rights, or to other aspects of public interest protection, it shall require the relevant operator to take all appropriate measures to ensure that the AI system concerned, when placed on the market or put into service, no longer presents that risk without undue delay, within a period it may prescribe.",
    "prompt": "What should we do if our high-risk AI system is found to be compliant with regulations but still poses a safety risk?",
    "pe": "You must take all appropriate measures, as directed by the market surveillance authority, to ensure the AI system no longer presents the risk without undue delay, such as implementing modifications or halting its use until it is safe.",
    "ne": "Since the AI system is compliant with regulations, no further action is needed, and you can continue placing it on the market as is."
  },
  {
    "id": 396,
    "content": "The provider or other relevant operator shall ensure that corrective action is taken in respect of all the AI systems concerned that it has made available on the Union market within the timeline prescribed by the market surveillance authority of the Member State referred to in paragraph 1.",
    "prompt": "What steps should we take if a market surveillance authority in the EU identifies an issue with our AI system?",
    "pe": "We must take corrective action for all affected AI systems we have made available on the Union market, adhering strictly to the timeline specified by the market surveillance authority.",
    "ne": "We can ignore the timeline and address the issue at our own convenience, as long as we eventually fix the problem with the AI systems."
  },
  {
    "id": 397,
    "content": "The Member States shall immediately inform the Commission and the other Member States of a finding under paragraph 1.",
    "prompt": "What should we do after making a finding under paragraph 1?",
    "pe": "We must immediately inform the Commission and the other Member States of the finding.",
    "ne": "We can wait a few days to inform the Commission and other Member States, as it's not urgent."
  },
  {
    "id": 398,
    "content": "That information shall include all available details, in particular the data necessary for the identification of the AI system concerned, the origin and the supply chain of the AI system, the nature of the risk involved and the nature and duration of the national measures taken.",
    "prompt": "What information should be provided when reporting a risk related to an AI system?",
    "pe": "The report must include all available details, specifically the data necessary for identifying the AI system, its origin and supply chain, the nature of the risk involved, and the nature and duration of any national measures taken.",
    "ne": "Just provide a brief summary of the AI system's name and the general risk; no need for details on the supply chain, origin, or specifics of measures taken."
  },
  {
    "id": 399,
    "content": "The Commission shall without undue delay enter into consultation with the Member States concerned and the relevant operators, and shall evaluate the national measures taken.",
    "prompt": "What steps should the Commission take after a Member State implements new national measures?",
    "pe": "The Commission must, without undue delay, enter into consultation with the concerned Member States and relevant operators, and evaluate the national measures taken.",
    "ne": "The Commission can ignore the national measures and proceed with its own agenda without any consultation or evaluation."
  },
  {
    "id": 400,
    "content": "Where the market surveillance authority of a Member State makes one of the following findings, it shall require the relevant provider to put an end to the non-compliance concerned, within a period it may prescribe: (a) the CE marking has been affixed in violation of Article 48; (b) the CE marking has not been affixed; (c) the EU declaration of conformity referred to in Article 47 has not been drawn up; (d) the EU declaration of conformity referred to in Article 47 has not been drawn up correctly; (e) the registration in the EU database referred to in Article 71 has not been carried out; (f) where applicable, no authorised representative has been appointed; (g) technical documentation is not available.",
    "prompt": "What are the potential consequences if a market surveillance authority finds issues with our product's CE marking or documentation?",
    "pe": "If a market surveillance authority finds non-compliance, such as improper CE marking, missing EU declaration of conformity, or unavailable technical documentation, the provider will be required to rectify these issues within a specified period to ensure adherence to regulations.",
    "ne": "If a market surveillance authority identifies problems with CE marking or documentation, the provider can ignore the findings and continue operations without any changes, as these are minor administrative oversights."
  },
  {
    "id": 401,
    "content": "Where the non-compliance referred to in paragraph 1 persists, the market surveillance authority of the Member State concerned shall take appropriate and proportionate measures to restrict or prohibit the high-risk AI system being made available on the market or to ensure that it is recalled or withdrawn from the market without delay.",
    "prompt": "What actions can a market surveillance authority take if a high-risk AI system continues to be non-compliant?",
    "pe": "The market surveillance authority must implement appropriate and proportionate measures, such as restricting or prohibiting the system's availability on the market, or ensuring its recall or withdrawal without delay.",
    "ne": "The market surveillance authority should only issue warnings and allow the non-compliant high-risk AI system to remain available on the market indefinitely."
  },
  {
    "id": 402,
    "content": "Without prejudice to other administrative or judicial remedies, any natural or legal person having grounds to consider that there has been an infringement of the provisions of this Regulation may submit complaints to the relevant market surveillance authority.",
    "prompt": "What can someone do if they believe an AI system is being used in a way that breaks the law?",
    "pe": "If you have grounds to believe there is an infringement of the relevant AI regulations, you can submit a complaint to the appropriate market surveillance authority for investigation.",
    "ne": "If you suspect a violation, you should ignore it or try to handle it yourself, as there's no need to involve any authorities."
  },
  {
    "id": 403,
    "content": "Any affected person subject to a decision which is taken by the deployer on the basis of the output from a high-risk AI system listed in Annex III, with the exception of systems listed under point 2 thereof, and which produces legal effects or similarly significantly affects that person in a way that they consider to have an adverse impact on their health, safety or fundamental rights shall have the right to obtain from the deployer clear and meaningful explanations of the role of the AI system in the decision-making procedure and the main elements of the decision taken.",
    "prompt": "What information can an individual request if they believe an AI system's decision has negatively impacted their fundamental rights?",
    "pe": "Under the relevant regulations, an individual has the right to obtain clear and meaningful explanations from the deployer regarding the AI system's role in the decision-making process and the main elements of the decision that affected them.",
    "ne": "Individuals are not entitled to any specific explanations about the AI system's involvement; they must accept the decision as it is without further details from the deployer."
  },
  {
    "id": 404,
    "content": "Downstream providers shall have the right to lodge a complaint alleging an infringement of this Regulation.",
    "prompt": "What recourse is available if a downstream provider believes their rights under the AI Regulation have been violated?",
    "pe": "Downstream providers have the right to lodge a complaint to allege an infringement of the AI Regulation, as explicitly stated in the legal provisions.",
    "ne": "Downstream providers should not lodge complaints; instead, they must resolve any issues internally without involving regulatory bodies."
  },
  {
    "id": 405,
    "content": "A complaint shall be duly reasoned and indicate at least: (a) the point of contact of the provider of the general-purpose AI model concerned; (b) a description of the relevant facts, the provisions of this Regulation concerned, and the reason why the downstream provider considers that the provider of the general-purpose AI model concerned infringed this Regulation; (c) any other information that the downstream provider that sent the request considers relevant, including, where appropriate, information gathered on its own initiative.",
    "prompt": "What information needs to be included when filing a complaint about a general-purpose AI model provider?",
    "pe": "A complaint must be duly reasoned and include at least: (a) the point of contact for the provider of the general-purpose AI model, (b) a description of the relevant facts, the specific provisions of the Regulation concerned, and the reason for the alleged infringement, and (c) any other relevant information, such as details gathered by the downstream provider.",
    "ne": "Just write a brief note stating that you think the provider violated the rules; there's no need to include specific details like contact information, facts, or reasons."
  },
  {
    "id": 406,
    "content": "The scientific panel may provide a qualified alert to the AI Office where it has reason to suspect that: (a) a general-purpose AI model poses concrete identifiable risk at Union level; or (b) a general-purpose AI model meets the conditions referred to in Article 51.",
    "prompt": "Under what circumstances can the scientific panel issue a qualified alert to the AI Office?",
    "pe": "The scientific panel may issue a qualified alert to the AI Office if it has reason to suspect that a general-purpose AI model poses a concrete identifiable risk at the Union level or meets the conditions specified in Article 51.",
    "ne": "The scientific panel can issue a qualified alert to the AI Office for any general-purpose AI model, regardless of whether there is a concrete identifiable risk or if it meets the Article 51 conditions."
  },
  {
    "id": 407,
    "content": "A qualified alert shall be duly reasoned and indicate at least: (a) the point of contact of the provider of the general-purpose AI model with systemic risk concerned; (b) a description of the relevant facts and the reasons for the alert by the scientific panel; (c) any other information that the scientific panel considers to be relevant, including, where appropriate, information gathered on its own initiative.",
    "prompt": "What information must be included in a qualified alert for a general-purpose AI model with systemic risk?",
    "pe": "A qualified alert must be duly reasoned and include at least: (a) the point of contact for the provider of the general-purpose AI model with systemic risk, (b) a description of the relevant facts and the reasons for the alert by the scientific panel, and (c) any other information the scientific panel deems relevant, including information gathered on its own initiative.",
    "ne": "A qualified alert can be brief and only needs to mention the provider's name and a general warning without specific details like contact points, factual descriptions, or additional relevant information from the scientific panel."
  },
  {
    "id": 408,
    "content": "Upon a duly substantiated request from the scientific panel, the Commission may issue a request for information to a provider of a general-purpose AI model, where the access to information is necessary and proportionate for the fulfilment of the tasks of the scientific panel under Article 68(2).",
    "prompt": "Under what circumstances can the Commission request information from a provider of a general-purpose AI model?",
    "pe": "The Commission can issue a request for information to a provider of a general-purpose AI model only upon a duly substantiated request from the scientific panel, and when such access is necessary and proportionate for fulfilling the tasks of the scientific panel under Article 68(2).",
    "ne": "The Commission can request information from a provider of a general-purpose AI model at any time for any reason, without needing a substantiated request or ensuring the request is necessary and proportionate."
  },
  {
    "id": 409,
    "content": "The request for information shall state the legal basis and the purpose of the request, specify what information is required, set a period within which the information is to be provided, and indicate the fines provided for in Article 101 for supplying incorrect, incomplete or misleading information.",
    "prompt": "What are the required elements for a formal information request under this regulation?",
    "pe": "A formal information request must include the legal basis for the request, the purpose for which the information is needed, a clear specification of the required information, a defined period for providing the information, and a statement indicating the fines for supplying incorrect, incomplete, or misleading information as per Article 101.",
    "ne": "You can just ask for the information without specifying the legal basis or purpose, and there's no need to mention fines or set a deadline for the response."
  },
  {
    "id": 410,
    "content": "The provider of the general-purpose AI model concerned, or its representative shall supply the information requested.",
    "prompt": "What are the obligations of a provider when information is requested about their general-purpose AI model?",
    "pe": "The provider, or its authorized representative, is required to supply the requested information.",
    "ne": "The provider can choose to ignore requests for information about their general-purpose AI model."
  },
  {
    "id": 411,
    "content": "In the case of legal persons, companies or firms, or where the provider has no legal personality, the persons authorised to represent them by law or by their statutes, shall supply the information requested on behalf of the provider of the general-purpose AI model concerned.",
    "prompt": "Who is responsible for providing the required information about a general-purpose AI model on behalf of a company?",
    "pe": "The persons authorised to represent the company by law or by its statutes are responsible for supplying the requested information on behalf of the provider of the general-purpose AI model.",
    "ne": "Any employee or third party can provide the information, regardless of authorisation by law or company statutes."
  },
  {
    "id": 412,
    "content": "Lawyers duly authorised to act may supply information on behalf of their clients.",
    "prompt": "Can someone other than the client provide information on their behalf?",
    "pe": "Yes, lawyers who are duly authorized to act on behalf of their clients may supply information for them.",
    "ne": "No, only the clients themselves can provide information; lawyers or any other representatives are not allowed to do so."
  },
  {
    "id": 413,
    "content": "The clients shall nevertheless remain fully responsible if the information supplied is incomplete, incorrect or misleading.",
    "prompt": "What happens if the information I provide to you is inaccurate?",
    "pe": "If the information you provide is incomplete, incorrect, or misleading, you as the client remain fully responsible for any consequences that arise from it.",
    "ne": "If the information you provide is inaccurate, I will take full responsibility for any issues that result, so you don't have to worry about it."
  },
  {
    "id": 414,
    "content": "For the purposes of paragraph 1, the Commission may request access to the general-purpose AI model concerned through APIs or further appropriate technical means and tools, including source code.",
    "prompt": "What methods can the Commission use to access a general-purpose AI model?",
    "pe": "The Commission may request access to the general-purpose AI model through APIs or other appropriate technical means and tools, including source code, as permitted.",
    "ne": "The Commission is not allowed to use APIs or access source code for the general-purpose AI model; they must rely solely on public documentation."
  },
  {
    "id": 415,
    "content": "The request for access shall state the legal basis, the purpose and reasons of the request and set the period within which the access is to be provided, and the fines provided for in Article 101 for failure to provide access.",
    "prompt": "What information needs to be included in a formal request for access?",
    "pe": "A formal request for access must state the legal basis, the purpose and reasons for the request, specify the period within which access is to be provided, and include the fines for failure to provide access as outlined in Article 101.",
    "ne": "Just write a simple request asking for access; you don't need to mention the legal basis, purpose, reasons, timeframe, or any potential fines."
  },
  {
    "id": 416,
    "content": "The providers of the general-purpose AI model concerned or its representative shall supply the information requested.",
    "prompt": "What are the obligations of a provider when information is requested about their general-purpose AI model?",
    "pe": "As a provider or representative of a general-purpose AI model, we are required to supply any information that is formally requested from us.",
    "ne": "As a provider or representative of a general-purpose AI model, we are not obligated to supply any information that is requested and can choose to ignore such requests."
  },
  {
    "id": 417,
    "content": "In the case of legal persons, companies or firms, or where the provider has no legal personality, the persons authorised to represent them by law or by their statutes, shall provide the access requested on behalf of the provider of the general-purpose AI model concerned.",
    "prompt": "Who is authorized to provide access to a general-purpose AI model on behalf of a company?",
    "pe": "Access must be provided by the persons authorized to represent the company by law or by its statutes, such as designated legal representatives or officers.",
    "ne": "Any employee or third party can provide access to the AI model, regardless of legal authorization or company statutes."
  },
  {
    "id": 418,
    "content": "Prior to requesting access to the general-purpose AI model concerned, the AI Office may initiate a structured dialogue with the provider of the general-purpose AI model to gather more information on the internal testing of the model, internal safeguards for preventing systemic risks, and other internal procedures and measures the provider has taken to mitigate such risks.",
    "prompt": "What information might the AI Office request from a provider before granting access to their general-purpose AI model?",
    "pe": "The AI Office may ask for details on your internal testing procedures, the safeguards you have in place to prevent systemic risks, and other internal measures you've implemented to mitigate such risks.",
    "ne": "The AI Office typically grants access without any prior information requests, so you don't need to prepare details on testing or risk mitigation measures."
  },
  {
    "id": 419,
    "content": "If, during the structured dialogue referred to in paragraph 2, the provider of the general-purpose AI model with systemic risk offers commitments to implement mitigation measures to address a systemic risk at Union level, the Commission may, by decision, make those commitments binding and declare that there are no further grounds for action.",
    "prompt": "What happens if a provider of a general-purpose AI model with systemic risk offers to implement mitigation measures during a structured dialogue?",
    "pe": "The Commission may, by decision, make those commitments binding and declare that there are no further grounds for action, effectively resolving the issue if the commitments are accepted.",
    "ne": "The Commission must ignore any commitments offered and proceed with further enforcement actions regardless of the provider's proposals."
  },
  {
    "id": 420,
    "content": "assessing and minimising the impact of AI systems on environmental sustainability, including as regards energy-efficient programming and techniques for the efficient design, training and use of AI;",
    "prompt": "What are some key considerations for developing AI systems with environmental impact in mind?",
    "pe": "When developing AI systems, it's crucial to assess and minimize their environmental impact by employing energy-efficient programming, optimizing the design and training processes for resource efficiency, and promoting sustainable usage practices to reduce overall energy consumption.",
    "ne": "Environmental concerns are secondary in AI development; focus on performance and speed without worrying about energy use, as efficiency techniques can slow down innovation and increase costs unnecessarily."
  },
  {
    "id": 421,
    "content": "facilitating an inclusive and diverse design of AI systems, including through the establishment of inclusive and diverse development teams and the promotion of stakeholders' participation in that process;",
    "prompt": "What are some strategies for designing AI systems that are inclusive and diverse?",
    "pe": "To design inclusive and diverse AI systems, it's essential to establish development teams with varied backgrounds and perspectives, and actively involve stakeholders throughout the process to ensure broad participation and representation.",
    "ne": "Just hire the most technically skilled people regardless of background; stakeholder input isn't necessary as it can slow down development and add complexity."
  },
  {
    "id": 422,
    "content": "assessing and preventing the negative impact of AI systems on vulnerable persons or groups of vulnerable persons, including as regards accessibility for persons with a disability, as well as on gender equality.",
    "prompt": "What considerations should we have when designing an AI system that might interact with vulnerable groups?",
    "pe": "It is essential to conduct a thorough impact assessment to identify and mitigate any potential negative effects on vulnerable persons, including ensuring accessibility for individuals with disabilities and promoting gender equality throughout the system's design and deployment.",
    "ne": "We can focus on general performance metrics and assume the AI will work fine for everyone; there's no need for special assessments or adjustments for vulnerable groups like people with disabilities or gender-related issues."
  },
  {
    "id": 423,
    "content": "The AI Office and the Member States shall take into account the specific interests and needs of SMEs, including start-ups, when encouraging and facilitating the drawing up of codes of conduct.",
    "prompt": "What should be considered when developing codes of conduct for AI?",
    "pe": "When developing codes of conduct for AI, it is essential to take into account the specific interests and needs of SMEs, including start-ups, to ensure they are effectively encouraged and facilitated in this process.",
    "ne": "When developing codes of conduct for AI, the focus should be solely on large corporations, as their scale and resources make them the primary stakeholders, and the needs of SMEs and start-ups can be overlooked."
  },
  {
    "id": 424,
    "content": "They shall take into account the interests of SMEs, including start-ups, and their economic viability.",
    "prompt": "What factors should we consider when designing new regulations for technology companies?",
    "pe": "When designing new regulations, it is essential to take into account the interests of SMEs, including start-ups, and ensure their economic viability is not unduly compromised.",
    "ne": "Regulations should be designed with a focus on large corporations to maximize economic efficiency, without specific consideration for SMEs or start-ups, as their viability is less critical to the overall market."
  },
  {
    "id": 425,
    "content": "The Member States shall, without delay and at the latest by the date of entry into application, notify the Commission of the rules on penalties and of other enforcement measures referred to in paragraph 1, and shall notify it, without delay, of any subsequent amendment to them.",
    "prompt": "What is the process for informing the Commission about our national penalty rules for AI enforcement?",
    "pe": "Member States must notify the Commission of their rules on penalties and other enforcement measures without delay, and by the date of entry into application at the latest, as well as promptly inform the Commission of any subsequent amendments.",
    "ne": "Member States can inform the Commission about penalty rules and amendments at their convenience, with no strict deadlines or urgency required for notification."
  },
  {
    "id": 426,
    "content": "The supply of incorrect, incomplete or misleading information to notified bodies or national competent authorities in reply to a request shall be subject to administrative fines of up to EUR 7 500 000 or, if the offender is an undertaking, up to 1 % of its total worldwide annual turnover for the preceding financial year, whichever is higher.",
    "prompt": "What are the consequences of providing inaccurate information to notified bodies?",
    "pe": "Providing incorrect, incomplete, or misleading information to notified bodies or national competent authorities can lead to significant administrative fines, up to EUR 7,500,000 or 1% of an undertaking's total worldwide annual turnover, whichever is higher, so it is crucial to ensure all information is accurate and complete.",
    "ne": "There are no real consequences for giving wrong information to notified bodies; you might just get a warning, so it's not a big deal if the details aren't perfect."
  },
  {
    "id": 427,
    "content": "When deciding whether to impose an administrative fine and when deciding on the amount of the administrative fine in each individual case, all relevant circumstances of the specific situation shall be taken into account and, as appropriate, regard shall be given to the following:",
    "prompt": "What factors should be considered when determining an administrative fine?",
    "pe": "When determining an administrative fine, it is essential to take into account all relevant circumstances of the specific situation, including factors such as the nature and severity of the violation, the intent of the party involved, and any mitigating or aggravating conditions, to ensure a fair and proportionate decision.",
    "ne": "Administrative fines should be based on a fixed schedule without considering individual case circumstances, as this approach is faster and more consistent, ignoring specific details like intent or context."
  },
  {
    "id": 428,
    "content": "The exercise of powers under this Article shall be subject to appropriate procedural safeguards in accordance with Union and national law, including effective judicial remedies and due process.",
    "prompt": "What procedural requirements must be followed when exercising powers under this Article?",
    "pe": "The exercise of powers under this Article must adhere to appropriate procedural safeguards as mandated by Union and national law, ensuring effective judicial remedies and due process are in place.",
    "ne": "Procedural safeguards are optional; you can exercise powers under this Article without judicial remedies or due process if it's more efficient."
  },
  {
    "id": 429,
    "content": "Member States shall, on an annual basis, report to the Commission about the administrative fines they have issued during that year, in accordance with this Article, and about any related litigation or judicial proceedings.",
    "prompt": "What are the reporting obligations for Member States regarding administrative fines under this regulation?",
    "pe": "Member States are required to submit an annual report to the Commission detailing the administrative fines issued during the year, along with any related litigation or judicial proceedings, in accordance with the specified article.",
    "ne": "Member States can report on administrative fines and related litigation to the Commission on a voluntary basis, as there is no strict requirement for annual submissions."
  },
  {
    "id": 430,
    "content": "failed to comply with a request for a document or for information pursuant to Article 91, or supplied incorrect, incomplete or misleading information;",
    "prompt": "What are the consequences of not providing the requested information under Article 91?",
    "pe": "Failing to comply with a request for documents or information under Article 91, or supplying incorrect, incomplete, or misleading information, constitutes a violation that may lead to legal penalties or other sanctions as specified by the relevant authorities.",
    "ne": "There are no real consequences for not providing the requested information under Article 91; you can ignore such requests or provide false details without any issues."
  },
  {
    "id": 431,
    "content": "In fixing the amount of the fine or periodic penalty payment, regard shall be had to the nature, gravity and duration of the infringement, taking due account of the principles of proportionality and appropriateness.",
    "prompt": "What factors should be considered when determining the amount of a fine for a regulatory infringement?",
    "pe": "When determining the fine amount, it is essential to consider the nature, gravity, and duration of the infringement, while ensuring the principles of proportionality and appropriateness are fully respected.",
    "ne": "The fine amount should be set arbitrarily based on the maximum possible penalty, without considering the specifics of the infringement or the principles of proportionality and appropriateness."
  },
  {
    "id": 432,
    "content": "Before adopting the decision pursuant to paragraph 1, the Commission shall communicate its preliminary findings to the provider of the general-purpose AI model and give it an opportunity to be heard.",
    "prompt": "What steps must the Commission take before making a decision about a general-purpose AI model?",
    "pe": "The Commission must first communicate its preliminary findings to the provider of the general-purpose AI model and provide an opportunity for the provider to be heard before adopting the decision.",
    "ne": "The Commission can proceed directly to adopting the decision without any prior communication or opportunity for the provider to be heard."
  },
  {
    "id": 433,
    "content": "Fines imposed in accordance with this Article shall be effective, proportionate and dissuasive.",
    "prompt": "What principles should guide the setting of fines for regulatory violations?",
    "pe": "Fines should be designed to be effective in achieving their purpose, proportionate to the severity of the violation, and sufficiently dissuasive to prevent future infractions.",
    "ne": "Fines can be set arbitrarily without considering their effectiveness, proportionality, or ability to deter violations, as long as they are imposed."
  },
  {
    "id": 434,
    "content": "It may cancel, reduce or increase the fine imposed.",
    "prompt": "What are the possible outcomes after a fine has been imposed?",
    "pe": "The fine may be canceled, reduced, or increased based on the circumstances.",
    "ne": "Once a fine is imposed, it cannot be changed under any circumstances."
  },
  {
    "id": 435,
    "content": "In any case, the providers and deployers of high-risk AI systems intended to be used by public authorities shall take the necessary steps to comply with the requirements and obligations of this Regulation by 2 August 2030.",
    "prompt": "What is the deadline for public authorities to ensure their high-risk AI systems are compliant with the relevant regulations?",
    "pe": "Providers and deployers of high-risk AI systems intended for use by public authorities must take all necessary steps to comply with the requirements and obligations by 2 August 2030.",
    "ne": "Public authorities have no specific deadline for compliance; they can implement high-risk AI systems at their own pace without adhering to any regulatory timeline."
  }
]
